{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prerequisites\n",
    "# Charalambos Themistocleous\n",
    "# Clean Memory before rerunning \n",
    "for name in dir():\n",
    "    if not name.startswith('_'): del globals()[name]\n",
    "#dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for dataset preparation, feature engineering, model training \n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve\n",
    "from scipy import interp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy, textblob, string, os\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (130,148,149,150,152,153,154,155) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# read text data\n",
    "textpath = 'data/text_data.tsv'\n",
    "#os.remove('data/combined_data.tsv')\n",
    "speechpath= 'phone_processed.csv'\n",
    "combinedpath = 'data/combined_data.tsv'\n",
    "\n",
    "cols = ['label_t', 'speaker', 'text']\n",
    "tdata = pd.read_table(textpath, header=None,names=cols)\n",
    "sdata = pd.read_table(speechpath, delimiter=\",\")\n",
    "# convert label to a numerical variable (category)\n",
    "tdata['label'] = tdata.label_t.map(\n",
    "    {\"s\": 0, \"l\": 1, \"n\": 2, \"naos\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text / NLP based features\n",
    "\n",
    "A number of extra text based features can also be created which sometimes are helpful for improving text classification models. Some examples are:\n",
    "\n",
    "* Word Count of the documents – total number of words in the documents\n",
    "* Character Count of the documents – total number of characters in the documents\n",
    "* Average Word Density of the documents – average length of the words used in the documents\n",
    "* Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
    "* Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
    "* Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
    "* Frequency distribution of Part of Speech Tags:\n",
    "\n",
    "- Noun Count\n",
    "- Verb Count\n",
    "- Adjective Count\n",
    "- Adverb Count\n",
    "- Pronoun Count\n",
    "\n",
    "These features are highly experimental ones and should be used according to the problem statement only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata['char_count'] = tdata['text'].apply(len)\n",
    "tdata['word_count'] = tdata['text'].apply(lambda x: len(x.split()))\n",
    "tdata['char_word_ratio'] = tdata['char_count'] / (tdata['word_count']+1)\n",
    "tdata['punctuation_count'] = tdata['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
    "tdata['title_word_count'] = tdata['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "tdata['upper_case_word_count'] = tdata['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_family = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "# function to check and get the part of speech tag count of a words in a given sentence\n",
    "def check_pos_tag(x, flag):\n",
    "    cnt = 0\n",
    "    try:\n",
    "        wiki = textblob.TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            if ppo in pos_family[flag]:\n",
    "                cnt += 1\n",
    "    except:\n",
    "        pass\n",
    "    return cnt\n",
    "\n",
    "tdata['noun_count'] = tdata['text'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
    "tdata['verb_count'] = tdata['text'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
    "tdata['adj_count'] = tdata['text'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
    "tdata['adv_count'] = tdata['text'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
    "tdata['pron_count'] = tdata['text'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
    "\n",
    "# POS Ratio\n",
    "tdata['noun_verb_ratio'] = tdata['noun_count']/tdata['verb_count']\n",
    "tdata['noun_adj_ratio'] = tdata['noun_count']/tdata['adj_count']\n",
    "tdata['noun_adv_ratio'] = tdata['noun_count']/tdata['adv_count']\n",
    "tdata['noun_pron_ratio'] = tdata['noun_count']/tdata['pron_count']\n",
    "tdata['verb_adj_ratio'] = tdata['verb_count']/tdata['adj_count']\n",
    "tdata['verb_adv_ratio'] = tdata['verb_count']/tdata['adv_count']\n",
    "tdata['verb_pron_ratio'] = tdata['verb_count']/tdata['pron_count']\n",
    "tdata['adj_adv_ratio'] = tdata['adj_count']/tdata['adv_count']\n",
    "tdata['adj_pron_ratio'] = tdata['adj_count']/tdata['pron_count']\n",
    "tdata['adv_pron_ratio'] = tdata['adv_count']/tdata['pron_count']\n",
    "\n",
    "# Mean POS per word\n",
    "tdata['mean_nouns'] = tdata['noun_count']/tdata['word_count']\n",
    "tdata['mean_pron'] = tdata['pron_count']/tdata['word_count']\n",
    "tdata['mean_verbs'] = tdata['verb_count']/tdata['word_count']\n",
    "tdata['mean_adj'] = tdata['adj_count']/tdata['word_count']\n",
    "tdata['mean_adv'] = tdata['adv_count']/tdata['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.merge(tdata, sdata, how='left', on=['speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_data.replace([np.inf, -np.inf], np.nan)\n",
    "df.to_csv(combinedpath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
