{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "# An automatic subtyping of PPA variants from connected speech samples using machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Memory before rerunning \n",
    "for name in dir():\n",
    "    if not name.startswith('_'): del globals()[name]\n",
    "#dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for dataset preparation, feature engineering, model training \n",
    "from scipy import interp\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve, classification_report\n",
    "\n",
    "\n",
    "from keras import layers, models, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import Dense, Dropout, GaussianNoise, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "from scipy import interp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy, textblob, string, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \"../\"\n",
    "TITLE_ID = \"PPAModel\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"figures\", str(fig_id) + \".png\")\n",
    "    print(\"Saving figure\", str(fig_id))\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  \n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3057: DtypeWarning: Columns (159,160) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "import pandas as pd\n",
    "SEED = 2000\n",
    "np.random.seed(SEED)\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "path = '../data/combined_data.tsv'\n",
    "df = pd.read_table(path, sep=\",\")\n",
    "df = df[df[\"phone\"]==\"V\"]\n",
    "df = df.loc[~df[\"speaker\"].isin([\"GFN6532\",\"JRK6031\",\"RBS8706\", \"RSN9920\", \"BWS1830\",\"ASR\", \"MBR6656\"])] # Unknown variant\n",
    "df = df.drop([\"phone\"], axis=1)\n",
    "obj_df = df.select_dtypes(include=['object']).copy()\n",
    "cleanup_nums = {\"gender\":   {\"F\": 0, \"M\": 1},\n",
    "                \"variant\":  {\"s\": 0, \"l\": 1, \"n\": 2, \"naos\": 2},\n",
    "                \"label_y\":    {\"AA\": 0, \"AE\": 1, \"AH\": 2, \"AO\": 3, \"AW\": 4, \"AY\": 5, \"EH\": 6, \"ER\": 7, \"EY\": 8, \"IH\": 9, \"IY\": 10, \"OW\": 11, \"OY\": 12, \"UH\": 13, \"UW\": 14},\n",
    "                \"speaker\":  {\"DEK\": 0, \"MPI\": 1, \"JEE\": 2, \"SRI\": 3, \"IJN\": 4, \"RFH\": 5, \"EAE\": 6, \"CKI\": 7, \"DNE\": 8, \"ERM\": 9, \"DUE\": 10, \"BLR\": 11, \"JBN\": 12, \"NCG\": 13, \n",
    "                             \"ABK\": 14, \"JWE\": 15, \"DRS\": 16, \"RLN\": 17, \"DPD\": 18, \"LRW\": 19, \"EBG\": 20, \"ACY\": 21, \"JJI\": 22, \"DPZ\": 23, \"JPS\": 24, \"GSH\": 25, \"JRA\": 26, \n",
    "                             \"AGG\": 27, \"CBN\": 28, \"LCR\": 29, \"DME\": 30, \"BIN\": 31, \"TBE\": 32, \"CCD\": 33, \"KBG\": 34, \"MOR\": 35, \"CBT\": 36, \"MVR\": 37, \"DCN\": 38, \n",
    "                             \"KGR8179\": 39, \"MSM\": 40, \"JSS\": 41, \"SLR8021\": 42, \"SFY\": 43, \"JHR\": 44, \"JSR9493\": 45, \"SRR\": 46, \"AGS2989\": 47, \"BSS7123\": 48, \"ASR\" : 49, \"CSZ0789\": 50, \"JTR8536\": 51,\"RPI8252\":52}}\n",
    "\n",
    "obj_df.replace(cleanup_nums, inplace=True)\n",
    "df.variant=obj_df.variant\n",
    "df.gender=obj_df.gender\n",
    "df.speaker=obj_df.speaker\n",
    "df.label=obj_df.label_y\n",
    "\n",
    "columns = ['variant',  'speaker', 'duration',  'pausedur', 'f1_15',  'f2_15', 'f3_15',  'f4_15', 'f5_15',  'f1_50', 'f2_50',  'f3_50', 'f4_50',  'f5_50', 'f1_75',  'f2_75',\n",
    "'f3_75',  'f4_75', 'f5_75',  'H1_H2',  'H1_A1', 'H1_A2',  'H1_A3',  'H1_H2_1', 'H1_A1_1',  'H1_A2_1',  'H1_A3_1', 'H1_H2_2',  'H1_A1_2',  'H1_A2_2', 'H1_A3_2',  'f0_min',\n",
    "'f0_mean', 'f0_max',  'int_min', 'int_mean', 'int_max',  'char_count', 'word_count', 'char_word_ratio', 'punctuation_count',  'title_word_count', 'upper_case_word_count',\n",
    "'noun_count',  'verb_count',  'adj_count',   'adv_count',  'pron_count',  'noun_verb_ratio',  'noun_adj_ratio',   'noun_adv_ratio',  'noun_pron_ratio',  'verb_adj_ratio',\n",
    "'verb_adv_ratio',  'verb_pron_ratio',  'adj_adv_ratio',  'adj_pron_ratio',  'adv_pron_ratio',  'mean_nouns',  'mean_pron',  'mean_verbs',  'mean_adj',  'mean_adv']  \n",
    "\n",
    "df = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:66: DeprecationWarning: Class Imputer is deprecated; Imputer was deprecated in version 0.20 and will be removed in 0.22. Import impute.SimpleImputer from sklearn instead.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Missing Values\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values='NaN', strategy='median', axis=0)\n",
    "dfim=imp.fit_transform(df)\n",
    "df = pd.DataFrame(dfim, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: divide by zero encountered in log\n",
      "  \n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in log\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "variant                  1.000000\n",
       "mean_nouns               0.548187\n",
       "noun_adj_ratio           0.484090\n",
       "noun_adv_ratio           0.478333\n",
       "char_word_ratio          0.422719\n",
       "noun_verb_ratio          0.419228\n",
       "verb_adv_ratio           0.400973\n",
       "noun_pron_ratio          0.286777\n",
       "verb_adj_ratio           0.207236\n",
       "adj_adv_ratio            0.191096\n",
       "int_max                  0.176155\n",
       "int_mean                 0.167318\n",
       "H1_A2_1                  0.152217\n",
       "H1_A2_2                  0.149384\n",
       "pausedur                 0.132358\n",
       "duration                 0.130563\n",
       "H1_A2                    0.122566\n",
       "H1_A3_2                  0.100095\n",
       "title_word_count         0.093625\n",
       "H1_A3_1                  0.092395\n",
       "verb_pron_ratio          0.088735\n",
       "H1_H2_2                  0.087012\n",
       "H1_A3                    0.077849\n",
       "H1_A1_1                  0.070363\n",
       "H1_A1                    0.067713\n",
       "H1_A1_2                  0.061868\n",
       "f0_max                   0.061174\n",
       "f0_mean                  0.041343\n",
       "f3_75                    0.031359\n",
       "f3_50                    0.029490\n",
       "                           ...   \n",
       "f0_min                   0.017003\n",
       "int_min                 -0.006529\n",
       "f1_50                   -0.013156\n",
       "H1_H2                   -0.015400\n",
       "f2_75                   -0.018045\n",
       "f1_75                   -0.024370\n",
       "f4_75                   -0.026371\n",
       "f1_15                   -0.029285\n",
       "f4_50                   -0.029831\n",
       "f2_15                   -0.031870\n",
       "f2_50                   -0.035676\n",
       "f4_15                   -0.037402\n",
       "adj_pron_ratio          -0.044521\n",
       "f5_75                   -0.047462\n",
       "noun_count              -0.048186\n",
       "f5_15                   -0.059941\n",
       "f5_50                   -0.067016\n",
       "mean_verbs              -0.083704\n",
       "mean_pron               -0.133644\n",
       "adj_count               -0.149870\n",
       "upper_case_word_count   -0.168655\n",
       "char_count              -0.172561\n",
       "punctuation_count       -0.178179\n",
       "pron_count              -0.187200\n",
       "word_count              -0.190826\n",
       "verb_count              -0.200349\n",
       "mean_adj                -0.304737\n",
       "adv_pron_ratio          -0.318421\n",
       "adv_count               -0.370073\n",
       "mean_adv                -0.394576\n",
       "Name: variant, Length: 62, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation Matrix\n",
    "a=np.log(df)\n",
    "a = a.drop([\"speaker\"], axis=1)\n",
    "corr_matrix=a.corr()\n",
    "corr_matrix['variant'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Select Labels for the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X is (10248, 53)\n",
      "Shape of y (10248,), \n",
      "Shape of groups (10248,)\n"
     ]
    }
   ],
   "source": [
    "y = df.variant\n",
    "y_0 = (y == 0)\n",
    "y_1 = (y == 1)\n",
    "y_2 = (y == 2)\n",
    "\n",
    "groups = df.speaker\n",
    "Xmarkers=['duration',\n",
    "'pausedur',\n",
    "'f1_15',\n",
    "'f2_15',\n",
    "'f3_15',\n",
    "'f4_15',\n",
    "'f5_15',         \n",
    "'f1_50',\n",
    "'f2_50',\n",
    "'f3_50',\n",
    "'f4_50',\n",
    "'f5_50',\n",
    "'f1_75',\n",
    "'f2_75',\n",
    "'f3_75',\n",
    "'f4_75',\n",
    "'f5_75',\n",
    "'H1_H2',\n",
    "'H1_A1',\n",
    "'H1_A2',\n",
    "'H1_A3',\n",
    "#'H1_H2_1',\n",
    "#'H1_A1_1',\n",
    "#'H1_A2_1',\n",
    "#'H1_A3_1',\n",
    "#'H1_H2_2',\n",
    "#'H1_A1_2',\n",
    "#'H1_A2_2',\n",
    "#'H1_A3_2',\n",
    "'f0_min',\n",
    "'f0_mean',\n",
    "'f0_max',\n",
    "#'int_min',\n",
    "#'int_mean',\n",
    "#'int_max',\n",
    "'char_count',\n",
    "'word_count',\n",
    "'char_word_ratio',\n",
    "#'punctuation_count',\n",
    "#'title_word_count',\n",
    "#'upper_case_word_count',\n",
    "#'noun_count',\n",
    "#'verb_count',\n",
    "#'adj_count',\n",
    "#'adv_count',\n",
    "#'pron_count',\n",
    "'noun_verb_ratio',\n",
    "'noun_adj_ratio',\n",
    "'noun_adv_ratio',\n",
    "'noun_pron_ratio',\n",
    "'verb_adj_ratio',\n",
    "'verb_adv_ratio',\n",
    "'verb_pron_ratio',\n",
    "'adj_adv_ratio',\n",
    "'adj_pron_ratio',\n",
    "'adv_pron_ratio',\n",
    "'mean_nouns',\n",
    "'mean_pron',\n",
    "'mean_verbs',\n",
    "'mean_adj',\n",
    "'mean_adv']\n",
    "X = df[Xmarkers]\n",
    "print('Shape of X is {}\\nShape of y {}, \\nShape of groups {}'.format(X.shape, y.shape, groups.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    19\n",
       "1.0    16\n",
       "0.0     9\n",
       "Name: variant, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many speakers belong to a variant \n",
    "mynice=df.pivot_table(values='variant', columns='speaker').T\n",
    "mynice.index.name = 'speaker'\n",
    "mynice.reset_index(inplace=True)\n",
    "mynice.variant.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "## Split Data: Speakers are randomly distributed in the sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hidePrompt": true
   },
   "source": [
    "This test simply aims to give an idea of what is going on in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "scaler = preprocessing.MinMaxScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "clf = svm.SVC(C=500).fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "clf.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split Data 80% and 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of speakers that should be in the test set based on the 20.0% calculation per variant:\n",
      "2.0    4.0\n",
      "1.0    3.0\n",
      "0.0    2.0\n",
      "Name: variant, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "mynice=df.pivot_table(values='variant', columns='speaker').T\n",
    "mynice.index.name = 'speaker'\n",
    "mynice.reset_index(inplace=True)\n",
    "# Number of speakers in the training set that needs to be removed. Basically I need to get an index\n",
    "test_speakers = np.round(mynice.variant.value_counts() * test_size)\n",
    "print(\"Number of speakers that should be in the test set based on the {}% calculation per variant:\\n{}\".format(test_size*100,test_speakers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = df.loc[df[\"speaker\"].isin([0,50,48,4,32, 11,36,43, 7,3,12])] # Unknown variant\n",
    "# Variant 1: 4,32, 11,36.\n",
    "# Variant 0: 0,50,48\n",
    "# Variant 2: 43, 7, 3,12\n",
    "\n",
    "y_tst = test_set.variant\n",
    "X_tst = test_set.drop([\"variant\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = df.loc[~df[\"speaker\"].isin([0,50,48,46,4,32, 11,36,29,43, 7,28, 3,12,31])] # Unknown variant\n",
    "y_tr = training_set.variant\n",
    "X_tr = training_set.drop([\"variant\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7128240109140518"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "scaler = preprocessing.StandardScaler().fit(X_tr)\n",
    "X_tr_transformed = scaler.transform(X_tr)\n",
    "clf = svm.SVC(C=10).fit(X_tr_transformed, y_tr)\n",
    "X_tst_transformed = scaler.transform(X_tst)\n",
    "clf.score(X_tst_transformed, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Classification\n",
      "WARNING:tensorflow:From /Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7152 samples, validate on 1789 samples\n",
      "Epoch 1/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 0.1493 - acc: 0.9425 - val_loss: 10.8157 - val_acc: 0.0151\n",
      "Epoch 2/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 0.0255 - acc: 0.9940 - val_loss: 12.5215 - val_acc: 0.0212\n",
      "Epoch 3/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 0.0507 - acc: 0.9934 - val_loss: 12.9429 - val_acc: 0.0358\n",
      "Epoch 4/600\n",
      "7152/7152 [==============================] - 7s 943us/step - loss: 0.0652 - acc: 0.9952 - val_loss: 13.5256 - val_acc: 0.0397\n",
      "Epoch 5/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 0.0086 - acc: 0.9986 - val_loss: 12.9839 - val_acc: 0.1381\n",
      "Epoch 6/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 0.0084 - acc: 0.9993 - val_loss: 13.6519 - val_acc: 0.1040\n",
      "Epoch 7/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 0.0270 - acc: 0.9973 - val_loss: 13.0048 - val_acc: 0.0900\n",
      "Epoch 8/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1966e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 9/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 10/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 11/600\n",
      "7152/7152 [==============================] - 17s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 12/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 13/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 14/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 15/600\n",
      "7152/7152 [==============================] - 17s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 16/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 17/600\n",
      "7152/7152 [==============================] - 7s 996us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 18/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 19/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 20/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 21/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 22/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 23/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 24/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 25/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 26/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 27/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 28/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 29/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 30/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 31/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 32/600\n",
      "7152/7152 [==============================] - 6s 897us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 33/600\n",
      "7152/7152 [==============================] - 6s 906us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 34/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 35/600\n",
      "7152/7152 [==============================] - 7s 961us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 36/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 37/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 38/600\n",
      "7152/7152 [==============================] - 7s 952us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 39/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 40/600\n",
      "7152/7152 [==============================] - 7s 951us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 41/600\n",
      "7152/7152 [==============================] - 6s 880us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 42/600\n",
      "7152/7152 [==============================] - 6s 894us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 43/600\n",
      "7152/7152 [==============================] - 6s 869us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 44/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 45/600\n",
      "7152/7152 [==============================] - 7s 921us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 46/600\n",
      "7152/7152 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 47/600\n",
      "7152/7152 [==============================] - 7s 922us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 48/600\n",
      "7152/7152 [==============================] - 6s 858us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 49/600\n",
      "7152/7152 [==============================] - 6s 854us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 50/600\n",
      "7152/7152 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 51/600\n",
      "7152/7152 [==============================] - 6s 902us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 52/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 53/600\n",
      "7152/7152 [==============================] - 6s 856us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 54/600\n",
      "7152/7152 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 55/600\n",
      "7152/7152 [==============================] - 6s 845us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 56/600\n",
      "7152/7152 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 57/600\n",
      "7152/7152 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 58/600\n",
      "7152/7152 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 59/600\n",
      "7152/7152 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 60/600\n",
      "7152/7152 [==============================] - 6s 835us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 61/600\n",
      "7152/7152 [==============================] - 6s 866us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 62/600\n",
      "7152/7152 [==============================] - 6s 869us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 63/600\n",
      "7152/7152 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 64/600\n",
      "7152/7152 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 65/600\n",
      "7152/7152 [==============================] - 6s 873us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 66/600\n",
      "7152/7152 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 67/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 68/600\n",
      "7152/7152 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 69/600\n",
      "7152/7152 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 70/600\n",
      "7152/7152 [==============================] - 6s 882us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 71/600\n",
      "7152/7152 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 72/600\n",
      "7152/7152 [==============================] - 6s 836us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 73/600\n",
      "7152/7152 [==============================] - 6s 854us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 74/600\n",
      "7152/7152 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 75/600\n",
      "7152/7152 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 76/600\n",
      "7152/7152 [==============================] - 6s 879us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 77/600\n",
      "7152/7152 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 78/600\n",
      "7152/7152 [==============================] - 7s 929us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 79/600\n",
      "7152/7152 [==============================] - 7s 915us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 80/600\n",
      "7152/7152 [==============================] - 7s 916us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 81/600\n",
      "7152/7152 [==============================] - 6s 887us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 82/600\n",
      "7152/7152 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 83/600\n",
      "7152/7152 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 84/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 85/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 86/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 87/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 88/600\n",
      "7152/7152 [==============================] - 7s 982us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 89/600\n",
      "7152/7152 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 90/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 91/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 92/600\n",
      "7152/7152 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 93/600\n",
      "7152/7152 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 94/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 95/600\n",
      "7152/7152 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 96/600\n",
      "7152/7152 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 97/600\n",
      "7152/7152 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 98/600\n",
      "7152/7152 [==============================] - 6s 847us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 99/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 100/600\n",
      "7152/7152 [==============================] - 7s 933us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 101/600\n",
      "7152/7152 [==============================] - 7s 911us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 102/600\n",
      "7152/7152 [==============================] - 7s 945us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 103/600\n",
      "7152/7152 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 104/600\n",
      "7152/7152 [==============================] - 6s 878us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 105/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 106/600\n",
      "7152/7152 [==============================] - 6s 886us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 107/600\n",
      "7152/7152 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 108/600\n",
      "7152/7152 [==============================] - 6s 831us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 109/600\n",
      "7152/7152 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 110/600\n",
      "7152/7152 [==============================] - 6s 857us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 111/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 112/600\n",
      "7152/7152 [==============================] - 7s 913us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 113/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 114/600\n",
      "7152/7152 [==============================] - 6s 779us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 115/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7152/7152 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 116/600\n",
      "7152/7152 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 117/600\n",
      "7152/7152 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 118/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 119/600\n",
      "7152/7152 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 120/600\n",
      "7152/7152 [==============================] - 6s 785us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 121/600\n",
      "7152/7152 [==============================] - 6s 908us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 122/600\n",
      "7152/7152 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 123/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 124/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 125/600\n",
      "7152/7152 [==============================] - 7s 910us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 126/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 127/600\n",
      "7152/7152 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 128/600\n",
      "7152/7152 [==============================] - 6s 864us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 129/600\n",
      "7152/7152 [==============================] - 6s 876us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 130/600\n",
      "7152/7152 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 131/600\n",
      "7152/7152 [==============================] - 7s 925us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 132/600\n",
      "7152/7152 [==============================] - 6s 885us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 133/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 134/600\n",
      "7152/7152 [==============================] - 7s 944us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 135/600\n",
      "7152/7152 [==============================] - 7s 929us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 136/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 137/600\n",
      "7152/7152 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 138/600\n",
      "7152/7152 [==============================] - 6s 876us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 139/600\n",
      "7152/7152 [==============================] - 6s 903us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 140/600\n",
      "7152/7152 [==============================] - 6s 886us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 141/600\n",
      "7152/7152 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 142/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 143/600\n",
      "7152/7152 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 144/600\n",
      "7152/7152 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 145/600\n",
      "7152/7152 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 146/600\n",
      "7152/7152 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 147/600\n",
      "7152/7152 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 148/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 149/600\n",
      "7152/7152 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 150/600\n",
      "7152/7152 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 151/600\n",
      "7152/7152 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 152/600\n",
      "7152/7152 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 153/600\n",
      "7152/7152 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 154/600\n",
      "7152/7152 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 155/600\n",
      "7152/7152 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 156/600\n",
      "7152/7152 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 157/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 158/600\n",
      "7152/7152 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 159/600\n",
      "7152/7152 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 160/600\n",
      "7152/7152 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 161/600\n",
      "7152/7152 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 162/600\n",
      "7152/7152 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 163/600\n",
      "7152/7152 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 164/600\n",
      "7152/7152 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 165/600\n",
      "7152/7152 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 166/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 167/600\n",
      "7152/7152 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 168/600\n",
      "7152/7152 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 169/600\n",
      "7152/7152 [==============================] - 6s 837us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 170/600\n",
      "7152/7152 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 171/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 172/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7152/7152 [==============================] - 7s 952us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 173/600\n",
      "7152/7152 [==============================] - 7s 972us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 174/600\n",
      "7152/7152 [==============================] - 6s 869us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 175/600\n",
      "7152/7152 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 176/600\n",
      "7152/7152 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 177/600\n",
      "7152/7152 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 178/600\n",
      "7152/7152 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 179/600\n",
      "7152/7152 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 180/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 181/600\n",
      "7152/7152 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 182/600\n",
      "7152/7152 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 183/600\n",
      "7152/7152 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 184/600\n",
      "7152/7152 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 185/600\n",
      "7152/7152 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 186/600\n",
      "7152/7152 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 187/600\n",
      "7152/7152 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 188/600\n",
      "7152/7152 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 189/600\n",
      "7152/7152 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 190/600\n",
      "7152/7152 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 191/600\n",
      "7152/7152 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 192/600\n",
      "7152/7152 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 193/600\n",
      "7152/7152 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 194/600\n",
      "7152/7152 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 195/600\n",
      "7152/7152 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 196/600\n",
      "7152/7152 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 197/600\n",
      "7152/7152 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 198/600\n",
      "7152/7152 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 199/600\n",
      "7152/7152 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 200/600\n",
      "7152/7152 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 201/600\n",
      "7152/7152 [==============================] - 6s 808us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 202/600\n",
      "7152/7152 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 203/600\n",
      "7152/7152 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 204/600\n",
      "7152/7152 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 205/600\n",
      "7152/7152 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 206/600\n",
      "7152/7152 [==============================] - 6s 834us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 207/600\n",
      "7152/7152 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 208/600\n",
      "7152/7152 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 209/600\n",
      "7152/7152 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 210/600\n",
      "7152/7152 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 211/600\n",
      "7152/7152 [==============================] - 6s 792us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 212/600\n",
      "7152/7152 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 213/600\n",
      "7152/7152 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 214/600\n",
      "7152/7152 [==============================] - 6s 797us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 215/600\n",
      "7152/7152 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 216/600\n",
      "7152/7152 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 217/600\n",
      "7152/7152 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 218/600\n",
      "7152/7152 [==============================] - 6s 834us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 219/600\n",
      "7152/7152 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 220/600\n",
      "7152/7152 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 221/600\n",
      "7152/7152 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 222/600\n",
      "7152/7152 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 223/600\n",
      "7152/7152 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 224/600\n",
      "7152/7152 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 225/600\n",
      "7152/7152 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 226/600\n",
      "7152/7152 [==============================] - 6s 842us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 227/600\n",
      "7152/7152 [==============================] - 6s 890us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 228/600\n",
      "7152/7152 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 229/600\n",
      "7152/7152 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 230/600\n",
      "7152/7152 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 231/600\n",
      "7152/7152 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 232/600\n",
      "7152/7152 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 233/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 234/600\n",
      "7152/7152 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 235/600\n",
      "7152/7152 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 236/600\n",
      "7152/7152 [==============================] - 7s 975us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 237/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 238/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 239/600\n",
      "7152/7152 [==============================] - 7s 965us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 240/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 241/600\n",
      "7152/7152 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 242/600\n",
      "7152/7152 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 243/600\n",
      "7152/7152 [==============================] - 5s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 244/600\n",
      "7152/7152 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 245/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 246/600\n",
      "7152/7152 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 247/600\n",
      "7152/7152 [==============================] - 6s 845us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 248/600\n",
      "7152/7152 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 249/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 250/600\n",
      "7152/7152 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 251/600\n",
      "7152/7152 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 252/600\n",
      "7152/7152 [==============================] - 5s 748us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 253/600\n",
      "7152/7152 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 254/600\n",
      "7152/7152 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 255/600\n",
      "7152/7152 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 256/600\n",
      "7152/7152 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 257/600\n",
      "7152/7152 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 258/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 259/600\n",
      "7152/7152 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 260/600\n",
      "7152/7152 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 261/600\n",
      "7152/7152 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 262/600\n",
      "7152/7152 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 263/600\n",
      "7152/7152 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 264/600\n",
      "7152/7152 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 265/600\n",
      "7152/7152 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 266/600\n",
      "7152/7152 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 267/600\n",
      "7152/7152 [==============================] - 5s 763us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 268/600\n",
      "7152/7152 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 269/600\n",
      "7152/7152 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 270/600\n",
      "7152/7152 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 271/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 272/600\n",
      "7152/7152 [==============================] - 6s 861us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 273/600\n",
      "7152/7152 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 274/600\n",
      "7152/7152 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 275/600\n",
      "7152/7152 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 276/600\n",
      "7152/7152 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 277/600\n",
      "7152/7152 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 278/600\n",
      "7152/7152 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 279/600\n",
      "7152/7152 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 280/600\n",
      "7152/7152 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 281/600\n",
      "7152/7152 [==============================] - 5s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 282/600\n",
      "7152/7152 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 283/600\n",
      "7152/7152 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 284/600\n",
      "7152/7152 [==============================] - 5s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 285/600\n",
      "7152/7152 [==============================] - 6s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 286/600\n",
      "7152/7152 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 287/600\n",
      "7152/7152 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 288/600\n",
      "7152/7152 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 289/600\n",
      "7152/7152 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 290/600\n",
      "7152/7152 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 291/600\n",
      "7152/7152 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 292/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 293/600\n",
      "7152/7152 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 294/600\n",
      "7152/7152 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 295/600\n",
      "7152/7152 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 296/600\n",
      "7152/7152 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 297/600\n",
      "7152/7152 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 298/600\n",
      "7152/7152 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 299/600\n",
      "7152/7152 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 300/600\n",
      "7152/7152 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 301/600\n",
      "7152/7152 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 302/600\n",
      "7152/7152 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 303/600\n",
      "7152/7152 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 304/600\n",
      "7152/7152 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 305/600\n",
      "7152/7152 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 306/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 307/600\n",
      "7152/7152 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 308/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 309/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 310/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 311/600\n",
      "7152/7152 [==============================] - 6s 903us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 312/600\n",
      "7152/7152 [==============================] - 6s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 313/600\n",
      "7152/7152 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 314/600\n",
      "7152/7152 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 315/600\n",
      "7152/7152 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 316/600\n",
      "7152/7152 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 317/600\n",
      "7152/7152 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 318/600\n",
      "7152/7152 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 319/600\n",
      "7152/7152 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 320/600\n",
      "7152/7152 [==============================] - 5s 717us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 321/600\n",
      "7152/7152 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 322/600\n",
      "7152/7152 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 323/600\n",
      "7152/7152 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 324/600\n",
      "7152/7152 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 325/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 326/600\n",
      "7152/7152 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 327/600\n",
      "7152/7152 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 328/600\n",
      "7152/7152 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 329/600\n",
      "7152/7152 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 330/600\n",
      "7152/7152 [==============================] - 6s 897us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 331/600\n",
      "7152/7152 [==============================] - 6s 857us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 332/600\n",
      "7152/7152 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 333/600\n",
      "7152/7152 [==============================] - 6s 880us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 334/600\n",
      "7152/7152 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 335/600\n",
      "7152/7152 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 336/600\n",
      "7152/7152 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 337/600\n",
      "7152/7152 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 338/600\n",
      "7152/7152 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 339/600\n",
      "7152/7152 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 340/600\n",
      "7152/7152 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 341/600\n",
      "7152/7152 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 342/600\n",
      "7152/7152 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 343/600\n",
      "7152/7152 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 344/600\n",
      "7152/7152 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 345/600\n",
      "7152/7152 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 346/600\n",
      "7152/7152 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 347/600\n",
      "7152/7152 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 348/600\n",
      "7152/7152 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 349/600\n",
      "7152/7152 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 350/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 351/600\n",
      "7152/7152 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 352/600\n",
      "7152/7152 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 353/600\n",
      "7152/7152 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 354/600\n",
      "7152/7152 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 355/600\n",
      "7152/7152 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 356/600\n",
      "7152/7152 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 357/600\n",
      "7152/7152 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 358/600\n",
      "7152/7152 [==============================] - 6s 862us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 359/600\n",
      "7152/7152 [==============================] - 5s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 360/600\n",
      "7152/7152 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 361/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 362/600\n",
      "7152/7152 [==============================] - 6s 823us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 363/600\n",
      "7152/7152 [==============================] - 6s 832us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 364/600\n",
      "7152/7152 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 365/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 366/600\n",
      "7152/7152 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 367/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 368/600\n",
      "7152/7152 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 369/600\n",
      "7152/7152 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 370/600\n",
      "7152/7152 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 371/600\n",
      "7152/7152 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 372/600\n",
      "7152/7152 [==============================] - 6s 859us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 373/600\n",
      "7152/7152 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 374/600\n",
      "7152/7152 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 375/600\n",
      "7152/7152 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 376/600\n",
      "7152/7152 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 377/600\n",
      "7152/7152 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 378/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 379/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 380/600\n",
      "7152/7152 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 381/600\n",
      "7152/7152 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 382/600\n",
      "7152/7152 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 383/600\n",
      "7152/7152 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 384/600\n",
      "7152/7152 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 385/600\n",
      "7152/7152 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 386/600\n",
      "7152/7152 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 387/600\n",
      "7152/7152 [==============================] - 6s 818us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 388/600\n",
      "7152/7152 [==============================] - 7s 954us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 389/600\n",
      "7152/7152 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 390/600\n",
      "7152/7152 [==============================] - 6s 887us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 391/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 392/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 393/600\n",
      "7152/7152 [==============================] - 7s 957us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 394/600\n",
      "7152/7152 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 395/600\n",
      "7152/7152 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 396/600\n",
      "7152/7152 [==============================] - 6s 863us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 397/600\n",
      "7152/7152 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 398/600\n",
      "7152/7152 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 400/600\n",
      "7152/7152 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 401/600\n",
      "7152/7152 [==============================] - 5s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 402/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 403/600\n",
      "7152/7152 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 404/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 405/600\n",
      "7152/7152 [==============================] - 6s 835us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 406/600\n",
      "7152/7152 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 407/600\n",
      "7152/7152 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 408/600\n",
      "7152/7152 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 409/600\n",
      "7152/7152 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 410/600\n",
      "7152/7152 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 411/600\n",
      "7152/7152 [==============================] - 7s 999us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 412/600\n",
      "7152/7152 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 413/600\n",
      "7152/7152 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 414/600\n",
      "7152/7152 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 415/600\n",
      "7152/7152 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 416/600\n",
      "7152/7152 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 417/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 418/600\n",
      "7152/7152 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 419/600\n",
      "7152/7152 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 420/600\n",
      "7152/7152 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 421/600\n",
      "7152/7152 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 422/600\n",
      "7152/7152 [==============================] - 6s 832us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 423/600\n",
      "7152/7152 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 424/600\n",
      "7152/7152 [==============================] - 6s 856us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 425/600\n",
      "7152/7152 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 426/600\n",
      "7152/7152 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 427/600\n",
      "7152/7152 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 428/600\n",
      "7152/7152 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 429/600\n",
      "7152/7152 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 430/600\n",
      "7152/7152 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 431/600\n",
      "7152/7152 [==============================] - 5s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 432/600\n",
      "7152/7152 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 433/600\n",
      "7152/7152 [==============================] - 5s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 434/600\n",
      "7152/7152 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 435/600\n",
      "7152/7152 [==============================] - 6s 859us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 436/600\n",
      "7152/7152 [==============================] - 6s 887us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 437/600\n",
      "7152/7152 [==============================] - 7s 992us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 438/600\n",
      "7152/7152 [==============================] - 6s 833us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 439/600\n",
      "7152/7152 [==============================] - 7s 914us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 440/600\n",
      "7152/7152 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 441/600\n",
      "7152/7152 [==============================] - 6s 858us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 442/600\n",
      "7152/7152 [==============================] - 6s 836us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 443/600\n",
      "7152/7152 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 444/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 445/600\n",
      "7152/7152 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 446/600\n",
      "7152/7152 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 447/600\n",
      "7152/7152 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 448/600\n",
      "7152/7152 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 449/600\n",
      "7152/7152 [==============================] - 7s 993us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 450/600\n",
      "7152/7152 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 451/600\n",
      "7152/7152 [==============================] - 6s 836us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 452/600\n",
      "7152/7152 [==============================] - 6s 845us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 453/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 454/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 455/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 456/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 457/600\n",
      "7152/7152 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 458/600\n",
      "7152/7152 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 459/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 460/600\n",
      "7152/7152 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 461/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 462/600\n",
      "7152/7152 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 463/600\n",
      "7152/7152 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 464/600\n",
      "7152/7152 [==============================] - 5s 629us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 465/600\n",
      "7152/7152 [==============================] - 5s 637us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 466/600\n",
      "7152/7152 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 467/600\n",
      "7152/7152 [==============================] - 5s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 468/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 469/600\n",
      "7152/7152 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 470/600\n",
      "7152/7152 [==============================] - 5s 644us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 471/600\n",
      "7152/7152 [==============================] - 5s 644us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 472/600\n",
      "7152/7152 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 473/600\n",
      "7152/7152 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 474/600\n",
      "7152/7152 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 475/600\n",
      "7152/7152 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 476/600\n",
      "7152/7152 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 477/600\n",
      "7152/7152 [==============================] - 5s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 478/600\n",
      "7152/7152 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 479/600\n",
      "7152/7152 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 480/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 481/600\n",
      "7152/7152 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 482/600\n",
      "7152/7152 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 483/600\n",
      "7152/7152 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 484/600\n",
      "7152/7152 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 485/600\n",
      "7152/7152 [==============================] - 6s 852us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 486/600\n",
      "7152/7152 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 487/600\n",
      "7152/7152 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 488/600\n",
      "7152/7152 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 489/600\n",
      "7152/7152 [==============================] - 7s 923us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 490/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 491/600\n",
      "7152/7152 [==============================] - 6s 901us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 492/600\n",
      "7152/7152 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 493/600\n",
      "7152/7152 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 494/600\n",
      "7152/7152 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 495/600\n",
      "7152/7152 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 496/600\n",
      "7152/7152 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 497/600\n",
      "7152/7152 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 498/600\n",
      "7152/7152 [==============================] - 5s 648us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 499/600\n",
      "7152/7152 [==============================] - 6s 826us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 500/600\n",
      "7152/7152 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 501/600\n",
      "7152/7152 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 502/600\n",
      "7152/7152 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 503/600\n",
      "7152/7152 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 504/600\n",
      "7152/7152 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 505/600\n",
      "7152/7152 [==============================] - 6s 851us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 506/600\n",
      "7152/7152 [==============================] - 6s 900us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 507/600\n",
      "7152/7152 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 508/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 509/600\n",
      "7152/7152 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 510/600\n",
      "7152/7152 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 511/600\n",
      "7152/7152 [==============================] - 6s 849us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 512/600\n",
      "7152/7152 [==============================] - 6s 785us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 513/600\n",
      "7152/7152 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 514/600\n",
      "7152/7152 [==============================] - 6s 887us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 515/600\n",
      "7152/7152 [==============================] - 6s 887us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 516/600\n",
      "7152/7152 [==============================] - 7s 924us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 517/600\n",
      "7152/7152 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 518/600\n",
      "7152/7152 [==============================] - 6s 831us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 519/600\n",
      "7152/7152 [==============================] - 6s 836us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 520/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 521/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 522/600\n",
      "7152/7152 [==============================] - 7s 913us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 523/600\n",
      "7152/7152 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 524/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 525/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 526/600\n",
      "7152/7152 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 527/600\n",
      "7152/7152 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 528/600\n",
      "7152/7152 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 529/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 530/600\n",
      "7152/7152 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 531/600\n",
      "7152/7152 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 532/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 533/600\n",
      "7152/7152 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 534/600\n",
      "7152/7152 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 535/600\n",
      "7152/7152 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 536/600\n",
      "7152/7152 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 537/600\n",
      "7152/7152 [==============================] - 6s 859us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 538/600\n",
      "7152/7152 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 539/600\n",
      "7152/7152 [==============================] - 6s 831us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 540/600\n",
      "7152/7152 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 541/600\n",
      "7152/7152 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 542/600\n",
      "7152/7152 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 543/600\n",
      "7152/7152 [==============================] - 6s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 544/600\n",
      "7152/7152 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 545/600\n",
      "7152/7152 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 546/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 547/600\n",
      "7152/7152 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 548/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 549/600\n",
      "7152/7152 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 550/600\n",
      "7152/7152 [==============================] - 6s 802us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 551/600\n",
      "7152/7152 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 552/600\n",
      "7152/7152 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 553/600\n",
      "7152/7152 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 554/600\n",
      "7152/7152 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 555/600\n",
      "7152/7152 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 556/600\n",
      "7152/7152 [==============================] - 5s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 557/600\n",
      "7152/7152 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 558/600\n",
      "7152/7152 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 559/600\n",
      "7152/7152 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 560/600\n",
      "7152/7152 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 561/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 562/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 563/600\n",
      "7152/7152 [==============================] - 6s 858us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 564/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 565/600\n",
      "7152/7152 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 566/600\n",
      "7152/7152 [==============================] - 6s 832us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 567/600\n",
      "7152/7152 [==============================] - 6s 797us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 568/600\n",
      "7152/7152 [==============================] - 7s 944us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 569/600\n",
      "7152/7152 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 570/600\n",
      "7152/7152 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 571/600\n",
      "7152/7152 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 572/600\n",
      "7152/7152 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 573/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 574/600\n",
      "7152/7152 [==============================] - 5s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 575/600\n",
      "7152/7152 [==============================] - 7s 989us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 576/600\n",
      "7152/7152 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 577/600\n",
      "7152/7152 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 578/600\n",
      "7152/7152 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 579/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 580/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 581/600\n",
      "7152/7152 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 582/600\n",
      "7152/7152 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 583/600\n",
      "7152/7152 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 584/600\n",
      "7152/7152 [==============================] - 7s 954us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 585/600\n",
      "7152/7152 [==============================] - 6s 771us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 586/600\n",
      "7152/7152 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 587/600\n",
      "7152/7152 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 588/600\n",
      "7152/7152 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 589/600\n",
      "7152/7152 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 590/600\n",
      "7152/7152 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 591/600\n",
      "7152/7152 [==============================] - 7s 928us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 592/600\n",
      "7152/7152 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 593/600\n",
      "7152/7152 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 594/600\n",
      "7152/7152 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 595/600\n",
      "7152/7152 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 596/600\n",
      "7152/7152 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 597/600\n",
      "7152/7152 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 598/600\n",
      "7152/7152 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 599/600\n",
      "7152/7152 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "Epoch 600/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 13.2003 - val_acc: 0.1017\n",
      "1307/1307 [==============================] - 0s 113us/step\n",
      "==========================================================================\n",
      "FOLD 0\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369]\n",
      "RF Accuracy B: [0.6702371843917369]\n",
      "RF Confusion Matrix: \n",
      "[[723   0   0]\n",
      " [  0  59 431]\n",
      " [  0   0  94]]\n",
      "SVM Accuracy A: [0.7712318286151492]\n",
      "SVM Accuracy B: [0.7712318286151492]\n",
      "SVM Confusion Matrix: \n",
      "[[723   0   0]\n",
      " [ 59 191 240]\n",
      " [  0   0  94]]\n",
      "DT Accuracy A: [0.8163733741392502]\n",
      "DT Accuracy B: [0.8163733741392502]\n",
      "DT Confusion Matrix: \n",
      "[[723   0   0]\n",
      " [240 250   0]\n",
      " [  0   0  94]]\n",
      "sNN Accuracy A: [0.4269319051262433]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184]]\n",
      "sNN Confusion Matrix: \n",
      "[[224 345 154]\n",
      " [ 41 240 209]\n",
      " [  0   0  94]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 1\n",
      "Train on 7193 samples, validate on 1799 samples\n",
      "Epoch 1/600\n",
      " 128/7193 [..............................] - ETA: 4s - loss: 1.1151 - acc: 0.9062"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7193/7193 [==============================] - 5s 696us/step - loss: 0.2317 - acc: 0.9691 - val_loss: 1.6704 - val_acc: 0.4008\n",
      "Epoch 2/600\n",
      "7193/7193 [==============================] - 5s 700us/step - loss: 0.0285 - acc: 0.9933 - val_loss: 3.8306 - val_acc: 0.6420\n",
      "Epoch 3/600\n",
      "7193/7193 [==============================] - 5s 681us/step - loss: 0.0364 - acc: 0.9962 - val_loss: 1.6733 - val_acc: 0.7526\n",
      "Epoch 4/600\n",
      "7193/7193 [==============================] - 5s 739us/step - loss: 0.0225 - acc: 0.9981 - val_loss: 2.7807 - val_acc: 0.7638\n",
      "Epoch 5/600\n",
      "7193/7193 [==============================] - 6s 800us/step - loss: 0.0179 - acc: 0.9981 - val_loss: 3.6938 - val_acc: 0.7148\n",
      "Epoch 6/600\n",
      "7193/7193 [==============================] - 7s 907us/step - loss: 0.0249 - acc: 0.9978 - val_loss: 2.2991 - val_acc: 0.8388\n",
      "Epoch 7/600\n",
      "7193/7193 [==============================] - 6s 808us/step - loss: 0.0052 - acc: 0.9992 - val_loss: 3.2475 - val_acc: 0.7743\n",
      "Epoch 8/600\n",
      "7193/7193 [==============================] - 6s 806us/step - loss: 0.0243 - acc: 0.9983 - val_loss: 6.9076 - val_acc: 0.5586\n",
      "Epoch 9/600\n",
      "7193/7193 [==============================] - 5s 758us/step - loss: 0.0128 - acc: 0.9990 - val_loss: 2.5540 - val_acc: 0.8382\n",
      "Epoch 10/600\n",
      "7193/7193 [==============================] - 6s 796us/step - loss: 0.0091 - acc: 0.9993 - val_loss: 2.6660 - val_acc: 0.8260\n",
      "Epoch 11/600\n",
      "7193/7193 [==============================] - 6s 780us/step - loss: 0.0134 - acc: 0.9989 - val_loss: 3.2926 - val_acc: 0.7476\n",
      "Epoch 12/600\n",
      "7193/7193 [==============================] - 6s 800us/step - loss: 9.9476e-04 - acc: 0.9997 - val_loss: 4.0671 - val_acc: 0.7021\n",
      "Epoch 13/600\n",
      "7193/7193 [==============================] - 6s 820us/step - loss: 1.1939e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 14/600\n",
      "7193/7193 [==============================] - 6s 783us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 15/600\n",
      "7193/7193 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 16/600\n",
      "7193/7193 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 17/600\n",
      "7193/7193 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 18/600\n",
      "7193/7193 [==============================] - 6s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 19/600\n",
      "7193/7193 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 20/600\n",
      "7193/7193 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 21/600\n",
      "7193/7193 [==============================] - 6s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 22/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 23/600\n",
      "7193/7193 [==============================] - 7s 909us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 24/600\n",
      "7193/7193 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 25/600\n",
      "7193/7193 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 26/600\n",
      "7193/7193 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 27/600\n",
      "7193/7193 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 28/600\n",
      "7193/7193 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 29/600\n",
      "7193/7193 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 30/600\n",
      "7193/7193 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 31/600\n",
      "7193/7193 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 32/600\n",
      "7193/7193 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 33/600\n",
      "7193/7193 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 34/600\n",
      "7193/7193 [==============================] - 5s 763us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 35/600\n",
      "7193/7193 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 36/600\n",
      "7193/7193 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 37/600\n",
      "7193/7193 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 38/600\n",
      "7193/7193 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 39/600\n",
      "7193/7193 [==============================] - 6s 792us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 40/600\n",
      "7193/7193 [==============================] - 6s 833us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 41/600\n",
      "7193/7193 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 42/600\n",
      "7193/7193 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 43/600\n",
      "7193/7193 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 44/600\n",
      "7193/7193 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 45/600\n",
      "7193/7193 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 46/600\n",
      "7193/7193 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 47/600\n",
      "7193/7193 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 48/600\n",
      "7193/7193 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 49/600\n",
      "7193/7193 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 50/600\n",
      "7193/7193 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 51/600\n",
      "7193/7193 [==============================] - 6s 851us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 52/600\n",
      "7193/7193 [==============================] - 6s 881us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 53/600\n",
      "7193/7193 [==============================] - 6s 894us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 54/600\n",
      "7193/7193 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 55/600\n",
      "7193/7193 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 56/600\n",
      "7193/7193 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 57/600\n",
      "7193/7193 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 58/600\n",
      "7193/7193 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 59/600\n",
      "7193/7193 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 60/600\n",
      "7193/7193 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 61/600\n",
      "7193/7193 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 62/600\n",
      "7193/7193 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 63/600\n",
      "7193/7193 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 64/600\n",
      "7193/7193 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 65/600\n",
      "7193/7193 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 66/600\n",
      "7193/7193 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 67/600\n",
      "7193/7193 [==============================] - 5s 640us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 68/600\n",
      "7193/7193 [==============================] - 5s 649us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 69/600\n",
      "7193/7193 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 70/600\n",
      "7193/7193 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 71/600\n",
      "7193/7193 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 72/600\n",
      "7193/7193 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 73/600\n",
      "7193/7193 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 74/600\n",
      "7193/7193 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 75/600\n",
      "7193/7193 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 76/600\n",
      "7193/7193 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 77/600\n",
      "7193/7193 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 78/600\n",
      "7193/7193 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 79/600\n",
      "7193/7193 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 80/600\n",
      "7193/7193 [==============================] - 6s 890us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 81/600\n",
      "7193/7193 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 82/600\n",
      "7193/7193 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 83/600\n",
      "7193/7193 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 84/600\n",
      "7193/7193 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 85/600\n",
      "7193/7193 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 86/600\n",
      "7193/7193 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 87/600\n",
      "7193/7193 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 88/600\n",
      "7193/7193 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 89/600\n",
      "7193/7193 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 90/600\n",
      "7193/7193 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 91/600\n",
      "7193/7193 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 92/600\n",
      "7193/7193 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 93/600\n",
      "7193/7193 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 94/600\n",
      "7193/7193 [==============================] - 5s 647us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 95/600\n",
      "7193/7193 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 96/600\n",
      "7193/7193 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 97/600\n",
      "7193/7193 [==============================] - 5s 633us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 98/600\n",
      "7193/7193 [==============================] - 5s 648us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 99/600\n",
      "7193/7193 [==============================] - 5s 633us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 100/600\n",
      "7193/7193 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 101/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 102/600\n",
      "7193/7193 [==============================] - 6s 878us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 103/600\n",
      "7193/7193 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 104/600\n",
      "7193/7193 [==============================] - 7s 932us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 105/600\n",
      "7193/7193 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 106/600\n",
      "7193/7193 [==============================] - 6s 882us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 107/600\n",
      "7193/7193 [==============================] - 6s 875us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 108/600\n",
      "7193/7193 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 109/600\n",
      "7193/7193 [==============================] - 7s 917us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 110/600\n",
      "7193/7193 [==============================] - 7s 960us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 111/600\n",
      "7193/7193 [==============================] - 7s 921us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 112/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 113/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 114/600\n",
      "7193/7193 [==============================] - 6s 875us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 115/600\n",
      "7193/7193 [==============================] - 7s 980us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 116/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7193/7193 [==============================] - 6s 876us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 117/600\n",
      "7193/7193 [==============================] - 7s 909us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 118/600\n",
      "7193/7193 [==============================] - 7s 955us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 119/600\n",
      "7193/7193 [==============================] - 7s 936us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 120/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 121/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 122/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 123/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 124/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 125/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 126/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 127/600\n",
      "7193/7193 [==============================] - 7s 914us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 128/600\n",
      "7193/7193 [==============================] - 7s 996us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 129/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 130/600\n",
      "7193/7193 [==============================] - 7s 963us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 131/600\n",
      "7193/7193 [==============================] - 6s 901us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 132/600\n",
      "7193/7193 [==============================] - 6s 878us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 133/600\n",
      "7193/7193 [==============================] - 7s 912us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 134/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 135/600\n",
      "7193/7193 [==============================] - 7s 992us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 136/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 137/600\n",
      "7193/7193 [==============================] - 7s 994us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 138/600\n",
      "7193/7193 [==============================] - 6s 892us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 139/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 140/600\n",
      "7193/7193 [==============================] - 6s 901us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 141/600\n",
      "7193/7193 [==============================] - 7s 949us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 142/600\n",
      "7193/7193 [==============================] - 7s 961us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 143/600\n",
      "7193/7193 [==============================] - 6s 874us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 144/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 145/600\n",
      "7193/7193 [==============================] - 6s 902us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 146/600\n",
      "7193/7193 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 147/600\n",
      "7193/7193 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 148/600\n",
      "7193/7193 [==============================] - 7s 946us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 149/600\n",
      "7193/7193 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 150/600\n",
      "7193/7193 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 151/600\n",
      "7193/7193 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 152/600\n",
      "7193/7193 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 153/600\n",
      "7193/7193 [==============================] - 6s 897us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 154/600\n",
      "7193/7193 [==============================] - 6s 894us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 155/600\n",
      "7193/7193 [==============================] - 6s 873us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 156/600\n",
      "7193/7193 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 157/600\n",
      "7193/7193 [==============================] - 6s 847us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 158/600\n",
      "7193/7193 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 159/600\n",
      "7193/7193 [==============================] - 7s 957us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 160/600\n",
      "7193/7193 [==============================] - 6s 854us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 161/600\n",
      "7193/7193 [==============================] - 6s 861us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 162/600\n",
      "7193/7193 [==============================] - 6s 852us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 163/600\n",
      "7193/7193 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 164/600\n",
      "7193/7193 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 165/600\n",
      "7193/7193 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 166/600\n",
      "7193/7193 [==============================] - 7s 994us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 167/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 168/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 169/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 170/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 171/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 172/600\n",
      "7193/7193 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 173/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 174/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 175/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 176/600\n",
      "7193/7193 [==============================] - 6s 835us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 177/600\n",
      "7193/7193 [==============================] - 7s 958us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 178/600\n",
      "7193/7193 [==============================] - 6s 866us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 179/600\n",
      "7193/7193 [==============================] - 7s 954us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 180/600\n",
      "7193/7193 [==============================] - 6s 898us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 181/600\n",
      "7193/7193 [==============================] - 6s 879us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 182/600\n",
      "7193/7193 [==============================] - 7s 920us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 183/600\n",
      "7193/7193 [==============================] - 7s 942us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 184/600\n",
      "7193/7193 [==============================] - 7s 932us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 185/600\n",
      "7193/7193 [==============================] - 7s 990us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 186/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 187/600\n",
      "7193/7193 [==============================] - 7s 963us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 188/600\n",
      "7193/7193 [==============================] - 7s 911us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 189/600\n",
      "7193/7193 [==============================] - 7s 921us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 190/600\n",
      "7193/7193 [==============================] - 7s 926us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 191/600\n",
      "7193/7193 [==============================] - 7s 965us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 192/600\n",
      "7193/7193 [==============================] - 6s 852us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 193/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 194/600\n",
      "7193/7193 [==============================] - 7s 993us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 195/600\n",
      "7193/7193 [==============================] - 7s 959us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 196/600\n",
      "7193/7193 [==============================] - 6s 876us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 197/600\n",
      "7193/7193 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 198/600\n",
      "7193/7193 [==============================] - 7s 907us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 199/600\n",
      "7193/7193 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 200/600\n",
      "7193/7193 [==============================] - 7s 914us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 201/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 202/600\n",
      "7193/7193 [==============================] - 7s 925us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 203/600\n",
      "7193/7193 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 204/600\n",
      "7193/7193 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 205/600\n",
      "7193/7193 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 206/600\n",
      "7193/7193 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 207/600\n",
      "7193/7193 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 208/600\n",
      "7193/7193 [==============================] - 6s 816us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 209/600\n",
      "7193/7193 [==============================] - 6s 858us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 210/600\n",
      "7193/7193 [==============================] - 7s 920us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 211/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 212/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 213/600\n",
      "7193/7193 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 214/600\n",
      "7193/7193 [==============================] - 6s 832us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 215/600\n",
      "7193/7193 [==============================] - 6s 885us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 216/600\n",
      "7193/7193 [==============================] - 6s 876us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 217/600\n",
      "7193/7193 [==============================] - 7s 918us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 218/600\n",
      "7193/7193 [==============================] - 6s 890us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 219/600\n",
      "7193/7193 [==============================] - 7s 950us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 220/600\n",
      "7193/7193 [==============================] - 7s 950us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 221/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 222/600\n",
      "7193/7193 [==============================] - 7s 937us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 223/600\n",
      "7193/7193 [==============================] - 6s 893us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 224/600\n",
      "7193/7193 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 225/600\n",
      "7193/7193 [==============================] - 7s 989us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 226/600\n",
      "7193/7193 [==============================] - 6s 880us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 227/600\n",
      "7193/7193 [==============================] - 7s 989us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 228/600\n",
      "7193/7193 [==============================] - 7s 947us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 229/600\n",
      "7193/7193 [==============================] - 6s 867us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 230/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7193/7193 [==============================] - 7s 905us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 231/600\n",
      "7193/7193 [==============================] - 7s 938us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 232/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 233/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 234/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 235/600\n",
      "7193/7193 [==============================] - 7s 966us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 236/600\n",
      "7193/7193 [==============================] - 6s 858us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 237/600\n",
      "7193/7193 [==============================] - 7s 957us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 238/600\n",
      "7193/7193 [==============================] - 7s 910us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 239/600\n",
      "7193/7193 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 240/600\n",
      "7193/7193 [==============================] - 6s 864us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 241/600\n",
      "7193/7193 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 242/600\n",
      "7193/7193 [==============================] - 7s 923us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 243/600\n",
      "7193/7193 [==============================] - 7s 935us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 244/600\n",
      "7193/7193 [==============================] - 6s 869us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 245/600\n",
      "7193/7193 [==============================] - 7s 944us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 246/600\n",
      "7193/7193 [==============================] - 7s 976us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 247/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 248/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 249/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 250/600\n",
      "7193/7193 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 251/600\n",
      "7193/7193 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 252/600\n",
      "7193/7193 [==============================] - 7s 933us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 253/600\n",
      "7193/7193 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 254/600\n",
      "7193/7193 [==============================] - 6s 816us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 255/600\n",
      "7193/7193 [==============================] - 6s 902us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 256/600\n",
      "7193/7193 [==============================] - 27s 4ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 257/600\n",
      "7193/7193 [==============================] - 6s 785us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 258/600\n",
      "7193/7193 [==============================] - 6s 771us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 259/600\n",
      "7193/7193 [==============================] - 6s 779us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 260/600\n",
      "7193/7193 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 261/600\n",
      "7193/7193 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 262/600\n",
      "7193/7193 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 263/600\n",
      "7193/7193 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 264/600\n",
      "7193/7193 [==============================] - 4s 620us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 265/600\n",
      "7193/7193 [==============================] - 5s 640us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 266/600\n",
      "7193/7193 [==============================] - 1044s 145ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 267/600\n",
      "7193/7193 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 268/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 269/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 270/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 271/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 272/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 273/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 274/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 275/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 276/600\n",
      "7193/7193 [==============================] - 1013s 141ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 277/600\n",
      "7193/7193 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 278/600\n",
      "7193/7193 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 279/600\n",
      "7193/7193 [==============================] - 6s 783us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 280/600\n",
      "7193/7193 [==============================] - 6s 869us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 281/600\n",
      "7193/7193 [==============================] - 7s 922us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 282/600\n",
      "7193/7193 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 283/600\n",
      "7193/7193 [==============================] - 6s 866us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 284/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 285/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 286/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 287/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 288/600\n",
      "7193/7193 [==============================] - 6s 864us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 289/600\n",
      "7193/7193 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 290/600\n",
      "7193/7193 [==============================] - 6s 849us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 291/600\n",
      "7193/7193 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 292/600\n",
      "7193/7193 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 293/600\n",
      "7193/7193 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 294/600\n",
      "7193/7193 [==============================] - 5s 635us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 295/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 296/600\n",
      "7193/7193 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 297/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 298/600\n",
      "7193/7193 [==============================] - 7s 990us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 299/600\n",
      "7193/7193 [==============================] - 6s 857us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 300/600\n",
      "7193/7193 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 301/600\n",
      "7193/7193 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 302/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 303/600\n",
      "7193/7193 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 304/600\n",
      "7193/7193 [==============================] - 6s 871us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 305/600\n",
      "7193/7193 [==============================] - 6s 854us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 306/600\n",
      "7193/7193 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 307/600\n",
      "7193/7193 [==============================] - 7s 967us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 308/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 309/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 310/600\n",
      "7193/7193 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 311/600\n",
      "7193/7193 [==============================] - 7s 920us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 312/600\n",
      "7193/7193 [==============================] - 7s 980us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 313/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 314/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 315/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 316/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 317/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 318/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 319/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 320/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 321/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 322/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 323/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 324/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 325/600\n",
      "7193/7193 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 326/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 327/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 328/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 329/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 330/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 331/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 332/600\n",
      "7193/7193 [==============================] - 7s 906us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 333/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 334/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 335/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 336/600\n",
      "7193/7193 [==============================] - 7s 932us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 337/600\n",
      "7193/7193 [==============================] - 7s 965us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 338/600\n",
      "7193/7193 [==============================] - 6s 888us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 339/600\n",
      "7193/7193 [==============================] - 6s 884us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 340/600\n",
      "7193/7193 [==============================] - 7s 976us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 341/600\n",
      "7193/7193 [==============================] - 7s 915us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 342/600\n",
      "7193/7193 [==============================] - 7s 953us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 343/600\n",
      "7193/7193 [==============================] - 7s 964us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 344/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 345/600\n",
      "7193/7193 [==============================] - 7s 923us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 346/600\n",
      "7193/7193 [==============================] - 7s 909us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 347/600\n",
      "7193/7193 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 348/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 349/600\n",
      "7193/7193 [==============================] - 7s 910us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 350/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 351/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 352/600\n",
      "7193/7193 [==============================] - 6s 857us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 353/600\n",
      "7193/7193 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 354/600\n",
      "7193/7193 [==============================] - 7s 937us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 355/600\n",
      "7193/7193 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 356/600\n",
      "7193/7193 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 357/600\n",
      "7193/7193 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 358/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 359/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 360/600\n",
      "7193/7193 [==============================] - 7s 951us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 361/600\n",
      "7193/7193 [==============================] - 7s 927us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 362/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 363/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 364/600\n",
      "7193/7193 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 365/600\n",
      "7193/7193 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 366/600\n",
      "7193/7193 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 367/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 368/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 369/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 370/600\n",
      "7193/7193 [==============================] - 7s 998us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 371/600\n",
      "7193/7193 [==============================] - 7s 981us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 372/600\n",
      "7193/7193 [==============================] - 7s 918us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 373/600\n",
      "7193/7193 [==============================] - 7s 986us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 374/600\n",
      "7193/7193 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 375/600\n",
      "7193/7193 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 376/600\n",
      "7193/7193 [==============================] - 7s 912us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 377/600\n",
      "7193/7193 [==============================] - 7s 982us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 378/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 379/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 380/600\n",
      "7193/7193 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 381/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 382/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 383/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 384/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 385/600\n",
      "7193/7193 [==============================] - 6s 887us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 386/600\n",
      "7193/7193 [==============================] - 7s 998us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 387/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 388/600\n",
      "7193/7193 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 389/600\n",
      "7193/7193 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 390/600\n",
      "7193/7193 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 391/600\n",
      "7193/7193 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 392/600\n",
      "7193/7193 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 393/600\n",
      "7193/7193 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 394/600\n",
      "7193/7193 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 395/600\n",
      "7193/7193 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 396/600\n",
      "7193/7193 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 397/600\n",
      "7193/7193 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 398/600\n",
      "7193/7193 [==============================] - 6s 783us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 399/600\n",
      "7193/7193 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 400/600\n",
      "7193/7193 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 401/600\n",
      "7193/7193 [==============================] - 6s 862us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 402/600\n",
      "7193/7193 [==============================] - 7s 964us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 403/600\n",
      "7193/7193 [==============================] - 7s 934us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 404/600\n",
      "7193/7193 [==============================] - 7s 970us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 405/600\n",
      "7193/7193 [==============================] - 7s 910us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 406/600\n",
      "7193/7193 [==============================] - 7s 919us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 407/600\n",
      "7193/7193 [==============================] - 7s 948us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 408/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 409/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 410/600\n",
      "7193/7193 [==============================] - 7s 925us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 411/600\n",
      "7193/7193 [==============================] - 7s 960us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 412/600\n",
      "7193/7193 [==============================] - 7s 921us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 413/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 414/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 415/600\n",
      "7193/7193 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 416/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 417/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 418/600\n",
      "7193/7193 [==============================] - 6s 874us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 419/600\n",
      "7193/7193 [==============================] - 7s 924us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 420/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 421/600\n",
      "7193/7193 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 422/600\n",
      "7193/7193 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 423/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 424/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 425/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 426/600\n",
      "7193/7193 [==============================] - 7s 972us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 427/600\n",
      "7193/7193 [==============================] - 7s 950us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 428/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 429/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 430/600\n",
      "7193/7193 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 431/600\n",
      "7193/7193 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 432/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 433/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 434/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 435/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 436/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 437/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 438/600\n",
      "7193/7193 [==============================] - 7s 987us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 439/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 440/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 441/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 442/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 443/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 444/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 445/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 446/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 447/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 448/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 449/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 450/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 451/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 452/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 453/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 454/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 455/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 456/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 457/600\n",
      "7193/7193 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 458/600\n",
      "7193/7193 [==============================] - 7s 999us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/600\n",
      "7193/7193 [==============================] - 7s 994us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 460/600\n",
      "7193/7193 [==============================] - 7s 998us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 461/600\n",
      "7193/7193 [==============================] - 7s 998us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 462/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 463/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 464/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 465/600\n",
      "7193/7193 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 466/600\n",
      "7193/7193 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 467/600\n",
      "7193/7193 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 468/600\n",
      "7193/7193 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 469/600\n",
      "7193/7193 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 470/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 471/600\n",
      "7193/7193 [==============================] - 7s 970us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 472/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 473/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 474/600\n",
      "7193/7193 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 475/600\n",
      "7193/7193 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 476/600\n",
      "7193/7193 [==============================] - 6s 802us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 477/600\n",
      "7193/7193 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 478/600\n",
      "7193/7193 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 479/600\n",
      "7193/7193 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 480/600\n",
      "7193/7193 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 481/600\n",
      "7193/7193 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 482/600\n",
      "7193/7193 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 483/600\n",
      "7193/7193 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 484/600\n",
      "7193/7193 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 485/600\n",
      "7193/7193 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 486/600\n",
      "7193/7193 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 487/600\n",
      "7193/7193 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 488/600\n",
      "7193/7193 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 489/600\n",
      "7193/7193 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 490/600\n",
      "7193/7193 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 491/600\n",
      "7193/7193 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 492/600\n",
      "7193/7193 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 493/600\n",
      "7193/7193 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 494/600\n",
      "7193/7193 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 495/600\n",
      "7193/7193 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 496/600\n",
      "7193/7193 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 497/600\n",
      "7193/7193 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 498/600\n",
      "7193/7193 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 499/600\n",
      "7193/7193 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 500/600\n",
      "7193/7193 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 501/600\n",
      "7193/7193 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 502/600\n",
      "7193/7193 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 503/600\n",
      "7193/7193 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 504/600\n",
      "7193/7193 [==============================] - 6s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 505/600\n",
      "7193/7193 [==============================] - 7s 939us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 506/600\n",
      "7193/7193 [==============================] - 6s 853us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 507/600\n",
      "7193/7193 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 508/600\n",
      "7193/7193 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 509/600\n",
      "7193/7193 [==============================] - 7s 959us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 510/600\n",
      "7193/7193 [==============================] - 6s 801us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 511/600\n",
      "7193/7193 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 512/600\n",
      "7193/7193 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 513/600\n",
      "7193/7193 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 514/600\n",
      "7193/7193 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 515/600\n",
      "7193/7193 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 516/600\n",
      "7193/7193 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 517/600\n",
      "7193/7193 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 518/600\n",
      "7193/7193 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 519/600\n",
      "7193/7193 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 520/600\n",
      "7193/7193 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 521/600\n",
      "7193/7193 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 522/600\n",
      "7193/7193 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 523/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 524/600\n",
      "7193/7193 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 525/600\n",
      "7193/7193 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 526/600\n",
      "7193/7193 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 527/600\n",
      "7193/7193 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 528/600\n",
      "7193/7193 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 529/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 530/600\n",
      "7193/7193 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 531/600\n",
      "7193/7193 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 532/600\n",
      "7193/7193 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 533/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 534/600\n",
      "7193/7193 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 535/600\n",
      "7193/7193 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 536/600\n",
      "7193/7193 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 537/600\n",
      "7193/7193 [==============================] - 5s 717us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 538/600\n",
      "7193/7193 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 539/600\n",
      "7193/7193 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 540/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 541/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 542/600\n",
      "7193/7193 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 543/600\n",
      "7193/7193 [==============================] - 6s 881us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 544/600\n",
      "7193/7193 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 545/600\n",
      "7193/7193 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 546/600\n",
      "7193/7193 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 547/600\n",
      "7193/7193 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 548/600\n",
      "7193/7193 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 549/600\n",
      "7193/7193 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 550/600\n",
      "7193/7193 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 551/600\n",
      "7193/7193 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 552/600\n",
      "7193/7193 [==============================] - 7s 918us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 553/600\n",
      "7193/7193 [==============================] - 6s 886us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 554/600\n",
      "7193/7193 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 555/600\n",
      "7193/7193 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 556/600\n",
      "7193/7193 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 557/600\n",
      "7193/7193 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 558/600\n",
      "7193/7193 [==============================] - 5s 748us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 559/600\n",
      "7193/7193 [==============================] - 6s 860us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 560/600\n",
      "7193/7193 [==============================] - 6s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 561/600\n",
      "7193/7193 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 562/600\n",
      "7193/7193 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 563/600\n",
      "7193/7193 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 564/600\n",
      "7193/7193 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 565/600\n",
      "7193/7193 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 566/600\n",
      "7193/7193 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 567/600\n",
      "7193/7193 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 568/600\n",
      "7193/7193 [==============================] - 7s 949us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 569/600\n",
      "7193/7193 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 570/600\n",
      "7193/7193 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 571/600\n",
      "7193/7193 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 572/600\n",
      "7193/7193 [==============================] - 7s 975us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 573/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7193/7193 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 574/600\n",
      "7193/7193 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 575/600\n",
      "7193/7193 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 576/600\n",
      "7193/7193 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 577/600\n",
      "7193/7193 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 578/600\n",
      "7193/7193 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 579/600\n",
      "7193/7193 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 580/600\n",
      "7193/7193 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 581/600\n",
      "7193/7193 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 582/600\n",
      "7193/7193 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 583/600\n",
      "7193/7193 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 584/600\n",
      "7193/7193 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 585/600\n",
      "7193/7193 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 586/600\n",
      "7193/7193 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 587/600\n",
      "7193/7193 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 588/600\n",
      "7193/7193 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 589/600\n",
      "7193/7193 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 590/600\n",
      "7193/7193 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 591/600\n",
      "7193/7193 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 592/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 593/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 594/600\n",
      "7193/7193 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 595/600\n",
      "7193/7193 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 596/600\n",
      "7193/7193 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 597/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 598/600\n",
      "7193/7193 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 599/600\n",
      "7193/7193 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "Epoch 600/600\n",
      "7193/7193 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 4.3016 - val_acc: 0.6937\n",
      "1256/1256 [==============================] - 0s 79us/step\n",
      "==========================================================================\n",
      "FOLD 1\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261]\n",
      "RF Confusion Matrix: \n",
      "[[704   0]\n",
      " [446 106]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261]\n",
      "SVM Confusion Matrix: \n",
      "[[  0   0   0]\n",
      " [  0 704   0]\n",
      " [266 180 106]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172]\n",
      "DT Confusion Matrix: \n",
      "[[704   0]\n",
      " [180 372]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956]]\n",
      "sNN Confusion Matrix: \n",
      "[[  0   0   0]\n",
      " [  0 704   0]\n",
      " [257   0 295]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 2\n",
      "Train on 7192 samples, validate on 1799 samples\n",
      "Epoch 1/600\n",
      " 128/7192 [..............................] - ETA: 4s - loss: 0.2518 - acc: 0.9844"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192/7192 [==============================] - 5s 703us/step - loss: 0.0217 - acc: 0.9983 - val_loss: 7.9847 - val_acc: 0.4069\n",
      "Epoch 2/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 0.0026 - acc: 0.9997 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 3/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 4/600\n",
      "7192/7192 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 5/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 6/600\n",
      "7192/7192 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 7/600\n",
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 8/600\n",
      "7192/7192 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 9/600\n",
      "7192/7192 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 10/600\n",
      "7192/7192 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 11/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 12/600\n",
      "7192/7192 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 13/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 14/600\n",
      "7192/7192 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 15/600\n",
      "7192/7192 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 16/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 17/600\n",
      "7192/7192 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 18/600\n",
      "7192/7192 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 19/600\n",
      "7192/7192 [==============================] - 5s 748us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 20/600\n",
      "7192/7192 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 21/600\n",
      "7192/7192 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 22/600\n",
      "7192/7192 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 23/600\n",
      "7192/7192 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 24/600\n",
      "7192/7192 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 25/600\n",
      "7192/7192 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 26/600\n",
      "7192/7192 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 27/600\n",
      "7192/7192 [==============================] - 5s 748us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 28/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 29/600\n",
      "7192/7192 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 30/600\n",
      "7192/7192 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 31/600\n",
      "7192/7192 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 32/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 33/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 34/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 35/600\n",
      "7192/7192 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 36/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 37/600\n",
      "7192/7192 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 38/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 39/600\n",
      "7192/7192 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 40/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 41/600\n",
      "7192/7192 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 42/600\n",
      "7192/7192 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 43/600\n",
      "7192/7192 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 44/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 45/600\n",
      "7192/7192 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 46/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 47/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 48/600\n",
      "7192/7192 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 49/600\n",
      "7192/7192 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 50/600\n",
      "7192/7192 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 51/600\n",
      "7192/7192 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 52/600\n",
      "7192/7192 [==============================] - 5s 717us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 53/600\n",
      "7192/7192 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 54/600\n",
      "7192/7192 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 55/600\n",
      "7192/7192 [==============================] - 7s 922us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 56/600\n",
      "7192/7192 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 57/600\n",
      "7192/7192 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 58/600\n",
      "7192/7192 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/600\n",
      "7192/7192 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 60/600\n",
      "7192/7192 [==============================] - 7s 908us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 61/600\n",
      "7192/7192 [==============================] - 6s 851us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 62/600\n",
      "7192/7192 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 63/600\n",
      "7192/7192 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 64/600\n",
      "7192/7192 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 65/600\n",
      "7192/7192 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 66/600\n",
      "7192/7192 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 67/600\n",
      "7192/7192 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 68/600\n",
      "7192/7192 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 69/600\n",
      "7192/7192 [==============================] - 7s 986us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 70/600\n",
      "7192/7192 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 71/600\n",
      "7192/7192 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 72/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 73/600\n",
      "7192/7192 [==============================] - 7s 956us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 74/600\n",
      "7192/7192 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 75/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 76/600\n",
      "7192/7192 [==============================] - 6s 898us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 77/600\n",
      "7192/7192 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 78/600\n",
      "7192/7192 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 79/600\n",
      "7192/7192 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 80/600\n",
      "7192/7192 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 81/600\n",
      "7192/7192 [==============================] - 6s 902us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 82/600\n",
      "7192/7192 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 83/600\n",
      "7192/7192 [==============================] - 6s 899us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 84/600\n",
      "7192/7192 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 85/600\n",
      "7192/7192 [==============================] - 6s 842us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 86/600\n",
      "7192/7192 [==============================] - 7s 971us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 87/600\n",
      "7192/7192 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 88/600\n",
      "7192/7192 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 89/600\n",
      "7192/7192 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 90/600\n",
      "7192/7192 [==============================] - 6s 851us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 91/600\n",
      "7192/7192 [==============================] - 7s 911us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 92/600\n",
      "7192/7192 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 93/600\n",
      "7192/7192 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 94/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 95/600\n",
      "7192/7192 [==============================] - 6s 886us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 96/600\n",
      "7192/7192 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 97/600\n",
      "7192/7192 [==============================] - 7s 921us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 98/600\n",
      "7192/7192 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 99/600\n",
      "7192/7192 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 100/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 101/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 102/600\n",
      "7192/7192 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 103/600\n",
      "7192/7192 [==============================] - 6s 785us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 104/600\n",
      "7192/7192 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 105/600\n",
      "7192/7192 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 106/600\n",
      "7192/7192 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 107/600\n",
      "7192/7192 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 108/600\n",
      "7192/7192 [==============================] - 6s 867us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 109/600\n",
      "7192/7192 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 110/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 111/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 112/600\n",
      "7192/7192 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 113/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 114/600\n",
      "7192/7192 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 115/600\n",
      "7192/7192 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 116/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 117/600\n",
      "7192/7192 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 118/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 119/600\n",
      "7192/7192 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 120/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 121/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 122/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 123/600\n",
      "7192/7192 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 124/600\n",
      "7192/7192 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 125/600\n",
      "7192/7192 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 126/600\n",
      "7192/7192 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 127/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 128/600\n",
      "7192/7192 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 129/600\n",
      "7192/7192 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 130/600\n",
      "7192/7192 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 131/600\n",
      "7192/7192 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 132/600\n",
      "7192/7192 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 133/600\n",
      "7192/7192 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 134/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 135/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 136/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 137/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 138/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 139/600\n",
      "7192/7192 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 140/600\n",
      "7192/7192 [==============================] - 5s 717us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 141/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 142/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 143/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 144/600\n",
      "7192/7192 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 145/600\n",
      "7192/7192 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 146/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 147/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 148/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 149/600\n",
      "7192/7192 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 150/600\n",
      "7192/7192 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 151/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 152/600\n",
      "7192/7192 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 153/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 154/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 155/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 156/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 157/600\n",
      "7192/7192 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 158/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 159/600\n",
      "7192/7192 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 160/600\n",
      "7192/7192 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 161/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 162/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 163/600\n",
      "7192/7192 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 164/600\n",
      "7192/7192 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 165/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 166/600\n",
      "7192/7192 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 167/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 168/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 169/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 170/600\n",
      "7192/7192 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 171/600\n",
      "7192/7192 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 172/600\n",
      "7192/7192 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 173/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192/7192 [==============================] - 7s 920us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 174/600\n",
      "7192/7192 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 175/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 176/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 177/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 178/600\n",
      "7192/7192 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 179/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 180/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 181/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 182/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 183/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 184/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 185/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 186/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 187/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 188/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 189/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 190/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 191/600\n",
      "7192/7192 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 192/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 193/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 194/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 195/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 196/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 197/600\n",
      "7192/7192 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 198/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 199/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 200/600\n",
      "7192/7192 [==============================] - 7s 924us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 201/600\n",
      "7192/7192 [==============================] - 7s 947us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 202/600\n",
      "7192/7192 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 203/600\n",
      "7192/7192 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 204/600\n",
      "7192/7192 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 205/600\n",
      "7192/7192 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 206/600\n",
      "7192/7192 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 207/600\n",
      "7192/7192 [==============================] - 6s 850us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 208/600\n",
      "7192/7192 [==============================] - 6s 878us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 209/600\n",
      "7192/7192 [==============================] - 7s 962us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 210/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 211/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 212/600\n",
      "7192/7192 [==============================] - 7s 998us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 213/600\n",
      "7192/7192 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 214/600\n",
      "7192/7192 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 215/600\n",
      "7192/7192 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 216/600\n",
      "7192/7192 [==============================] - 6s 892us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 217/600\n",
      "7192/7192 [==============================] - 7s 938us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 218/600\n",
      "7192/7192 [==============================] - 7s 930us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 219/600\n",
      "7192/7192 [==============================] - 7s 916us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 220/600\n",
      "7192/7192 [==============================] - 6s 899us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 221/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 222/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 223/600\n",
      "7192/7192 [==============================] - 6s 833us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 224/600\n",
      "7192/7192 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 225/600\n",
      "7192/7192 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 226/600\n",
      "7192/7192 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 227/600\n",
      "7192/7192 [==============================] - 6s 815us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 228/600\n",
      "7192/7192 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 229/600\n",
      "7192/7192 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 230/600\n",
      "7192/7192 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 231/600\n",
      "7192/7192 [==============================] - 7s 938us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 232/600\n",
      "7192/7192 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 233/600\n",
      "7192/7192 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 234/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 235/600\n",
      "7192/7192 [==============================] - 6s 842us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 236/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 237/600\n",
      "7192/7192 [==============================] - 7s 975us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 238/600\n",
      "7192/7192 [==============================] - 7s 947us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 239/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 240/600\n",
      "7192/7192 [==============================] - 7s 995us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 241/600\n",
      "7192/7192 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 242/600\n",
      "7192/7192 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 243/600\n",
      "7192/7192 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 244/600\n",
      "7192/7192 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 245/600\n",
      "7192/7192 [==============================] - 7s 958us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 246/600\n",
      "7192/7192 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 247/600\n",
      "7192/7192 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 248/600\n",
      "7192/7192 [==============================] - 6s 792us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 249/600\n",
      "7192/7192 [==============================] - 6s 892us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 250/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 251/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 252/600\n",
      "7192/7192 [==============================] - 7s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 253/600\n",
      "7192/7192 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 254/600\n",
      "7192/7192 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 255/600\n",
      "7192/7192 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 256/600\n",
      "7192/7192 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 257/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 258/600\n",
      "7192/7192 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 259/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 260/600\n",
      "7192/7192 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 261/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 262/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 263/600\n",
      "7192/7192 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 264/600\n",
      "7192/7192 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 265/600\n",
      "7192/7192 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 266/600\n",
      "7192/7192 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 267/600\n",
      "7192/7192 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 268/600\n",
      "7192/7192 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 269/600\n",
      "7192/7192 [==============================] - 7s 926us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 270/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 271/600\n",
      "7192/7192 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 272/600\n",
      "7192/7192 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 273/600\n",
      "7192/7192 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 274/600\n",
      "7192/7192 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 275/600\n",
      "7192/7192 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 276/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 277/600\n",
      "7192/7192 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 278/600\n",
      "7192/7192 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 279/600\n",
      "7192/7192 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 280/600\n",
      "7192/7192 [==============================] - 6s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 281/600\n",
      "7192/7192 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 282/600\n",
      "7192/7192 [==============================] - 6s 890us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 283/600\n",
      "7192/7192 [==============================] - 5s 763us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 284/600\n",
      "7192/7192 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 285/600\n",
      "7192/7192 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 286/600\n",
      "7192/7192 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 287/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 288/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 289/600\n",
      "7192/7192 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 290/600\n",
      "7192/7192 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 291/600\n",
      "7192/7192 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 292/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 293/600\n",
      "7192/7192 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 294/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 295/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 296/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 297/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 298/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 299/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 300/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 301/600\n",
      "7192/7192 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 302/600\n",
      "7192/7192 [==============================] - 7s 959us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 303/600\n",
      "7192/7192 [==============================] - 6s 840us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 304/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 305/600\n",
      "7192/7192 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 306/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 307/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 308/600\n",
      "7192/7192 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 309/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 310/600\n",
      "7192/7192 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 311/600\n",
      "7192/7192 [==============================] - 6s 792us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 312/600\n",
      "7192/7192 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 313/600\n",
      "7192/7192 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 314/600\n",
      "7192/7192 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 315/600\n",
      "7192/7192 [==============================] - 7s 945us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 316/600\n",
      "7192/7192 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 317/600\n",
      "7192/7192 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 318/600\n",
      "7192/7192 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 319/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 320/600\n",
      "7192/7192 [==============================] - 6s 837us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 321/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 322/600\n",
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 323/600\n",
      "7192/7192 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 324/600\n",
      "7192/7192 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 325/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 326/600\n",
      "7192/7192 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 327/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 328/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 329/600\n",
      "7192/7192 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 330/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 331/600\n",
      "7192/7192 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 332/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 333/600\n",
      "7192/7192 [==============================] - 7s 942us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 334/600\n",
      "7192/7192 [==============================] - 6s 873us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 335/600\n",
      "7192/7192 [==============================] - 6s 840us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 336/600\n",
      "7192/7192 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 337/600\n",
      "7192/7192 [==============================] - 6s 849us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 338/600\n",
      "7192/7192 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 339/600\n",
      "7192/7192 [==============================] - 6s 862us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 340/600\n",
      "7192/7192 [==============================] - 7s 991us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 341/600\n",
      "7192/7192 [==============================] - 6s 815us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 342/600\n",
      "7192/7192 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 343/600\n",
      "7192/7192 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 344/600\n",
      "7192/7192 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 345/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 346/600\n",
      "7192/7192 [==============================] - 7s 929us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 347/600\n",
      "7192/7192 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 348/600\n",
      "7192/7192 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 349/600\n",
      "7192/7192 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 350/600\n",
      "7192/7192 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 351/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 352/600\n",
      "7192/7192 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 353/600\n",
      "7192/7192 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 354/600\n",
      "7192/7192 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 355/600\n",
      "7192/7192 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 356/600\n",
      "7192/7192 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 357/600\n",
      "7192/7192 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 358/600\n",
      "7192/7192 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 359/600\n",
      "7192/7192 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 360/600\n",
      "7192/7192 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 361/600\n",
      "7192/7192 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 362/600\n",
      "7192/7192 [==============================] - 6s 854us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 363/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 364/600\n",
      "7192/7192 [==============================] - 5s 763us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 365/600\n",
      "7192/7192 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 366/600\n",
      "7192/7192 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 367/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 368/600\n",
      "7192/7192 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 369/600\n",
      "7192/7192 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 370/600\n",
      "7192/7192 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 371/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 372/600\n",
      "7192/7192 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 373/600\n",
      "7192/7192 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 374/600\n",
      "7192/7192 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 375/600\n",
      "7192/7192 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 376/600\n",
      "7192/7192 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 377/600\n",
      "7192/7192 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 378/600\n",
      "7192/7192 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 379/600\n",
      "7192/7192 [==============================] - 7s 959us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 380/600\n",
      "7192/7192 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 381/600\n",
      "7192/7192 [==============================] - 7s 936us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 382/600\n",
      "7192/7192 [==============================] - 7s 909us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 383/600\n",
      "7192/7192 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 384/600\n",
      "7192/7192 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 385/600\n",
      "7192/7192 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 386/600\n",
      "7192/7192 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 387/600\n",
      "7192/7192 [==============================] - 6s 798us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 388/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 389/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 390/600\n",
      "7192/7192 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 391/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 392/600\n",
      "7192/7192 [==============================] - 6s 869us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 393/600\n",
      "7192/7192 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 394/600\n",
      "7192/7192 [==============================] - 6s 871us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 395/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 396/600\n",
      "7192/7192 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 397/600\n",
      "7192/7192 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 398/600\n",
      "7192/7192 [==============================] - 6s 900us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 399/600\n",
      "7192/7192 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 400/600\n",
      "7192/7192 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 401/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192/7192 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 402/600\n",
      "7192/7192 [==============================] - 6s 895us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 403/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 404/600\n",
      "7192/7192 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 405/600\n",
      "7192/7192 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 406/600\n",
      "7192/7192 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 407/600\n",
      "7192/7192 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 408/600\n",
      "7192/7192 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 409/600\n",
      "7192/7192 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 410/600\n",
      "7192/7192 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 411/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 412/600\n",
      "7192/7192 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 413/600\n",
      "7192/7192 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 414/600\n",
      "7192/7192 [==============================] - 6s 771us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 415/600\n",
      "7192/7192 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 416/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 417/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 418/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 419/600\n",
      "7192/7192 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 420/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 421/600\n",
      "7192/7192 [==============================] - 6s 897us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 422/600\n",
      "7192/7192 [==============================] - 7s 951us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 423/600\n",
      "7192/7192 [==============================] - 7s 988us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 424/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 425/600\n",
      "7192/7192 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 426/600\n",
      "7192/7192 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 427/600\n",
      "7192/7192 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 428/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 429/600\n",
      "7192/7192 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 430/600\n",
      "7192/7192 [==============================] - 6s 859us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 431/600\n",
      "7192/7192 [==============================] - 7s 957us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 432/600\n",
      "7192/7192 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 433/600\n",
      "7192/7192 [==============================] - 6s 798us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 434/600\n",
      "7192/7192 [==============================] - 7s 926us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 435/600\n",
      "7192/7192 [==============================] - 7s 986us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 436/600\n",
      "7192/7192 [==============================] - 7s 908us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 437/600\n",
      "7192/7192 [==============================] - 7s 965us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 438/600\n",
      "7192/7192 [==============================] - 6s 837us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 439/600\n",
      "7192/7192 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 440/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 441/600\n",
      "7192/7192 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 442/600\n",
      "7192/7192 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 443/600\n",
      "7192/7192 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 444/600\n",
      "7192/7192 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 445/600\n",
      "7192/7192 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 446/600\n",
      "7192/7192 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 447/600\n",
      "7192/7192 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 448/600\n",
      "7192/7192 [==============================] - 5s 748us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 449/600\n",
      "7192/7192 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 450/600\n",
      "7192/7192 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 451/600\n",
      "7192/7192 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 452/600\n",
      "7192/7192 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 453/600\n",
      "7192/7192 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 454/600\n",
      "7192/7192 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 455/600\n",
      "7192/7192 [==============================] - 7s 997us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 456/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 457/600\n",
      "7192/7192 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 458/600\n",
      "7192/7192 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 459/600\n",
      "7192/7192 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 460/600\n",
      "7192/7192 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 461/600\n",
      "7192/7192 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 462/600\n",
      "7192/7192 [==============================] - 6s 771us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 463/600\n",
      "7192/7192 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 464/600\n",
      "7192/7192 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 465/600\n",
      "7192/7192 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 466/600\n",
      "7192/7192 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 467/600\n",
      "7192/7192 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 468/600\n",
      "7192/7192 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 469/600\n",
      "7192/7192 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 470/600\n",
      "7192/7192 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 471/600\n",
      "7192/7192 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 472/600\n",
      "7192/7192 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 473/600\n",
      "7192/7192 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 474/600\n",
      "7192/7192 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 475/600\n",
      "7192/7192 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 476/600\n",
      "7192/7192 [==============================] - 5s 763us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 477/600\n",
      "7192/7192 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 478/600\n",
      "7192/7192 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 479/600\n",
      "7192/7192 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 480/600\n",
      "7192/7192 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 481/600\n",
      "7192/7192 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 482/600\n",
      "7192/7192 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 483/600\n",
      "7192/7192 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 484/600\n",
      "7192/7192 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 485/600\n",
      "7192/7192 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 486/600\n",
      "7192/7192 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 487/600\n",
      "7192/7192 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 488/600\n",
      "7192/7192 [==============================] - 6s 785us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 489/600\n",
      "7192/7192 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 490/600\n",
      "7192/7192 [==============================] - 7s 912us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 491/600\n",
      "7192/7192 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 492/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 493/600\n",
      "7192/7192 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 494/600\n",
      "7192/7192 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 495/600\n",
      "7192/7192 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 496/600\n",
      "7192/7192 [==============================] - 6s 771us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 497/600\n",
      "7192/7192 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 498/600\n",
      "7192/7192 [==============================] - 7s 905us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 499/600\n",
      "7192/7192 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 500/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 501/600\n",
      "7192/7192 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 502/600\n",
      "7192/7192 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 503/600\n",
      "7192/7192 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 504/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 505/600\n",
      "7192/7192 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 506/600\n",
      "7192/7192 [==============================] - 6s 798us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 507/600\n",
      "7192/7192 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 508/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 509/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 510/600\n",
      "7192/7192 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 511/600\n",
      "7192/7192 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 512/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 513/600\n",
      "7192/7192 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 514/600\n",
      "7192/7192 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 515/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7192/7192 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 516/600\n",
      "7192/7192 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 517/600\n",
      "7192/7192 [==============================] - 6s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 518/600\n",
      "7192/7192 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 519/600\n",
      "7192/7192 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 520/600\n",
      "7192/7192 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 521/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 522/600\n",
      "7192/7192 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 523/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 524/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 525/600\n",
      "7192/7192 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 526/600\n",
      "7192/7192 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 527/600\n",
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 528/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 529/600\n",
      "7192/7192 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 530/600\n",
      "7192/7192 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 531/600\n",
      "7192/7192 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 532/600\n",
      "7192/7192 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 533/600\n",
      "7192/7192 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 534/600\n",
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 535/600\n",
      "7192/7192 [==============================] - 6s 866us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 536/600\n",
      "7192/7192 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 537/600\n",
      "7192/7192 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 538/600\n",
      "7192/7192 [==============================] - 7s 990us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 539/600\n",
      "7192/7192 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 540/600\n",
      "7192/7192 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 541/600\n",
      "7192/7192 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 542/600\n",
      "7192/7192 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 543/600\n",
      "7192/7192 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 544/600\n",
      "7192/7192 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 545/600\n",
      "7192/7192 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 546/600\n",
      "7192/7192 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 547/600\n",
      "7192/7192 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 548/600\n",
      "7192/7192 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 549/600\n",
      "7192/7192 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 550/600\n",
      "7192/7192 [==============================] - 6s 845us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 551/600\n",
      "7192/7192 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 552/600\n",
      "7192/7192 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 553/600\n",
      "7192/7192 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 554/600\n",
      "7192/7192 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 555/600\n",
      "7192/7192 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 556/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 557/600\n",
      "7192/7192 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 558/600\n",
      "7192/7192 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 559/600\n",
      "7192/7192 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 560/600\n",
      "7192/7192 [==============================] - 7s 947us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 561/600\n",
      "7192/7192 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 562/600\n",
      "7192/7192 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 563/600\n",
      "7192/7192 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 564/600\n",
      "7192/7192 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 565/600\n",
      "7192/7192 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 566/600\n",
      "7192/7192 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 567/600\n",
      "7192/7192 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 568/600\n",
      "7192/7192 [==============================] - 5s 717us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 569/600\n",
      "7192/7192 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 570/600\n",
      "7192/7192 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 571/600\n",
      "7192/7192 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 572/600\n",
      "7192/7192 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 573/600\n",
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 574/600\n",
      "7192/7192 [==============================] - 6s 840us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 575/600\n",
      "7192/7192 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 576/600\n",
      "7192/7192 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 577/600\n",
      "7192/7192 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 578/600\n",
      "7192/7192 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 579/600\n",
      "7192/7192 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 580/600\n",
      "7192/7192 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 581/600\n",
      "7192/7192 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 582/600\n",
      "7192/7192 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 583/600\n",
      "7192/7192 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 584/600\n",
      "7192/7192 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 585/600\n",
      "7192/7192 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 586/600\n",
      "7192/7192 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 587/600\n",
      "7192/7192 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 588/600\n",
      "7192/7192 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 589/600\n",
      "7192/7192 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 590/600\n",
      "7192/7192 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 591/600\n",
      "7192/7192 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 592/600\n",
      "7192/7192 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 593/600\n",
      "7192/7192 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 594/600\n",
      "7192/7192 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 595/600\n",
      "7192/7192 [==============================] - 6s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 596/600\n",
      "7192/7192 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 597/600\n",
      "7192/7192 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 598/600\n",
      "7192/7192 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 599/600\n",
      "7192/7192 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "Epoch 600/600\n",
      "7192/7192 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.2786 - val_acc: 0.6359\n",
      "1257/1257 [==============================] - 0s 80us/step\n",
      "==========================================================================\n",
      "FOLD 2\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214]\n",
      "RF Confusion Matrix: \n",
      "[[199 273   0]\n",
      " [  0   0 110]\n",
      " [  0 542 133]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319]\n",
      "SVM Confusion Matrix: \n",
      "[[  0 472   0]\n",
      " [  0 110   0]\n",
      " [  0   0 675]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319]\n",
      "DT Confusion Matrix: \n",
      "[[  0 472   0]\n",
      " [  0 110   0]\n",
      " [  0   0 675]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355]]\n",
      "sNN Confusion Matrix: \n",
      "[[459   0  13]\n",
      " [  0 110   0]\n",
      " [  0   0 675]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 3\n",
      "Train on 7164 samples, validate on 1792 samples\n",
      "Epoch 1/600\n",
      " 128/7164 [..............................] - ETA: 4s - loss: 1.1921e-07 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 697us/step - loss: 0.0126 - acc: 0.9983 - val_loss: 4.0449 - val_acc: 0.7210\n",
      "Epoch 2/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 1.5611e-07 - acc: 1.0000 - val_loss: 5.0005 - val_acc: 0.6590\n",
      "Epoch 3/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0095 - acc: 0.9993 - val_loss: 3.4334 - val_acc: 0.7723\n",
      "Epoch 4/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0026 - acc: 0.9997 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 5/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 6/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 7/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 8/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 9/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 10/600\n",
      "7164/7164 [==============================] - 5s 745us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 11/600\n",
      "7164/7164 [==============================] - 5s 748us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 12/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 13/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 14/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 15/600\n",
      "7164/7164 [==============================] - 5s 724us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 16/600\n",
      "7164/7164 [==============================] - 5s 754us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 17/600\n",
      "7164/7164 [==============================] - 6s 778us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 18/600\n",
      "7164/7164 [==============================] - 6s 886us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 19/600\n",
      "7164/7164 [==============================] - 6s 784us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 20/600\n",
      "7164/7164 [==============================] - 5s 766us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 21/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 22/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 23/600\n",
      "7164/7164 [==============================] - 5s 710us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 24/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 25/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 26/600\n",
      "7164/7164 [==============================] - 5s 729us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 27/600\n",
      "7164/7164 [==============================] - 6s 784us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 28/600\n",
      "7164/7164 [==============================] - 6s 803us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 29/600\n",
      "7164/7164 [==============================] - 5s 715us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 30/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 31/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 32/600\n",
      "7164/7164 [==============================] - 5s 689us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 33/600\n",
      "7164/7164 [==============================] - 5s 685us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 34/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 35/600\n",
      "7164/7164 [==============================] - 6s 876us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 36/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 37/600\n",
      "7164/7164 [==============================] - 7s 944us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 38/600\n",
      "7164/7164 [==============================] - 5s 721us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 39/600\n",
      "7164/7164 [==============================] - 6s 852us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 40/600\n",
      "7164/7164 [==============================] - 6s 881us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 41/600\n",
      "7164/7164 [==============================] - 6s 826us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 42/600\n",
      "7164/7164 [==============================] - 5s 682us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 43/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 44/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 45/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 46/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 47/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 48/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 49/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 50/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 51/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 52/600\n",
      "7164/7164 [==============================] - 5s 716us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 53/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 54/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 55/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 56/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 57/600\n",
      "7164/7164 [==============================] - 5s 681us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 58/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 59/600\n",
      "7164/7164 [==============================] - 5s 717us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 60/600\n",
      "7164/7164 [==============================] - 5s 668us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 61/600\n",
      "7164/7164 [==============================] - 6s 817us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 62/600\n",
      "7164/7164 [==============================] - 7s 931us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 63/600\n",
      "7164/7164 [==============================] - 5s 725us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 64/600\n",
      "7164/7164 [==============================] - 5s 712us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 65/600\n",
      "7164/7164 [==============================] - 6s 798us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 66/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 67/600\n",
      "7164/7164 [==============================] - 5s 723us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 68/600\n",
      "7164/7164 [==============================] - 5s 740us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 69/600\n",
      "7164/7164 [==============================] - 6s 880us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 70/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 71/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 72/600\n",
      "7164/7164 [==============================] - 5s 737us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 73/600\n",
      "7164/7164 [==============================] - 7s 951us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 74/600\n",
      "7164/7164 [==============================] - 7s 924us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 75/600\n",
      "7164/7164 [==============================] - 5s 706us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 76/600\n",
      "7164/7164 [==============================] - 6s 878us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 77/600\n",
      "7164/7164 [==============================] - 7s 917us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 78/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 79/600\n",
      "7164/7164 [==============================] - 6s 862us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 80/600\n",
      "7164/7164 [==============================] - 7s 940us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 81/600\n",
      "7164/7164 [==============================] - 7s 963us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 82/600\n",
      "7164/7164 [==============================] - 6s 882us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 83/600\n",
      "7164/7164 [==============================] - 5s 744us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 84/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 85/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 86/600\n",
      "7164/7164 [==============================] - 6s 864us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 87/600\n",
      "7164/7164 [==============================] - 5s 696us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 88/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 89/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 90/600\n",
      "7164/7164 [==============================] - 5s 700us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 91/600\n",
      "7164/7164 [==============================] - 5s 765us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 92/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 93/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 94/600\n",
      "7164/7164 [==============================] - 6s 793us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 95/600\n",
      "7164/7164 [==============================] - 5s 721us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 96/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 97/600\n",
      "7164/7164 [==============================] - 6s 798us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 98/600\n",
      "7164/7164 [==============================] - 5s 766us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 99/600\n",
      "7164/7164 [==============================] - 6s 786us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 100/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 101/600\n",
      "7164/7164 [==============================] - 6s 844us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 102/600\n",
      "7164/7164 [==============================] - 6s 884us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 103/600\n",
      "7164/7164 [==============================] - 6s 769us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 104/600\n",
      "7164/7164 [==============================] - 7s 914us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 105/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 106/600\n",
      "7164/7164 [==============================] - 6s 866us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 107/600\n",
      "7164/7164 [==============================] - 6s 892us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 108/600\n",
      "7164/7164 [==============================] - 6s 845us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 109/600\n",
      "7164/7164 [==============================] - 6s 820us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 110/600\n",
      "7164/7164 [==============================] - 6s 852us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 111/600\n",
      "7164/7164 [==============================] - 6s 869us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 112/600\n",
      "7164/7164 [==============================] - 6s 857us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 113/600\n",
      "7164/7164 [==============================] - 6s 882us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 114/600\n",
      "7164/7164 [==============================] - 6s 862us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 115/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 116/600\n",
      "7164/7164 [==============================] - 6s 864us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 117/600\n",
      "7164/7164 [==============================] - 6s 883us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 118/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 119/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 120/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 121/600\n",
      "7164/7164 [==============================] - 5s 722us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 122/600\n",
      "7164/7164 [==============================] - 6s 832us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 123/600\n",
      "7164/7164 [==============================] - 5s 739us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 124/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 125/600\n",
      "7164/7164 [==============================] - 6s 793us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 126/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 127/600\n",
      "7164/7164 [==============================] - 6s 842us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 128/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 129/600\n",
      "7164/7164 [==============================] - 6s 769us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 130/600\n",
      "7164/7164 [==============================] - 6s 784us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 131/600\n",
      "7164/7164 [==============================] - 6s 828us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 132/600\n",
      "7164/7164 [==============================] - 6s 821us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 133/600\n",
      "7164/7164 [==============================] - 5s 754us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 134/600\n",
      "7164/7164 [==============================] - 6s 780us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 135/600\n",
      "7164/7164 [==============================] - 5s 718us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 136/600\n",
      "7164/7164 [==============================] - 5s 668us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 137/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 138/600\n",
      "7164/7164 [==============================] - 5s 741us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 139/600\n",
      "7164/7164 [==============================] - 5s 720us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 140/600\n",
      "7164/7164 [==============================] - 6s 824us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 141/600\n",
      "7164/7164 [==============================] - 6s 816us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 142/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 143/600\n",
      "7164/7164 [==============================] - 5s 655us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 144/600\n",
      "7164/7164 [==============================] - 5s 716us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 145/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 146/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 147/600\n",
      "7164/7164 [==============================] - 6s 788us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 148/600\n",
      "7164/7164 [==============================] - 6s 837us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 149/600\n",
      "7164/7164 [==============================] - 5s 753us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 150/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 151/600\n",
      "7164/7164 [==============================] - 7s 919us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 152/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 153/600\n",
      "7164/7164 [==============================] - 6s 863us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 154/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 155/600\n",
      "7164/7164 [==============================] - 5s 763us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 156/600\n",
      "7164/7164 [==============================] - 5s 743us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 157/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 158/600\n",
      "7164/7164 [==============================] - 6s 805us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 159/600\n",
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 160/600\n",
      "7164/7164 [==============================] - 6s 810us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 161/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 162/600\n",
      "7164/7164 [==============================] - 6s 805us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 163/600\n",
      "7164/7164 [==============================] - 6s 782us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 164/600\n",
      "7164/7164 [==============================] - 6s 895us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 165/600\n",
      "7164/7164 [==============================] - 5s 749us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 166/600\n",
      "7164/7164 [==============================] - 6s 803us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 167/600\n",
      "7164/7164 [==============================] - 6s 771us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 168/600\n",
      "7164/7164 [==============================] - 6s 788us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 169/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 170/600\n",
      "7164/7164 [==============================] - 7s 938us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 171/600\n",
      "7164/7164 [==============================] - 7s 985us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 172/600\n",
      "7164/7164 [==============================] - 6s 861us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 173/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 174/600\n",
      "7164/7164 [==============================] - 5s 748us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 175/600\n",
      "7164/7164 [==============================] - 6s 874us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 176/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 177/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 178/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 6s 768us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 179/600\n",
      "7164/7164 [==============================] - 6s 808us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 180/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 181/600\n",
      "7164/7164 [==============================] - 6s 797us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 182/600\n",
      "7164/7164 [==============================] - 6s 795us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 183/600\n",
      "7164/7164 [==============================] - 6s 795us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 184/600\n",
      "7164/7164 [==============================] - 6s 851us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 185/600\n",
      "7164/7164 [==============================] - 6s 779us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 186/600\n",
      "7164/7164 [==============================] - 6s 847us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 187/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 188/600\n",
      "7164/7164 [==============================] - 5s 686us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 189/600\n",
      "7164/7164 [==============================] - 6s 817us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 190/600\n",
      "7164/7164 [==============================] - 5s 722us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 191/600\n",
      "7164/7164 [==============================] - 5s 739us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 192/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 193/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 194/600\n",
      "7164/7164 [==============================] - 5s 744us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 195/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 196/600\n",
      "7164/7164 [==============================] - 9s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 197/600\n",
      "7164/7164 [==============================] - 7s 961us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 198/600\n",
      "7164/7164 [==============================] - 5s 739us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 199/600\n",
      "7164/7164 [==============================] - 5s 716us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 200/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 201/600\n",
      "7164/7164 [==============================] - 5s 710us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 202/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 203/600\n",
      "7164/7164 [==============================] - 5s 699us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 204/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 205/600\n",
      "7164/7164 [==============================] - 6s 867us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 206/600\n",
      "7164/7164 [==============================] - 6s 842us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 207/600\n",
      "7164/7164 [==============================] - 6s 780us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 208/600\n",
      "7164/7164 [==============================] - 6s 872us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 209/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 210/600\n",
      "7164/7164 [==============================] - 6s 823us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 211/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 212/600\n",
      "7164/7164 [==============================] - 5s 741us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 213/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 214/600\n",
      "7164/7164 [==============================] - 6s 806us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 215/600\n",
      "7164/7164 [==============================] - 6s 856us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 216/600\n",
      "7164/7164 [==============================] - 7s 923us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 217/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 218/600\n",
      "7164/7164 [==============================] - 6s 812us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 219/600\n",
      "7164/7164 [==============================] - 6s 783us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 220/600\n",
      "7164/7164 [==============================] - 5s 718us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 221/600\n",
      "7164/7164 [==============================] - 5s 726us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 222/600\n",
      "7164/7164 [==============================] - 7s 944us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 223/600\n",
      "7164/7164 [==============================] - 5s 704us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 224/600\n",
      "7164/7164 [==============================] - 7s 975us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 225/600\n",
      "7164/7164 [==============================] - 6s 786us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 226/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 227/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 228/600\n",
      "7164/7164 [==============================] - 5s 707us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 229/600\n",
      "7164/7164 [==============================] - 5s 704us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 230/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 231/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 232/600\n",
      "7164/7164 [==============================] - 5s 693us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 233/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 234/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 235/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 236/600\n",
      "7164/7164 [==============================] - 5s 697us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 237/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 692us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 238/600\n",
      "7164/7164 [==============================] - 5s 689us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 239/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 240/600\n",
      "7164/7164 [==============================] - 5s 706us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 241/600\n",
      "7164/7164 [==============================] - 5s 745us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 242/600\n",
      "7164/7164 [==============================] - 5s 713us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 243/600\n",
      "7164/7164 [==============================] - 5s 703us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 244/600\n",
      "7164/7164 [==============================] - 6s 853us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 245/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 246/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 247/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 248/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 249/600\n",
      "7164/7164 [==============================] - 5s 690us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 250/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 251/600\n",
      "7164/7164 [==============================] - 5s 689us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 252/600\n",
      "7164/7164 [==============================] - 6s 802us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 253/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 254/600\n",
      "7164/7164 [==============================] - 5s 703us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 255/600\n",
      "7164/7164 [==============================] - 5s 692us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 256/600\n",
      "7164/7164 [==============================] - 5s 687us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 257/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 258/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 259/600\n",
      "7164/7164 [==============================] - 5s 729us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 260/600\n",
      "7164/7164 [==============================] - 6s 866us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 261/600\n",
      "7164/7164 [==============================] - 6s 770us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 262/600\n",
      "7164/7164 [==============================] - 12s 2ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 263/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 264/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 265/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 266/600\n",
      "7164/7164 [==============================] - 6s 796us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 267/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 268/600\n",
      "7164/7164 [==============================] - 5s 713us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 269/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 270/600\n",
      "7164/7164 [==============================] - 5s 719us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 271/600\n",
      "7164/7164 [==============================] - 5s 714us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 272/600\n",
      "7164/7164 [==============================] - 5s 718us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 273/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 274/600\n",
      "7164/7164 [==============================] - 5s 722us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 275/600\n",
      "7164/7164 [==============================] - 5s 721us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 276/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 277/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 278/600\n",
      "7164/7164 [==============================] - 6s 782us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 279/600\n",
      "7164/7164 [==============================] - 7s 958us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 280/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 281/600\n",
      "7164/7164 [==============================] - 6s 862us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 282/600\n",
      "7164/7164 [==============================] - 7s 943us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 283/600\n",
      "7164/7164 [==============================] - 7s 929us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 284/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 285/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 286/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 287/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 288/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 289/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 290/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 291/600\n",
      "7164/7164 [==============================] - 5s 684us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 292/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 293/600\n",
      "7164/7164 [==============================] - 5s 682us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 294/600\n",
      "7164/7164 [==============================] - 5s 690us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 295/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 296/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 297/600\n",
      "7164/7164 [==============================] - 5s 665us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 298/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 299/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 300/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 301/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 302/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 303/600\n",
      "7164/7164 [==============================] - 5s 668us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 304/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 305/600\n",
      "7164/7164 [==============================] - 5s 692us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 306/600\n",
      "7164/7164 [==============================] - 5s 681us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 307/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 308/600\n",
      "7164/7164 [==============================] - 5s 697us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 309/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 310/600\n",
      "7164/7164 [==============================] - 5s 666us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 311/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 312/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 313/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 314/600\n",
      "7164/7164 [==============================] - 5s 681us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 315/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 316/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 317/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 318/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 319/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 320/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 321/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 322/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 323/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 324/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 325/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 326/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 327/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 328/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 329/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 330/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 331/600\n",
      "7164/7164 [==============================] - 6s 839us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 332/600\n",
      "7164/7164 [==============================] - 5s 713us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 333/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 334/600\n",
      "7164/7164 [==============================] - 6s 856us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 335/600\n",
      "7164/7164 [==============================] - 6s 862us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 336/600\n",
      "7164/7164 [==============================] - 5s 709us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 337/600\n",
      "7164/7164 [==============================] - 5s 690us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 338/600\n",
      "7164/7164 [==============================] - 5s 686us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 339/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 340/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 341/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 342/600\n",
      "7164/7164 [==============================] - 5s 681us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 343/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 344/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 345/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 346/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 347/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 348/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 349/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 350/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 351/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 352/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 353/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 354/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 355/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 356/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 357/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 358/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 359/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 360/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 361/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 362/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 363/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 364/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 365/600\n",
      "7164/7164 [==============================] - 5s 687us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 366/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 367/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 368/600\n",
      "7164/7164 [==============================] - 5s 709us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 369/600\n",
      "7164/7164 [==============================] - 5s 689us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 370/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 371/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 372/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 373/600\n",
      "7164/7164 [==============================] - 5s 668us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 374/600\n",
      "7164/7164 [==============================] - 5s 660us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 375/600\n",
      "7164/7164 [==============================] - 5s 667us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 376/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 377/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 378/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 379/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 380/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 381/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 382/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 383/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 384/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 385/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 386/600\n",
      "7164/7164 [==============================] - 5s 682us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 387/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 388/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 389/600\n",
      "7164/7164 [==============================] - 5s 667us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 390/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 391/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 392/600\n",
      "7164/7164 [==============================] - 5s 668us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 393/600\n",
      "7164/7164 [==============================] - 5s 682us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 394/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 395/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 396/600\n",
      "7164/7164 [==============================] - 5s 709us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 397/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 398/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 399/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 400/600\n",
      "7164/7164 [==============================] - 7s 929us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 401/600\n",
      "7164/7164 [==============================] - 6s 844us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 402/600\n",
      "7164/7164 [==============================] - 6s 905us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 403/600\n",
      "7164/7164 [==============================] - 7s 994us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 404/600\n",
      "7164/7164 [==============================] - 6s 835us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 405/600\n",
      "7164/7164 [==============================] - 7s 953us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 406/600\n",
      "7164/7164 [==============================] - 6s 849us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 407/600\n",
      "7164/7164 [==============================] - 6s 857us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 408/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 409/600\n",
      "7164/7164 [==============================] - 6s 796us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 410/600\n",
      "7164/7164 [==============================] - 6s 800us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 411/600\n",
      "7164/7164 [==============================] - 6s 857us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 412/600\n",
      "7164/7164 [==============================] - 6s 799us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 413/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 414/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 6s 783us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 415/600\n",
      "7164/7164 [==============================] - 6s 827us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 416/600\n",
      "7164/7164 [==============================] - 6s 872us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 417/600\n",
      "7164/7164 [==============================] - 7s 927us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 418/600\n",
      "7164/7164 [==============================] - 6s 810us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 419/600\n",
      "7164/7164 [==============================] - 5s 707us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 420/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 421/600\n",
      "7164/7164 [==============================] - 6s 835us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 422/600\n",
      "7164/7164 [==============================] - 6s 797us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 423/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 424/600\n",
      "7164/7164 [==============================] - 6s 896us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 425/600\n",
      "7164/7164 [==============================] - 6s 812us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 426/600\n",
      "7164/7164 [==============================] - 6s 814us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 427/600\n",
      "7164/7164 [==============================] - 6s 831us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 428/600\n",
      "7164/7164 [==============================] - 6s 800us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 429/600\n",
      "7164/7164 [==============================] - 6s 785us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 430/600\n",
      "7164/7164 [==============================] - 6s 798us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 431/600\n",
      "7164/7164 [==============================] - 6s 768us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 432/600\n",
      "7164/7164 [==============================] - 5s 710us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 433/600\n",
      "7164/7164 [==============================] - 5s 696us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 434/600\n",
      "7164/7164 [==============================] - 5s 713us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 435/600\n",
      "7164/7164 [==============================] - 5s 725us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 436/600\n",
      "7164/7164 [==============================] - 5s 707us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 437/600\n",
      "7164/7164 [==============================] - 5s 723us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 438/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 439/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 440/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 441/600\n",
      "7164/7164 [==============================] - 5s 725us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 442/600\n",
      "7164/7164 [==============================] - 6s 874us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 443/600\n",
      "7164/7164 [==============================] - 7s 935us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 444/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 445/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 446/600\n",
      "7164/7164 [==============================] - 6s 867us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 447/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 448/600\n",
      "7164/7164 [==============================] - 5s 761us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 449/600\n",
      "7164/7164 [==============================] - 5s 721us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 450/600\n",
      "7164/7164 [==============================] - 5s 763us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 451/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 452/600\n",
      "7164/7164 [==============================] - 5s 761us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 453/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 454/600\n",
      "7164/7164 [==============================] - 5s 733us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 455/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 456/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 457/600\n",
      "7164/7164 [==============================] - 6s 794us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 458/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 459/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 460/600\n",
      "7164/7164 [==============================] - 6s 801us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 461/600\n",
      "7164/7164 [==============================] - 5s 729us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 462/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 463/600\n",
      "7164/7164 [==============================] - 6s 784us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 464/600\n",
      "7164/7164 [==============================] - 5s 723us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 465/600\n",
      "7164/7164 [==============================] - 5s 742us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 466/600\n",
      "7164/7164 [==============================] - 6s 793us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 467/600\n",
      "7164/7164 [==============================] - 6s 799us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 468/600\n",
      "7164/7164 [==============================] - 6s 899us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 469/600\n",
      "7164/7164 [==============================] - 7s 911us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 470/600\n",
      "7164/7164 [==============================] - 6s 804us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 471/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 472/600\n",
      "7164/7164 [==============================] - 5s 726us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 473/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 6s 799us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 474/600\n",
      "7164/7164 [==============================] - 5s 749us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 475/600\n",
      "7164/7164 [==============================] - 7s 936us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 476/600\n",
      "7164/7164 [==============================] - 7s 916us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 477/600\n",
      "7164/7164 [==============================] - 6s 863us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 478/600\n",
      "7164/7164 [==============================] - 6s 880us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 479/600\n",
      "7164/7164 [==============================] - 6s 904us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 480/600\n",
      "7164/7164 [==============================] - 6s 860us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 481/600\n",
      "7164/7164 [==============================] - 6s 827us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 482/600\n",
      "7164/7164 [==============================] - 6s 900us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 483/600\n",
      "7164/7164 [==============================] - 7s 931us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 484/600\n",
      "7164/7164 [==============================] - 7s 931us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 485/600\n",
      "7164/7164 [==============================] - 7s 968us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 486/600\n",
      "7164/7164 [==============================] - 6s 855us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 487/600\n",
      "7164/7164 [==============================] - 5s 725us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 488/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 489/600\n",
      "7164/7164 [==============================] - 6s 877us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 490/600\n",
      "7164/7164 [==============================] - 6s 854us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 491/600\n",
      "7164/7164 [==============================] - 5s 741us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 492/600\n",
      "7164/7164 [==============================] - 6s 804us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 493/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 494/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 495/600\n",
      "7164/7164 [==============================] - 6s 828us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 496/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 497/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 498/600\n",
      "7164/7164 [==============================] - 5s 730us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 499/600\n",
      "7164/7164 [==============================] - 5s 732us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 500/600\n",
      "7164/7164 [==============================] - 6s 770us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 501/600\n",
      "7164/7164 [==============================] - 7s 937us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 502/600\n",
      "7164/7164 [==============================] - 7s 956us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 503/600\n",
      "7164/7164 [==============================] - 9s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 504/600\n",
      "7164/7164 [==============================] - 6s 904us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 505/600\n",
      "7164/7164 [==============================] - 6s 779us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 506/600\n",
      "7164/7164 [==============================] - 6s 890us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 507/600\n",
      "7164/7164 [==============================] - 5s 758us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 508/600\n",
      "7164/7164 [==============================] - 5s 700us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 509/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 510/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 511/600\n",
      "7164/7164 [==============================] - 6s 820us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 512/600\n",
      "7164/7164 [==============================] - 7s 913us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 513/600\n",
      "7164/7164 [==============================] - 5s 696us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 514/600\n",
      "7164/7164 [==============================] - 6s 808us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 515/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 516/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 517/600\n",
      "7164/7164 [==============================] - 5s 662us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 518/600\n",
      "7164/7164 [==============================] - 5s 712us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 519/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 520/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 521/600\n",
      "7164/7164 [==============================] - 5s 725us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 522/600\n",
      "7164/7164 [==============================] - 5s 666us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 523/600\n",
      "7164/7164 [==============================] - 5s 727us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 524/600\n",
      "7164/7164 [==============================] - 5s 719us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 525/600\n",
      "7164/7164 [==============================] - 5s 740us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 526/600\n",
      "7164/7164 [==============================] - 5s 697us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 527/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 528/600\n",
      "7164/7164 [==============================] - 5s 714us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 529/600\n",
      "7164/7164 [==============================] - 5s 719us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 530/600\n",
      "7164/7164 [==============================] - 5s 706us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 531/600\n",
      "7164/7164 [==============================] - 5s 759us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 532/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 691us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 533/600\n",
      "7164/7164 [==============================] - 5s 710us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 534/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 535/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 536/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 537/600\n",
      "7164/7164 [==============================] - 5s 712us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 538/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 539/600\n",
      "7164/7164 [==============================] - 7s 954us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 540/600\n",
      "7164/7164 [==============================] - 5s 759us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 541/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 542/600\n",
      "7164/7164 [==============================] - 6s 870us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 543/600\n",
      "7164/7164 [==============================] - 7s 913us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 544/600\n",
      "7164/7164 [==============================] - 7s 996us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 545/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 546/600\n",
      "7164/7164 [==============================] - 7s 995us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 547/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 548/600\n",
      "7164/7164 [==============================] - 6s 801us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 549/600\n",
      "7164/7164 [==============================] - 6s 856us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 550/600\n",
      "7164/7164 [==============================] - 6s 855us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 551/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 552/600\n",
      "7164/7164 [==============================] - 5s 720us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 553/600\n",
      "7164/7164 [==============================] - 5s 765us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 554/600\n",
      "7164/7164 [==============================] - 6s 810us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 555/600\n",
      "7164/7164 [==============================] - 5s 758us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 556/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 557/600\n",
      "7164/7164 [==============================] - 7s 961us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 558/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 559/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 560/600\n",
      "7164/7164 [==============================] - 5s 697us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 561/600\n",
      "7164/7164 [==============================] - 6s 883us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 562/600\n",
      "7164/7164 [==============================] - 5s 729us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 563/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 564/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 565/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 566/600\n",
      "7164/7164 [==============================] - 5s 646us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 567/600\n",
      "7164/7164 [==============================] - 5s 660us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 568/600\n",
      "7164/7164 [==============================] - 5s 718us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 569/600\n",
      "7164/7164 [==============================] - 5s 687us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 570/600\n",
      "7164/7164 [==============================] - 5s 672us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 571/600\n",
      "7164/7164 [==============================] - 5s 652us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 572/600\n",
      "7164/7164 [==============================] - 6s 806us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 573/600\n",
      "7164/7164 [==============================] - 6s 874us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 574/600\n",
      "7164/7164 [==============================] - 6s 801us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 575/600\n",
      "7164/7164 [==============================] - 7s 999us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 576/600\n",
      "7164/7164 [==============================] - 7s 946us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 577/600\n",
      "7164/7164 [==============================] - 7s 942us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 578/600\n",
      "7164/7164 [==============================] - 6s 809us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 579/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 580/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 581/600\n",
      "7164/7164 [==============================] - 7s 970us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 582/600\n",
      "7164/7164 [==============================] - 6s 883us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 583/600\n",
      "7164/7164 [==============================] - 7s 911us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 584/600\n",
      "7164/7164 [==============================] - 6s 860us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 585/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 586/600\n",
      "7164/7164 [==============================] - 5s 748us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 587/600\n",
      "7164/7164 [==============================] - 6s 906us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 588/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 589/600\n",
      "7164/7164 [==============================] - 7s 950us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 590/600\n",
      "7164/7164 [==============================] - 6s 874us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 591/600\n",
      "7164/7164 [==============================] - 6s 829us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 592/600\n",
      "7164/7164 [==============================] - 5s 745us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 593/600\n",
      "7164/7164 [==============================] - 6s 868us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 594/600\n",
      "7164/7164 [==============================] - 6s 871us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 595/600\n",
      "7164/7164 [==============================] - 5s 692us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 596/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 597/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 598/600\n",
      "7164/7164 [==============================] - 5s 682us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 599/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "Epoch 600/600\n",
      "7164/7164 [==============================] - 5s 695us/step - loss: 0.0022 - acc: 0.9999 - val_loss: 2.5388 - val_acc: 0.8371\n",
      "1292/1292 [==============================] - 0s 74us/step\n",
      "==========================================================================\n",
      "FOLD 3\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467]\n",
      "RF Confusion Matrix: \n",
      "[[  0   0   0]\n",
      " [ 74 119 565]\n",
      " [  0   0 534]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984]\n",
      "SVM Confusion Matrix: \n",
      "[[  0   0   0]\n",
      " [119  74 565]\n",
      " [  0 220 314]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467]\n",
      "DT Confusion Matrix: \n",
      "[[119 639]\n",
      " [  0 534]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355, 1.0]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355], [1.1920928955078125e-07, 1.0]]\n",
      "sNN Confusion Matrix: \n",
      "[[758   0]\n",
      " [  0 534]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 4\n",
      "Train on 7189 samples, validate on 1798 samples\n",
      "Epoch 1/600\n",
      " 128/7189 [..............................] - ETA: 4s - loss: 1.8813e-07 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7189/7189 [==============================] - 5s 673us/step - loss: 0.0661 - acc: 0.9955 - val_loss: 6.3613 - val_acc: 0.5979\n",
      "Epoch 2/600\n",
      "7189/7189 [==============================] - 5s 691us/step - loss: 0.0085 - acc: 0.9993 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 3/600\n",
      "7189/7189 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 4/600\n",
      "7189/7189 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 5/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 6/600\n",
      "7189/7189 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 7/600\n",
      "7189/7189 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 8/600\n",
      "7189/7189 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 9/600\n",
      "7189/7189 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 10/600\n",
      "7189/7189 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 11/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 12/600\n",
      "7189/7189 [==============================] - 6s 898us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 13/600\n",
      "7189/7189 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 14/600\n",
      "7189/7189 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 15/600\n",
      "7189/7189 [==============================] - 6s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 16/600\n",
      "7189/7189 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 17/600\n",
      "7189/7189 [==============================] - 5s 763us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 18/600\n",
      "7189/7189 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 19/600\n",
      "7189/7189 [==============================] - 6s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 20/600\n",
      "7189/7189 [==============================] - 6s 864us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 21/600\n",
      "7189/7189 [==============================] - 6s 797us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 22/600\n",
      "7189/7189 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 23/600\n",
      "7189/7189 [==============================] - 6s 831us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 24/600\n",
      "7189/7189 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 25/600\n",
      "7189/7189 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 26/600\n",
      "7189/7189 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 27/600\n",
      "7189/7189 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 28/600\n",
      "7189/7189 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 29/600\n",
      "7189/7189 [==============================] - 7s 962us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 30/600\n",
      "7189/7189 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 31/600\n",
      "7189/7189 [==============================] - 6s 867us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 32/600\n",
      "7189/7189 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 33/600\n",
      "7189/7189 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 34/600\n",
      "7189/7189 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 35/600\n",
      "7189/7189 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 36/600\n",
      "7189/7189 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 37/600\n",
      "7189/7189 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 38/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 39/600\n",
      "7189/7189 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 40/600\n",
      "7189/7189 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 41/600\n",
      "7189/7189 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 42/600\n",
      "7189/7189 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 43/600\n",
      "7189/7189 [==============================] - 7s 956us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 44/600\n",
      "7189/7189 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 45/600\n",
      "7189/7189 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 46/600\n",
      "7189/7189 [==============================] - 6s 847us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 47/600\n",
      "7189/7189 [==============================] - 6s 816us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 48/600\n",
      "7189/7189 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 49/600\n",
      "7189/7189 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 50/600\n",
      "7189/7189 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 51/600\n",
      "7189/7189 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 52/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 53/600\n",
      "7189/7189 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 54/600\n",
      "7189/7189 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 55/600\n",
      "7189/7189 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 56/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 57/600\n",
      "7189/7189 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 58/600\n",
      "7189/7189 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/600\n",
      "7189/7189 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 60/600\n",
      "7189/7189 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 61/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 62/600\n",
      "7189/7189 [==============================] - 5s 648us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 63/600\n",
      "7189/7189 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 64/600\n",
      "7189/7189 [==============================] - 6s 783us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 65/600\n",
      "7189/7189 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 66/600\n",
      "7189/7189 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 67/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 68/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 69/600\n",
      "7189/7189 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 70/600\n",
      "7189/7189 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 71/600\n",
      "7189/7189 [==============================] - 6s 870us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 72/600\n",
      "7189/7189 [==============================] - 6s 792us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 73/600\n",
      "7189/7189 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 74/600\n",
      "7189/7189 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 75/600\n",
      "7189/7189 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 76/600\n",
      "7189/7189 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 77/600\n",
      "7189/7189 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 78/600\n",
      "7189/7189 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 79/600\n",
      "7189/7189 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 80/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 81/600\n",
      "7189/7189 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 82/600\n",
      "7189/7189 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 83/600\n",
      "7189/7189 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 84/600\n",
      "7189/7189 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 85/600\n",
      "7189/7189 [==============================] - 6s 894us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 86/600\n",
      "7189/7189 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 87/600\n",
      "7189/7189 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 88/600\n",
      "7189/7189 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 89/600\n",
      "7189/7189 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 90/600\n",
      "7189/7189 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 91/600\n",
      "7189/7189 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 92/600\n",
      "7189/7189 [==============================] - 6s 842us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 93/600\n",
      "7189/7189 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 94/600\n",
      "7189/7189 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 95/600\n",
      "7189/7189 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 96/600\n",
      "7189/7189 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 97/600\n",
      "7189/7189 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 98/600\n",
      "7189/7189 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 99/600\n",
      "7189/7189 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 100/600\n",
      "7189/7189 [==============================] - 5s 646us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 101/600\n",
      "7189/7189 [==============================] - 5s 644us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 102/600\n",
      "7189/7189 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 103/600\n",
      "7189/7189 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 104/600\n",
      "7189/7189 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 105/600\n",
      "7189/7189 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 106/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 107/600\n",
      "7189/7189 [==============================] - 5s 635us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 108/600\n",
      "7189/7189 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 109/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 110/600\n",
      "7189/7189 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 111/600\n",
      "7189/7189 [==============================] - 5s 643us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 112/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 113/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 114/600\n",
      "7189/7189 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 115/600\n",
      "7189/7189 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 116/600\n",
      "7189/7189 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 117/600\n",
      "7189/7189 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 118/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 119/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 120/600\n",
      "7189/7189 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 121/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 122/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 123/600\n",
      "7189/7189 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 124/600\n",
      "7189/7189 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 125/600\n",
      "7189/7189 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 126/600\n",
      "7189/7189 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 127/600\n",
      "7189/7189 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 128/600\n",
      "7189/7189 [==============================] - 94s 13ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 129/600\n",
      "7189/7189 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 130/600\n",
      "7189/7189 [==============================] - 7s 977us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 131/600\n",
      "7189/7189 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 132/600\n",
      "7189/7189 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 133/600\n",
      "7189/7189 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 134/600\n",
      "7189/7189 [==============================] - 6s 864us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 135/600\n",
      "7189/7189 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 136/600\n",
      "7189/7189 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 137/600\n",
      "7189/7189 [==============================] - 7s 926us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 138/600\n",
      "7189/7189 [==============================] - 6s 833us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 139/600\n",
      "7189/7189 [==============================] - 6s 835us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 140/600\n",
      "7189/7189 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 141/600\n",
      "7189/7189 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 142/600\n",
      "7189/7189 [==============================] - 6s 792us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 143/600\n",
      "7189/7189 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 144/600\n",
      "7189/7189 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 145/600\n",
      "7189/7189 [==============================] - 6s 892us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 146/600\n",
      "7189/7189 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 147/600\n",
      "7189/7189 [==============================] - 7s 920us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 148/600\n",
      "7189/7189 [==============================] - 6s 888us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 149/600\n",
      "7189/7189 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 150/600\n",
      "7189/7189 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 151/600\n",
      "7189/7189 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 152/600\n",
      "7189/7189 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 153/600\n",
      "7189/7189 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 154/600\n",
      "7189/7189 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 155/600\n",
      "7189/7189 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 156/600\n",
      "7189/7189 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 157/600\n",
      "7189/7189 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 158/600\n",
      "7189/7189 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 159/600\n",
      "7189/7189 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 160/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 161/600\n",
      "7189/7189 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 162/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 163/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 164/600\n",
      "7189/7189 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 165/600\n",
      "7189/7189 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 166/600\n",
      "7189/7189 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 167/600\n",
      "7189/7189 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 168/600\n",
      "7189/7189 [==============================] - 6s 872us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 169/600\n",
      "7189/7189 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 170/600\n",
      "7189/7189 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 171/600\n",
      "7189/7189 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 172/600\n",
      "7189/7189 [==============================] - 7s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 173/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7189/7189 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 174/600\n",
      "7189/7189 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 175/600\n",
      "7189/7189 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 176/600\n",
      "7189/7189 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 177/600\n",
      "7189/7189 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 178/600\n",
      "7189/7189 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 179/600\n",
      "7189/7189 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 180/600\n",
      "7189/7189 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 181/600\n",
      "7189/7189 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 182/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 183/600\n",
      "7189/7189 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 184/600\n",
      "7189/7189 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 185/600\n",
      "7189/7189 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 186/600\n",
      "7189/7189 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 187/600\n",
      "7189/7189 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 188/600\n",
      "7189/7189 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 189/600\n",
      "7189/7189 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 190/600\n",
      "7189/7189 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 191/600\n",
      "7189/7189 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 192/600\n",
      "7189/7189 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 193/600\n",
      "7189/7189 [==============================] - 6s 801us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 194/600\n",
      "7189/7189 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 195/600\n",
      "7189/7189 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 196/600\n",
      "7189/7189 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 197/600\n",
      "7189/7189 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 198/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 199/600\n",
      "7189/7189 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 200/600\n",
      "7189/7189 [==============================] - 6s 773us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 201/600\n",
      "7189/7189 [==============================] - 6s 840us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 202/600\n",
      "7189/7189 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 203/600\n",
      "7189/7189 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 204/600\n",
      "7189/7189 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 205/600\n",
      "7189/7189 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 206/600\n",
      "7189/7189 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 207/600\n",
      "7189/7189 [==============================] - 5s 720us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 208/600\n",
      "7189/7189 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 209/600\n",
      "7189/7189 [==============================] - 6s 836us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 210/600\n",
      "7189/7189 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 211/600\n",
      "7189/7189 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 212/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 213/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 214/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 215/600\n",
      "7189/7189 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 216/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 217/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 218/600\n",
      "7189/7189 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 219/600\n",
      "7189/7189 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 220/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 221/600\n",
      "7189/7189 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 222/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 223/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 224/600\n",
      "7189/7189 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 225/600\n",
      "7189/7189 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 226/600\n",
      "7189/7189 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 227/600\n",
      "7189/7189 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 228/600\n",
      "7189/7189 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 229/600\n",
      "7189/7189 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 230/600\n",
      "7189/7189 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 231/600\n",
      "7189/7189 [==============================] - 5s 647us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 232/600\n",
      "7189/7189 [==============================] - 5s 636us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 233/600\n",
      "7189/7189 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 234/600\n",
      "7189/7189 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 235/600\n",
      "7189/7189 [==============================] - 5s 650us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 236/600\n",
      "7189/7189 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 237/600\n",
      "7189/7189 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 238/600\n",
      "7189/7189 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 239/600\n",
      "7189/7189 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 240/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 241/600\n",
      "7189/7189 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 242/600\n",
      "7189/7189 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 243/600\n",
      "7189/7189 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 244/600\n",
      "7189/7189 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 245/600\n",
      "7189/7189 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 246/600\n",
      "7189/7189 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 247/600\n",
      "7189/7189 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 248/600\n",
      "7189/7189 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 249/600\n",
      "7189/7189 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 250/600\n",
      "7189/7189 [==============================] - 5s 639us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 251/600\n",
      "7189/7189 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 252/600\n",
      "7189/7189 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 253/600\n",
      "7189/7189 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 254/600\n",
      "7189/7189 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 255/600\n",
      "7189/7189 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 256/600\n",
      "7189/7189 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 257/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 258/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 259/600\n",
      "7189/7189 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 260/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 261/600\n",
      "7189/7189 [==============================] - 6s 894us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 262/600\n",
      "7189/7189 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 263/600\n",
      "7189/7189 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 264/600\n",
      "7189/7189 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 265/600\n",
      "7189/7189 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 266/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 267/600\n",
      "7189/7189 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 268/600\n",
      "7189/7189 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 269/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 270/600\n",
      "7189/7189 [==============================] - 6s 834us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 271/600\n",
      "7189/7189 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 272/600\n",
      "7189/7189 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 273/600\n",
      "7189/7189 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 274/600\n",
      "7189/7189 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 275/600\n",
      "7189/7189 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 276/600\n",
      "7189/7189 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 277/600\n",
      "7189/7189 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 278/600\n",
      "7189/7189 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 279/600\n",
      "7189/7189 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 280/600\n",
      "7189/7189 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 281/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 282/600\n",
      "7189/7189 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 283/600\n",
      "7189/7189 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 284/600\n",
      "7189/7189 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 285/600\n",
      "7189/7189 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 286/600\n",
      "7189/7189 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 287/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7189/7189 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 288/600\n",
      "7189/7189 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 289/600\n",
      "7189/7189 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 290/600\n",
      "7189/7189 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 291/600\n",
      "7189/7189 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 292/600\n",
      "7189/7189 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 293/600\n",
      "7189/7189 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 294/600\n",
      "7189/7189 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 295/600\n",
      "7189/7189 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 296/600\n",
      "7189/7189 [==============================] - 6s 779us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 297/600\n",
      "7189/7189 [==============================] - 6s 852us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 298/600\n",
      "7189/7189 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 299/600\n",
      "7189/7189 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 300/600\n",
      "7189/7189 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 301/600\n",
      "7189/7189 [==============================] - 6s 800us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 302/600\n",
      "7189/7189 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 303/600\n",
      "7189/7189 [==============================] - 6s 802us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 304/600\n",
      "7189/7189 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 305/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 306/600\n",
      "7189/7189 [==============================] - 6s 801us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 307/600\n",
      "7189/7189 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 308/600\n",
      "7189/7189 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 309/600\n",
      "7189/7189 [==============================] - 7s 936us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 310/600\n",
      "7189/7189 [==============================] - 6s 823us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 311/600\n",
      "7189/7189 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 312/600\n",
      "7189/7189 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 313/600\n",
      "7189/7189 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 314/600\n",
      "7189/7189 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 315/600\n",
      "7189/7189 [==============================] - 6s 797us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 316/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 317/600\n",
      "7189/7189 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 318/600\n",
      "7189/7189 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 319/600\n",
      "7189/7189 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 320/600\n",
      "7189/7189 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 321/600\n",
      "7189/7189 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 322/600\n",
      "7189/7189 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 323/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 324/600\n",
      "7189/7189 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 325/600\n",
      "7189/7189 [==============================] - 6s 891us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 326/600\n",
      "7189/7189 [==============================] - 7s 919us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 327/600\n",
      "7189/7189 [==============================] - 6s 886us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 328/600\n",
      "7189/7189 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 329/600\n",
      "7189/7189 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 330/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 331/600\n",
      "7189/7189 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 332/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 333/600\n",
      "7189/7189 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 334/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 335/600\n",
      "7189/7189 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 336/600\n",
      "7189/7189 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 337/600\n",
      "7189/7189 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 338/600\n",
      "7189/7189 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 339/600\n",
      "7189/7189 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 340/600\n",
      "7189/7189 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 341/600\n",
      "7189/7189 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 342/600\n",
      "7189/7189 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 343/600\n",
      "7189/7189 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 344/600\n",
      "7189/7189 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 345/600\n",
      "7189/7189 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 346/600\n",
      "7189/7189 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 347/600\n",
      "7189/7189 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 348/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 349/600\n",
      "7189/7189 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 350/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 351/600\n",
      "7189/7189 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 352/600\n",
      "7189/7189 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 353/600\n",
      "7189/7189 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 354/600\n",
      "7189/7189 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 355/600\n",
      "7189/7189 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 356/600\n",
      "7189/7189 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 357/600\n",
      "7189/7189 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 358/600\n",
      "7189/7189 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 359/600\n",
      "7189/7189 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 360/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 361/600\n",
      "7189/7189 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 362/600\n",
      "7189/7189 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 363/600\n",
      "7189/7189 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 364/600\n",
      "7189/7189 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 365/600\n",
      "7189/7189 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 366/600\n",
      "7189/7189 [==============================] - 6s 902us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 367/600\n",
      "7189/7189 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 368/600\n",
      "7189/7189 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 369/600\n",
      "7189/7189 [==============================] - 6s 829us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 370/600\n",
      "7189/7189 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 371/600\n",
      "7189/7189 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 372/600\n",
      "7189/7189 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 373/600\n",
      "7189/7189 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 374/600\n",
      "7189/7189 [==============================] - 6s 835us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 375/600\n",
      "7189/7189 [==============================] - 5s 765us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 376/600\n",
      "7189/7189 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 377/600\n",
      "7189/7189 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 378/600\n",
      "7189/7189 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 379/600\n",
      "7189/7189 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 380/600\n",
      "7189/7189 [==============================] - 6s 880us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 381/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 382/600\n",
      "7189/7189 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 383/600\n",
      "7189/7189 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 384/600\n",
      "7189/7189 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 385/600\n",
      "7189/7189 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 386/600\n",
      "7189/7189 [==============================] - 6s 833us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 387/600\n",
      "7189/7189 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 388/600\n",
      "7189/7189 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 389/600\n",
      "7189/7189 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 390/600\n",
      "7189/7189 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 391/600\n",
      "7189/7189 [==============================] - 7s 950us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 392/600\n",
      "7189/7189 [==============================] - 6s 798us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 393/600\n",
      "7189/7189 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 394/600\n",
      "7189/7189 [==============================] - 6s 894us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 395/600\n",
      "7189/7189 [==============================] - 6s 895us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 396/600\n",
      "7189/7189 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 397/600\n",
      "7189/7189 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 398/600\n",
      "7189/7189 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 399/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 400/600\n",
      "7189/7189 [==============================] - 7s 931us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 401/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7189/7189 [==============================] - 7s 931us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 402/600\n",
      "7189/7189 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 403/600\n",
      "7189/7189 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 404/600\n",
      "7189/7189 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 405/600\n",
      "7189/7189 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 406/600\n",
      "7189/7189 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 407/600\n",
      "7189/7189 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 408/600\n",
      "7189/7189 [==============================] - 5s 639us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 409/600\n",
      "7189/7189 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 410/600\n",
      "7189/7189 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 411/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 412/600\n",
      "7189/7189 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 413/600\n",
      "7189/7189 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 414/600\n",
      "7189/7189 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 415/600\n",
      "7189/7189 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 416/600\n",
      "7189/7189 [==============================] - 5s 645us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 417/600\n",
      "7189/7189 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 418/600\n",
      "7189/7189 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 419/600\n",
      "7189/7189 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 420/600\n",
      "7189/7189 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 421/600\n",
      "7189/7189 [==============================] - 7s 986us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 422/600\n",
      "7189/7189 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 423/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 424/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 425/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 426/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 427/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 428/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 429/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 430/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 431/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 432/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 433/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 434/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 435/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 436/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 437/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 438/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 439/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 440/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 441/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 442/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 443/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 444/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 445/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 446/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 447/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 448/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 449/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 450/600\n",
      "7189/7189 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 451/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 452/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 453/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 454/600\n",
      "7189/7189 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 455/600\n",
      "7189/7189 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 456/600\n",
      "7189/7189 [==============================] - 6s 885us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 457/600\n",
      "7189/7189 [==============================] - 7s 942us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 458/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 459/600\n",
      "7189/7189 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 460/600\n",
      "7189/7189 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 461/600\n",
      "7189/7189 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 462/600\n",
      "7189/7189 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 463/600\n",
      "7189/7189 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 464/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 465/600\n",
      "7189/7189 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 466/600\n",
      "7189/7189 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 467/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 468/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 469/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 470/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 471/600\n",
      "7189/7189 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 472/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 473/600\n",
      "7189/7189 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 474/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 475/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 476/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 477/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 478/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 479/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 480/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 481/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 482/600\n",
      "7189/7189 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 483/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 484/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 485/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 486/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 487/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 488/600\n",
      "7189/7189 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 489/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 490/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 491/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 492/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 493/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 494/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 495/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 496/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 497/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 498/600\n",
      "7189/7189 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 499/600\n",
      "7189/7189 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 500/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 501/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 502/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 503/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 504/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 505/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 506/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 507/600\n",
      "7189/7189 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 508/600\n",
      "7189/7189 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 509/600\n",
      "7189/7189 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 510/600\n",
      "7189/7189 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 511/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 512/600\n",
      "7189/7189 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 513/600\n",
      "7189/7189 [==============================] - 6s 830us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 514/600\n",
      "7189/7189 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 515/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7189/7189 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 516/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 517/600\n",
      "7189/7189 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 518/600\n",
      "7189/7189 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 519/600\n",
      "7189/7189 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 520/600\n",
      "7189/7189 [==============================] - 7s 905us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 521/600\n",
      "7189/7189 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 522/600\n",
      "7189/7189 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 523/600\n",
      "7189/7189 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 524/600\n",
      "7189/7189 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 525/600\n",
      "7189/7189 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 526/600\n",
      "7189/7189 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 527/600\n",
      "7189/7189 [==============================] - 5s 646us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 528/600\n",
      "7189/7189 [==============================] - 5s 636us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 529/600\n",
      "7189/7189 [==============================] - 6s 859us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 530/600\n",
      "7189/7189 [==============================] - 6s 845us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 531/600\n",
      "7189/7189 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 532/600\n",
      "7189/7189 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 533/600\n",
      "7189/7189 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 534/600\n",
      "7189/7189 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 535/600\n",
      "7189/7189 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 536/600\n",
      "7189/7189 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 537/600\n",
      "7189/7189 [==============================] - 6s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 538/600\n",
      "7189/7189 [==============================] - 6s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 539/600\n",
      "7189/7189 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 540/600\n",
      "7189/7189 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 541/600\n",
      "7189/7189 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 542/600\n",
      "7189/7189 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 543/600\n",
      "7189/7189 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 544/600\n",
      "7189/7189 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 545/600\n",
      "7189/7189 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 546/600\n",
      "7189/7189 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 547/600\n",
      "7189/7189 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 548/600\n",
      "7189/7189 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 549/600\n",
      "7189/7189 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 550/600\n",
      "7189/7189 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 551/600\n",
      "7189/7189 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 552/600\n",
      "7189/7189 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 553/600\n",
      "7189/7189 [==============================] - 5s 734us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 554/600\n",
      "7189/7189 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 555/600\n",
      "7189/7189 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 556/600\n",
      "7189/7189 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 557/600\n",
      "7189/7189 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 558/600\n",
      "7189/7189 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 559/600\n",
      "7189/7189 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 560/600\n",
      "7189/7189 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 561/600\n",
      "7189/7189 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 562/600\n",
      "7189/7189 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 563/600\n",
      "7189/7189 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 564/600\n",
      "7189/7189 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 565/600\n",
      "7189/7189 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 566/600\n",
      "7189/7189 [==============================] - 5s 748us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 567/600\n",
      "7189/7189 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 568/600\n",
      "7189/7189 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 569/600\n",
      "7189/7189 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 570/600\n",
      "7189/7189 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 571/600\n",
      "7189/7189 [==============================] - 7s 925us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 572/600\n",
      "7189/7189 [==============================] - 7s 981us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 573/600\n",
      "7189/7189 [==============================] - 6s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 574/600\n",
      "7189/7189 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 575/600\n",
      "7189/7189 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 576/600\n",
      "7189/7189 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 577/600\n",
      "7189/7189 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 578/600\n",
      "7189/7189 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 579/600\n",
      "7189/7189 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 580/600\n",
      "7189/7189 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 581/600\n",
      "7189/7189 [==============================] - 6s 860us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 582/600\n",
      "7189/7189 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 583/600\n",
      "7189/7189 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 584/600\n",
      "7189/7189 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 585/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 586/600\n",
      "7189/7189 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 587/600\n",
      "7189/7189 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 588/600\n",
      "7189/7189 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 589/600\n",
      "7189/7189 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 590/600\n",
      "7189/7189 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 591/600\n",
      "7189/7189 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 592/600\n",
      "7189/7189 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 593/600\n",
      "7189/7189 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 594/600\n",
      "7189/7189 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 595/600\n",
      "7189/7189 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 596/600\n",
      "7189/7189 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 597/600\n",
      "7189/7189 [==============================] - 5s 753us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 598/600\n",
      "7189/7189 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 599/600\n",
      "7189/7189 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "Epoch 600/600\n",
      "7189/7189 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 5.4500 - val_acc: 0.6457\n",
      "1261/1261 [==============================] - 0s 73us/step\n",
      "==========================================================================\n",
      "FOLD 4\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624]\n",
      "RF Confusion Matrix: \n",
      "[[  0  69   0]\n",
      " [235   0 653]\n",
      " [  0   0 304]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755]\n",
      "SVM Confusion Matrix: \n",
      "[[  0  69   0]\n",
      " [235   0 653]\n",
      " [  0   6 298]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755]\n",
      "DT Confusion Matrix: \n",
      "[[  0  69   0]\n",
      " [235   0 653]\n",
      " [  0   6 298]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355, 1.0, 0.9920697858842189]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355], [1.1920928955078125e-07, 1.0], [0.0848679807713635, 0.9920697858842189]]\n",
      "sNN Confusion Matrix: \n",
      "[[ 69   0   0]\n",
      " [  0 878  10]\n",
      " [  0   0 304]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 5\n",
      "Train on 7177 samples, validate on 1795 samples\n",
      "Epoch 1/600\n",
      " 128/7177 [..............................] - ETA: 4s - loss: 1.1921e-07 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7177/7177 [==============================] - 5s 676us/step - loss: 0.0010 - acc: 0.9999 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 2/600\n",
      "7177/7177 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 3/600\n",
      "7177/7177 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 4/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 5/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 6/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 7/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 8/600\n",
      "7177/7177 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 9/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 10/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 11/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 12/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 13/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 14/600\n",
      "7177/7177 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 15/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 16/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 17/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 18/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 19/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 20/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 21/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 22/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 23/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 24/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 25/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 26/600\n",
      "7177/7177 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 27/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 28/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 29/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 30/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 31/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 32/600\n",
      "7177/7177 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 33/600\n",
      "7177/7177 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 34/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 35/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 36/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 37/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 38/600\n",
      "7177/7177 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 39/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 40/600\n",
      "7177/7177 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 41/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 42/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 43/600\n",
      "7177/7177 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 44/600\n",
      "7177/7177 [==============================] - 5s 703us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 45/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 46/600\n",
      "7177/7177 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 47/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 48/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 49/600\n",
      "7177/7177 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 50/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 51/600\n",
      "7177/7177 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 52/600\n",
      "7177/7177 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 53/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 54/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 55/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 56/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 57/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 58/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/600\n",
      "7177/7177 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 60/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 61/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 62/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 63/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 64/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 65/600\n",
      "7177/7177 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 66/600\n",
      "7177/7177 [==============================] - 5s 669us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 67/600\n",
      "7177/7177 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 68/600\n",
      "7177/7177 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 69/600\n",
      "7177/7177 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 70/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 71/600\n",
      "7177/7177 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 72/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 73/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 74/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 75/600\n",
      "7177/7177 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 76/600\n",
      "7177/7177 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 77/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 78/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 79/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 80/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 81/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 82/600\n",
      "7177/7177 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 83/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 84/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 85/600\n",
      "7177/7177 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 86/600\n",
      "7177/7177 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 87/600\n",
      "7177/7177 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 88/600\n",
      "7177/7177 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 89/600\n",
      "7177/7177 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 90/600\n",
      "7177/7177 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 91/600\n",
      "7177/7177 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 92/600\n",
      "7177/7177 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 93/600\n",
      "7177/7177 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 94/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 95/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 96/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 97/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 98/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 99/600\n",
      "7177/7177 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 100/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 101/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 102/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 103/600\n",
      "7177/7177 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 104/600\n",
      "7177/7177 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 105/600\n",
      "7177/7177 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 106/600\n",
      "7177/7177 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 107/600\n",
      "7177/7177 [==============================] - 6s 769us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 108/600\n",
      "7177/7177 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 109/600\n",
      "7177/7177 [==============================] - 6s 815us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 110/600\n",
      "7177/7177 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 111/600\n",
      "7177/7177 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 112/600\n",
      "7177/7177 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 113/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 114/600\n",
      "7177/7177 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 115/600\n",
      "7177/7177 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 116/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 117/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 118/600\n",
      "7177/7177 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 119/600\n",
      "7177/7177 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 120/600\n",
      "7177/7177 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 121/600\n",
      "7177/7177 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 122/600\n",
      "7177/7177 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 123/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 124/600\n",
      "7177/7177 [==============================] - 5s 705us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 125/600\n",
      "7177/7177 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 126/600\n",
      "7177/7177 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 127/600\n",
      "7177/7177 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 128/600\n",
      "7177/7177 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 129/600\n",
      "7177/7177 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 130/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 131/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 132/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 133/600\n",
      "7177/7177 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 134/600\n",
      "7177/7177 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 135/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 136/600\n",
      "7177/7177 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 137/600\n",
      "7177/7177 [==============================] - 5s 639us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 138/600\n",
      "7177/7177 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 139/600\n",
      "7177/7177 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 140/600\n",
      "7177/7177 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 141/600\n",
      "7177/7177 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 142/600\n",
      "7177/7177 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 143/600\n",
      "7177/7177 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 144/600\n",
      "7177/7177 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 145/600\n",
      "7177/7177 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 146/600\n",
      "7177/7177 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 147/600\n",
      "7177/7177 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 148/600\n",
      "7177/7177 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 149/600\n",
      "7177/7177 [==============================] - 5s 651us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 150/600\n",
      "7177/7177 [==============================] - 5s 640us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 151/600\n",
      "7177/7177 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 152/600\n",
      "7177/7177 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 153/600\n",
      "7177/7177 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 154/600\n",
      "7177/7177 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 155/600\n",
      "7177/7177 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 156/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 157/600\n",
      "7177/7177 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 158/600\n",
      "7177/7177 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 159/600\n",
      "7177/7177 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 160/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 161/600\n",
      "7177/7177 [==============================] - 5s 647us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 162/600\n",
      "7177/7177 [==============================] - 5s 645us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 163/600\n",
      "7177/7177 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 164/600\n",
      "7177/7177 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 165/600\n",
      "7177/7177 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 166/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 167/600\n",
      "7177/7177 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 168/600\n",
      "7177/7177 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 169/600\n",
      "7177/7177 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 170/600\n",
      "7177/7177 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 171/600\n",
      "7177/7177 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 172/600\n",
      "7177/7177 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 173/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7177/7177 [==============================] - 5s 717us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 174/600\n",
      "7177/7177 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 175/600\n",
      "7177/7177 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 176/600\n",
      "7177/7177 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 177/600\n",
      "7177/7177 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 178/600\n",
      "7177/7177 [==============================] - 6s 801us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 179/600\n",
      "7177/7177 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 180/600\n",
      "7177/7177 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 181/600\n",
      "7177/7177 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 182/600\n",
      "7177/7177 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 183/600\n",
      "7177/7177 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 184/600\n",
      "7177/7177 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 185/600\n",
      "7177/7177 [==============================] - 6s 815us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 186/600\n",
      "7177/7177 [==============================] - 6s 836us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 187/600\n",
      "7177/7177 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 188/600\n",
      "7177/7177 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 189/600\n",
      "7177/7177 [==============================] - 6s 779us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 190/600\n",
      "7177/7177 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 191/600\n",
      "7177/7177 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 192/600\n",
      "7177/7177 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 193/600\n",
      "7177/7177 [==============================] - 6s 771us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 194/600\n",
      "7177/7177 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 195/600\n",
      "7177/7177 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 196/600\n",
      "7177/7177 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 197/600\n",
      "7177/7177 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 198/600\n",
      "7177/7177 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 199/600\n",
      "7177/7177 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 200/600\n",
      "7177/7177 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 201/600\n",
      "7177/7177 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 202/600\n",
      "7177/7177 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 203/600\n",
      "7177/7177 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 204/600\n",
      "7177/7177 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 205/600\n",
      "7177/7177 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 206/600\n",
      "7177/7177 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 207/600\n",
      "7177/7177 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 208/600\n",
      "7177/7177 [==============================] - 6s 873us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 209/600\n",
      "7177/7177 [==============================] - 7s 934us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 210/600\n",
      "7177/7177 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 211/600\n",
      "7177/7177 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 212/600\n",
      "7177/7177 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 213/600\n",
      "7177/7177 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 214/600\n",
      "7177/7177 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 215/600\n",
      "7177/7177 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 216/600\n",
      "7177/7177 [==============================] - 5s 747us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 217/600\n",
      "7177/7177 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 218/600\n",
      "7177/7177 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 219/600\n",
      "7177/7177 [==============================] - 6s 826us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 220/600\n",
      "7177/7177 [==============================] - 5s 766us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 221/600\n",
      "7177/7177 [==============================] - 6s 799us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 222/600\n",
      "7177/7177 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 223/600\n",
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 224/600\n",
      "7177/7177 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 225/600\n",
      "7177/7177 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 226/600\n",
      "7177/7177 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 227/600\n",
      "7177/7177 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 228/600\n",
      "7177/7177 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 229/600\n",
      "7177/7177 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 230/600\n",
      "7177/7177 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 231/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 232/600\n",
      "7177/7177 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 233/600\n",
      "7177/7177 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 234/600\n",
      "7177/7177 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 235/600\n",
      "7177/7177 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 236/600\n",
      "7177/7177 [==============================] - 5s 764us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 237/600\n",
      "7177/7177 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 238/600\n",
      "7177/7177 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 239/600\n",
      "7177/7177 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 240/600\n",
      "7177/7177 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 241/600\n",
      "7177/7177 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 242/600\n",
      "7177/7177 [==============================] - 6s 801us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 243/600\n",
      "7177/7177 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 244/600\n",
      "7177/7177 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 245/600\n",
      "7177/7177 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 246/600\n",
      "7177/7177 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 247/600\n",
      "7177/7177 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 248/600\n",
      "7177/7177 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 249/600\n",
      "7177/7177 [==============================] - 5s 715us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 250/600\n",
      "7177/7177 [==============================] - 5s 735us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 251/600\n",
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 252/600\n",
      "7177/7177 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 253/600\n",
      "7177/7177 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 254/600\n",
      "7177/7177 [==============================] - 5s 725us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 255/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 256/600\n",
      "7177/7177 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 257/600\n",
      "7177/7177 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 258/600\n",
      "7177/7177 [==============================] - 5s 647us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 259/600\n",
      "7177/7177 [==============================] - 5s 632us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 260/600\n",
      "7177/7177 [==============================] - 5s 644us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 261/600\n",
      "7177/7177 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 262/600\n",
      "7177/7177 [==============================] - 5s 666us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 263/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 264/600\n",
      "7177/7177 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 265/600\n",
      "7177/7177 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 266/600\n",
      "7177/7177 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 267/600\n",
      "7177/7177 [==============================] - 5s 681us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 268/600\n",
      "7177/7177 [==============================] - 5s 646us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 269/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 270/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 271/600\n",
      "7177/7177 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 272/600\n",
      "7177/7177 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 273/600\n",
      "7177/7177 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 274/600\n",
      "7177/7177 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 275/600\n",
      "7177/7177 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 276/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 277/600\n",
      "7177/7177 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 278/600\n",
      "7177/7177 [==============================] - 6s 797us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 279/600\n",
      "7177/7177 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 280/600\n",
      "7177/7177 [==============================] - 5s 716us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 281/600\n",
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 282/600\n",
      "7177/7177 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 283/600\n",
      "7177/7177 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 284/600\n",
      "7177/7177 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 285/600\n",
      "7177/7177 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 286/600\n",
      "7177/7177 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 287/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7177/7177 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 288/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 289/600\n",
      "7177/7177 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 290/600\n",
      "7177/7177 [==============================] - 7s 917us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 291/600\n",
      "7177/7177 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 292/600\n",
      "7177/7177 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 293/600\n",
      "7177/7177 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 294/600\n",
      "7177/7177 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 295/600\n",
      "7177/7177 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 296/600\n",
      "7177/7177 [==============================] - 7s 961us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 297/600\n",
      "7177/7177 [==============================] - 7s 941us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 298/600\n",
      "7177/7177 [==============================] - 6s 834us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 299/600\n",
      "7177/7177 [==============================] - 7s 997us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 300/600\n",
      "7177/7177 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 301/600\n",
      "7177/7177 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 302/600\n",
      "7177/7177 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 303/600\n",
      "7177/7177 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 304/600\n",
      "7177/7177 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 305/600\n",
      "7177/7177 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 306/600\n",
      "7177/7177 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 307/600\n",
      "7177/7177 [==============================] - 6s 892us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 308/600\n",
      "7177/7177 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 309/600\n",
      "7177/7177 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 310/600\n",
      "7177/7177 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 311/600\n",
      "7177/7177 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 312/600\n",
      "7177/7177 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 313/600\n",
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 314/600\n",
      "7177/7177 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 315/600\n",
      "7177/7177 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 316/600\n",
      "7177/7177 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 317/600\n",
      "7177/7177 [==============================] - 7s 987us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 318/600\n",
      "7177/7177 [==============================] - 7s 972us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 319/600\n",
      "7177/7177 [==============================] - 7s 939us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 320/600\n",
      "7177/7177 [==============================] - 6s 888us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 321/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 322/600\n",
      "7177/7177 [==============================] - 7s 959us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 323/600\n",
      "7177/7177 [==============================] - 6s 823us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 324/600\n",
      "7177/7177 [==============================] - 6s 861us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 325/600\n",
      "7177/7177 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 326/600\n",
      "7177/7177 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 327/600\n",
      "7177/7177 [==============================] - 6s 774us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 328/600\n",
      "7177/7177 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 329/600\n",
      "7177/7177 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 330/600\n",
      "7177/7177 [==============================] - 6s 798us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 331/600\n",
      "7177/7177 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 332/600\n",
      "7177/7177 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 333/600\n",
      "7177/7177 [==============================] - 7s 934us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 334/600\n",
      "7177/7177 [==============================] - 6s 784us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 335/600\n",
      "7177/7177 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 336/600\n",
      "7177/7177 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 337/600\n",
      "7177/7177 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 338/600\n",
      "7177/7177 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 339/600\n",
      "7177/7177 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 340/600\n",
      "7177/7177 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 341/600\n",
      "7177/7177 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 342/600\n",
      "7177/7177 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 343/600\n",
      "7177/7177 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 344/600\n",
      "7177/7177 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 345/600\n",
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 346/600\n",
      "7177/7177 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 347/600\n",
      "7177/7177 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 348/600\n",
      "7177/7177 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 349/600\n",
      "7177/7177 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 350/600\n",
      "7177/7177 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 351/600\n",
      "7177/7177 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 352/600\n",
      "7177/7177 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 353/600\n",
      "7177/7177 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 354/600\n",
      "7177/7177 [==============================] - 7s 906us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 355/600\n",
      "7177/7177 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 356/600\n",
      "7177/7177 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 357/600\n",
      "7177/7177 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 358/600\n",
      "7177/7177 [==============================] - 6s 852us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 359/600\n",
      "7177/7177 [==============================] - 6s 791us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 360/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 361/600\n",
      "7177/7177 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 362/600\n",
      "7177/7177 [==============================] - 6s 868us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 363/600\n",
      "7177/7177 [==============================] - 6s 808us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 364/600\n",
      "7177/7177 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 365/600\n",
      "7177/7177 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 366/600\n",
      "7177/7177 [==============================] - 6s 854us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 367/600\n",
      "7177/7177 [==============================] - 7s 985us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 368/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 369/600\n",
      "7177/7177 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 370/600\n",
      "7177/7177 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 371/600\n",
      "7177/7177 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 372/600\n",
      "7177/7177 [==============================] - 6s 806us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 373/600\n",
      "7177/7177 [==============================] - 6s 826us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 374/600\n",
      "7177/7177 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 375/600\n",
      "7177/7177 [==============================] - 6s 802us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 376/600\n",
      "7177/7177 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 377/600\n",
      "7177/7177 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 378/600\n",
      "7177/7177 [==============================] - 7s 929us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 379/600\n",
      "7177/7177 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 380/600\n",
      "7177/7177 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 381/600\n",
      "7177/7177 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 382/600\n",
      "7177/7177 [==============================] - 6s 864us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 383/600\n",
      "7177/7177 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 384/600\n",
      "7177/7177 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 385/600\n",
      "7177/7177 [==============================] - 6s 808us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 386/600\n",
      "7177/7177 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 387/600\n",
      "7177/7177 [==============================] - 6s 862us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 388/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 389/600\n",
      "7177/7177 [==============================] - 7s 952us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 390/600\n",
      "7177/7177 [==============================] - 7s 931us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 391/600\n",
      "7177/7177 [==============================] - 6s 817us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 392/600\n",
      "7177/7177 [==============================] - 6s 828us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 393/600\n",
      "7177/7177 [==============================] - 6s 793us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 394/600\n",
      "7177/7177 [==============================] - 6s 843us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 395/600\n",
      "7177/7177 [==============================] - 7s 924us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 396/600\n",
      "7177/7177 [==============================] - 7s 959us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 397/600\n",
      "7177/7177 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 398/600\n",
      "7177/7177 [==============================] - 6s 901us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 399/600\n",
      "7177/7177 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 400/600\n",
      "7177/7177 [==============================] - 6s 889us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 401/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7177/7177 [==============================] - 7s 964us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 402/600\n",
      "7177/7177 [==============================] - 6s 839us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 403/600\n",
      "7177/7177 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 404/600\n",
      "7177/7177 [==============================] - 6s 888us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 405/600\n",
      "7177/7177 [==============================] - 5s 726us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 406/600\n",
      "7177/7177 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 407/600\n",
      "7177/7177 [==============================] - 5s 733us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 408/600\n",
      "7177/7177 [==============================] - 6s 790us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 409/600\n",
      "7177/7177 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 410/600\n",
      "7177/7177 [==============================] - 6s 772us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 411/600\n",
      "7177/7177 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 412/600\n",
      "7177/7177 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 413/600\n",
      "7177/7177 [==============================] - 5s 739us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 414/600\n",
      "7177/7177 [==============================] - 7s 960us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 415/600\n",
      "7177/7177 [==============================] - 7s 908us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 416/600\n",
      "7177/7177 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 417/600\n",
      "7177/7177 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 418/600\n",
      "7177/7177 [==============================] - 6s 787us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 419/600\n",
      "7177/7177 [==============================] - 6s 898us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 420/600\n",
      "7177/7177 [==============================] - 7s 982us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 421/600\n",
      "7177/7177 [==============================] - 7s 997us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 422/600\n",
      "7177/7177 [==============================] - 6s 852us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 423/600\n",
      "7177/7177 [==============================] - 7s 945us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 424/600\n",
      "7177/7177 [==============================] - 7s 945us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 425/600\n",
      "7177/7177 [==============================] - 7s 907us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 426/600\n",
      "7177/7177 [==============================] - 6s 797us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 427/600\n",
      "7177/7177 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 428/600\n",
      "7177/7177 [==============================] - 6s 795us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 429/600\n",
      "7177/7177 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 430/600\n",
      "7177/7177 [==============================] - 6s 903us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 431/600\n",
      "7177/7177 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 432/600\n",
      "7177/7177 [==============================] - 6s 876us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 433/600\n",
      "7177/7177 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 434/600\n",
      "7177/7177 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 435/600\n",
      "7177/7177 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 436/600\n",
      "7177/7177 [==============================] - 5s 741us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 437/600\n",
      "7177/7177 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 438/600\n",
      "7177/7177 [==============================] - 5s 723us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 439/600\n",
      "7177/7177 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 440/600\n",
      "7177/7177 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 441/600\n",
      "7177/7177 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 442/600\n",
      "7177/7177 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 443/600\n",
      "7177/7177 [==============================] - 5s 729us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 444/600\n",
      "7177/7177 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 445/600\n",
      "7177/7177 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 446/600\n",
      "7177/7177 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 447/600\n",
      "7177/7177 [==============================] - 5s 757us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 448/600\n",
      "7177/7177 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 449/600\n",
      "7177/7177 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 450/600\n",
      "7177/7177 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 451/600\n",
      "7177/7177 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 452/600\n",
      "7177/7177 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 453/600\n",
      "7177/7177 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 454/600\n",
      "7177/7177 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 455/600\n",
      "7177/7177 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 456/600\n",
      "7177/7177 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 457/600\n",
      "7177/7177 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 458/600\n",
      "7177/7177 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 459/600\n",
      "7177/7177 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 460/600\n",
      "7177/7177 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 461/600\n",
      "7177/7177 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 462/600\n",
      "7177/7177 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 463/600\n",
      "7177/7177 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 464/600\n",
      "7177/7177 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 465/600\n",
      "7177/7177 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 466/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 467/600\n",
      "7177/7177 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 468/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 469/600\n",
      "7177/7177 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 470/600\n",
      "7177/7177 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 471/600\n",
      "7177/7177 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 472/600\n",
      "7177/7177 [==============================] - 6s 865us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 473/600\n",
      "7177/7177 [==============================] - 6s 871us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 474/600\n",
      "7177/7177 [==============================] - 7s 948us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 475/600\n",
      "7177/7177 [==============================] - 7s 944us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 476/600\n",
      "7177/7177 [==============================] - 6s 881us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 477/600\n",
      "7177/7177 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 478/600\n",
      "7177/7177 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 479/600\n",
      "7177/7177 [==============================] - 6s 867us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 480/600\n",
      "7177/7177 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 481/600\n",
      "7177/7177 [==============================] - 6s 846us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 482/600\n",
      "7177/7177 [==============================] - 6s 848us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 483/600\n",
      "7177/7177 [==============================] - 6s 838us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 484/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 485/600\n",
      "7177/7177 [==============================] - 6s 847us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 486/600\n",
      "7177/7177 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 487/600\n",
      "7177/7177 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 488/600\n",
      "7177/7177 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 489/600\n",
      "7177/7177 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 490/600\n",
      "7177/7177 [==============================] - 7s 912us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 491/600\n",
      "7177/7177 [==============================] - 7s 929us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 492/600\n",
      "7177/7177 [==============================] - 7s 996us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 493/600\n",
      "7177/7177 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 494/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 495/600\n",
      "7177/7177 [==============================] - 6s 833us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 496/600\n",
      "7177/7177 [==============================] - 6s 785us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 497/600\n",
      "7177/7177 [==============================] - 6s 796us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 498/600\n",
      "7177/7177 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 499/600\n",
      "7177/7177 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 500/600\n",
      "7177/7177 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 501/600\n",
      "7177/7177 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 502/600\n",
      "7177/7177 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 503/600\n",
      "7177/7177 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 504/600\n",
      "7177/7177 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 505/600\n",
      "7177/7177 [==============================] - 7s 914us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 506/600\n",
      "7177/7177 [==============================] - 6s 821us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 507/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 508/600\n",
      "7177/7177 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 509/600\n",
      "7177/7177 [==============================] - 6s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 510/600\n",
      "7177/7177 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 511/600\n",
      "7177/7177 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 512/600\n",
      "7177/7177 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 513/600\n",
      "7177/7177 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 514/600\n",
      "7177/7177 [==============================] - 5s 709us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 515/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 516/600\n",
      "7177/7177 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 517/600\n",
      "7177/7177 [==============================] - 5s 690us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 518/600\n",
      "7177/7177 [==============================] - 5s 732us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 519/600\n",
      "7177/7177 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 520/600\n",
      "7177/7177 [==============================] - 5s 721us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 521/600\n",
      "7177/7177 [==============================] - 5s 731us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 522/600\n",
      "7177/7177 [==============================] - 5s 754us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 523/600\n",
      "7177/7177 [==============================] - 5s 719us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 524/600\n",
      "7177/7177 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 525/600\n",
      "7177/7177 [==============================] - 5s 737us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 526/600\n",
      "7177/7177 [==============================] - 5s 728us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 527/600\n",
      "7177/7177 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 528/600\n",
      "7177/7177 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 529/600\n",
      "7177/7177 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 530/600\n",
      "7177/7177 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 531/600\n",
      "7177/7177 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 532/600\n",
      "7177/7177 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 533/600\n",
      "7177/7177 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 534/600\n",
      "7177/7177 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 535/600\n",
      "7177/7177 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 536/600\n",
      "7177/7177 [==============================] - 5s 688us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 537/600\n",
      "7177/7177 [==============================] - 5s 702us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 538/600\n",
      "7177/7177 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 539/600\n",
      "7177/7177 [==============================] - 5s 679us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 540/600\n",
      "7177/7177 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 541/600\n",
      "7177/7177 [==============================] - 5s 677us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 542/600\n",
      "7177/7177 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 543/600\n",
      "7177/7177 [==============================] - 5s 673us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 544/600\n",
      "7177/7177 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 545/600\n",
      "7177/7177 [==============================] - 7s 935us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 546/600\n",
      "7177/7177 [==============================] - 5s 713us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 547/600\n",
      "7177/7177 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 548/600\n",
      "7177/7177 [==============================] - 7s 943us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 549/600\n",
      "7177/7177 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 550/600\n",
      "7177/7177 [==============================] - 6s 768us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 551/600\n",
      "7177/7177 [==============================] - 6s 805us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 552/600\n",
      "7177/7177 [==============================] - 6s 802us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 553/600\n",
      "7177/7177 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 554/600\n",
      "7177/7177 [==============================] - 6s 884us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 555/600\n",
      "7177/7177 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 556/600\n",
      "7177/7177 [==============================] - 7s 910us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 557/600\n",
      "7177/7177 [==============================] - 6s 875us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 558/600\n",
      "7177/7177 [==============================] - 6s 878us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 559/600\n",
      "7177/7177 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 560/600\n",
      "7177/7177 [==============================] - 7s 995us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 561/600\n",
      "7177/7177 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 562/600\n",
      "7177/7177 [==============================] - 7s 935us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 563/600\n",
      "7177/7177 [==============================] - 7s 915us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 564/600\n",
      "7177/7177 [==============================] - 7s 932us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 565/600\n",
      "7177/7177 [==============================] - 7s 928us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 566/600\n",
      "7177/7177 [==============================] - 7s 921us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 567/600\n",
      "7177/7177 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 568/600\n",
      "7177/7177 [==============================] - 6s 851us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 569/600\n",
      "7177/7177 [==============================] - 6s 897us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 570/600\n",
      "7177/7177 [==============================] - 7s 991us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 571/600\n",
      "7177/7177 [==============================] - 6s 861us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 572/600\n",
      "7177/7177 [==============================] - 6s 872us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 573/600\n",
      "7177/7177 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 574/600\n",
      "7177/7177 [==============================] - 6s 881us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 575/600\n",
      "7177/7177 [==============================] - 6s 898us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 576/600\n",
      "7177/7177 [==============================] - 6s 812us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 577/600\n",
      "7177/7177 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 578/600\n",
      "7177/7177 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 579/600\n",
      "7177/7177 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 580/600\n",
      "7177/7177 [==============================] - 7s 976us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 581/600\n",
      "7177/7177 [==============================] - 7s 916us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 582/600\n",
      "7177/7177 [==============================] - 6s 857us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 583/600\n",
      "7177/7177 [==============================] - 6s 867us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 584/600\n",
      "7177/7177 [==============================] - 6s 815us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 585/600\n",
      "7177/7177 [==============================] - 6s 896us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 586/600\n",
      "7177/7177 [==============================] - 6s 827us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 587/600\n",
      "7177/7177 [==============================] - 6s 840us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 588/600\n",
      "7177/7177 [==============================] - 6s 860us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 589/600\n",
      "7177/7177 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 590/600\n",
      "7177/7177 [==============================] - 6s 824us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 591/600\n",
      "7177/7177 [==============================] - 5s 752us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 592/600\n",
      "7177/7177 [==============================] - 6s 898us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 593/600\n",
      "7177/7177 [==============================] - 6s 811us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 594/600\n",
      "7177/7177 [==============================] - 6s 804us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 595/600\n",
      "7177/7177 [==============================] - 7s 929us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 596/600\n",
      "7177/7177 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 597/600\n",
      "7177/7177 [==============================] - 6s 853us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 598/600\n",
      "7177/7177 [==============================] - 6s 879us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 599/600\n",
      "7177/7177 [==============================] - 6s 842us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "Epoch 600/600\n",
      "7177/7177 [==============================] - 7s 946us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8385 - val_acc: 0.8106\n",
      "1276/1276 [==============================] - 0s 80us/step\n",
      "==========================================================================\n",
      "FOLD 5\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461]\n",
      "RF Confusion Matrix: \n",
      "[[  1 312   0]\n",
      " [459   0   0]\n",
      " [  0  92 412]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436]\n",
      "SVM Confusion Matrix: \n",
      "[[  0 313   0]\n",
      " [  0   0 459]\n",
      " [  0 190 314]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663]\n",
      "DT Confusion Matrix: \n",
      "[[313   0   0]\n",
      " [459   0   0]\n",
      " [ 92 152 260]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355, 1.0, 0.9920697858842189, 0.7547021943573667]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355], [1.1920928955078125e-07, 1.0], [0.0848679807713635, 0.9920697858842189], [3.9528459112472296, 0.7547021943573667]]\n",
      "sNN Confusion Matrix: \n",
      "[[  0 313   0]\n",
      " [  0 459   0]\n",
      " [  0   0 504]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 6\n",
      "Train on 7152 samples, validate on 1789 samples\n",
      "Epoch 1/600\n",
      " 128/7152 [..............................] - ETA: 4s - loss: 1.1921e-07 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7152/7152 [==============================] - 5s 759us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 2/600\n",
      "7152/7152 [==============================] - 5s 755us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 3/600\n",
      "7152/7152 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 4/600\n",
      "7152/7152 [==============================] - 6s 809us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 5/600\n",
      "7152/7152 [==============================] - 5s 736us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 6/600\n",
      "7152/7152 [==============================] - 5s 712us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 7/600\n",
      "7152/7152 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 8/600\n",
      "7152/7152 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 9/600\n",
      "7152/7152 [==============================] - 5s 689us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 10/600\n",
      "7152/7152 [==============================] - 5s 767us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 11/600\n",
      "7152/7152 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 12/600\n",
      "7152/7152 [==============================] - 6s 845us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 13/600\n",
      "7152/7152 [==============================] - 6s 881us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 14/600\n",
      "7152/7152 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 15/600\n",
      "7152/7152 [==============================] - 5s 751us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 16/600\n",
      "7152/7152 [==============================] - 6s 819us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 17/600\n",
      "7152/7152 [==============================] - 6s 903us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 18/600\n",
      "7152/7152 [==============================] - 6s 820us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 19/600\n",
      "7152/7152 [==============================] - 6s 861us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 20/600\n",
      "7152/7152 [==============================] - 6s 841us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 21/600\n",
      "7152/7152 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 22/600\n",
      "7152/7152 [==============================] - 6s 873us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 23/600\n",
      "7152/7152 [==============================] - 6s 810us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 24/600\n",
      "7152/7152 [==============================] - 5s 762us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 25/600\n",
      "7152/7152 [==============================] - 6s 870us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 26/600\n",
      "7152/7152 [==============================] - 5s 761us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 27/600\n",
      "7152/7152 [==============================] - 6s 779us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 28/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 29/600\n",
      "7152/7152 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 30/600\n",
      "7152/7152 [==============================] - 5s 742us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 31/600\n",
      "7152/7152 [==============================] - 5s 697us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 32/600\n",
      "7152/7152 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 33/600\n",
      "7152/7152 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 34/600\n",
      "7152/7152 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 35/600\n",
      "7152/7152 [==============================] - 5s 668us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 36/600\n",
      "7152/7152 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 37/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 38/600\n",
      "7152/7152 [==============================] - 6s 789us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 39/600\n",
      "7152/7152 [==============================] - 6s 832us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 40/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 41/600\n",
      "7152/7152 [==============================] - 5s 760us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 42/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 43/600\n",
      "7152/7152 [==============================] - 5s 671us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 44/600\n",
      "7152/7152 [==============================] - 5s 661us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 45/600\n",
      "7152/7152 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 46/600\n",
      "7152/7152 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 47/600\n",
      "7152/7152 [==============================] - 6s 778us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 48/600\n",
      "7152/7152 [==============================] - 6s 780us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 49/600\n",
      "7152/7152 [==============================] - 6s 814us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 50/600\n",
      "7152/7152 [==============================] - 5s 730us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 51/600\n",
      "7152/7152 [==============================] - 5s 756us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 52/600\n",
      "7152/7152 [==============================] - 5s 750us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 53/600\n",
      "7152/7152 [==============================] - 5s 746us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 54/600\n",
      "7152/7152 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 55/600\n",
      "7152/7152 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 56/600\n",
      "7152/7152 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 57/600\n",
      "7152/7152 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 58/600\n",
      "7152/7152 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/600\n",
      "7152/7152 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 60/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 61/600\n",
      "7152/7152 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 62/600\n",
      "7152/7152 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 63/600\n",
      "7152/7152 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 64/600\n",
      "7152/7152 [==============================] - 5s 704us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 65/600\n",
      "7152/7152 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 66/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 67/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 68/600\n",
      "7152/7152 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 69/600\n",
      "7152/7152 [==============================] - 5s 662us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 70/600\n",
      "7152/7152 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 71/600\n",
      "7152/7152 [==============================] - 6s 823us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 72/600\n",
      "7152/7152 [==============================] - 5s 749us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 73/600\n",
      "7152/7152 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 74/600\n",
      "7152/7152 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 75/600\n",
      "7152/7152 [==============================] - 5s 707us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 76/600\n",
      "7152/7152 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 77/600\n",
      "7152/7152 [==============================] - 6s 842us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 78/600\n",
      "7152/7152 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 79/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 80/600\n",
      "7152/7152 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 81/600\n",
      "7152/7152 [==============================] - 5s 695us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 82/600\n",
      "7152/7152 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 83/600\n",
      "7152/7152 [==============================] - 5s 706us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 84/600\n",
      "7152/7152 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 85/600\n",
      "7152/7152 [==============================] - 5s 685us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 86/600\n",
      "7152/7152 [==============================] - 5s 691us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 87/600\n",
      "7152/7152 [==============================] - 5s 699us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 88/600\n",
      "7152/7152 [==============================] - 5s 692us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 89/600\n",
      "7152/7152 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 90/600\n",
      "7152/7152 [==============================] - 6s 801us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 91/600\n",
      "7152/7152 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 92/600\n",
      "7152/7152 [==============================] - 5s 687us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 93/600\n",
      "7152/7152 [==============================] - 5s 696us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 94/600\n",
      "7152/7152 [==============================] - 5s 698us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 95/600\n",
      "7152/7152 [==============================] - 5s 694us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 96/600\n",
      "7152/7152 [==============================] - 5s 684us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 97/600\n",
      "7152/7152 [==============================] - 6s 807us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 98/600\n",
      "7152/7152 [==============================] - 6s 904us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 99/600\n",
      "7152/7152 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 100/600\n",
      "7152/7152 [==============================] - 5s 683us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 101/600\n",
      "7152/7152 [==============================] - 6s 786us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 102/600\n",
      "7152/7152 [==============================] - 5s 700us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 103/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 104/600\n",
      "7152/7152 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 105/600\n",
      "7152/7152 [==============================] - ETA: 0s - loss: 1.1921e-07 - acc: 1.000 - 5s 646us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 106/600\n",
      "7152/7152 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 107/600\n",
      "7152/7152 [==============================] - 5s 701us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 108/600\n",
      "7152/7152 [==============================] - 5s 708us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 109/600\n",
      "7152/7152 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 110/600\n",
      "7152/7152 [==============================] - 5s 641us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 111/600\n",
      "7152/7152 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 112/600\n",
      "7152/7152 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 113/600\n",
      "7152/7152 [==============================] - 5s 652us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 114/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 115/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 116/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 117/600\n",
      "7152/7152 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 118/600\n",
      "7152/7152 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 119/600\n",
      "7152/7152 [==============================] - 5s 640us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 120/600\n",
      "7152/7152 [==============================] - 5s 646us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 121/600\n",
      "7152/7152 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 122/600\n",
      "7152/7152 [==============================] - 5s 686us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 123/600\n",
      "7152/7152 [==============================] - 5s 660us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 124/600\n",
      "7152/7152 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 125/600\n",
      "7152/7152 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 126/600\n",
      "7152/7152 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 127/600\n",
      "7152/7152 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 128/600\n",
      "7152/7152 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 129/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 130/600\n",
      "7152/7152 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 131/600\n",
      "7152/7152 [==============================] - 5s 659us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 132/600\n",
      "7152/7152 [==============================] - 5s 641us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 133/600\n",
      "7152/7152 [==============================] - 5s 644us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 134/600\n",
      "7152/7152 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 135/600\n",
      "7152/7152 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 136/600\n",
      "7152/7152 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 137/600\n",
      "7152/7152 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 138/600\n",
      "7152/7152 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 139/600\n",
      "7152/7152 [==============================] - 5s 657us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 140/600\n",
      "7152/7152 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 141/600\n",
      "7152/7152 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 142/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 143/600\n",
      "7152/7152 [==============================] - 5s 680us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 144/600\n",
      "7152/7152 [==============================] - 5s 676us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 145/600\n",
      "7152/7152 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 146/600\n",
      "7152/7152 [==============================] - 5s 670us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 147/600\n",
      "7152/7152 [==============================] - 5s 667us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 148/600\n",
      "7152/7152 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 149/600\n",
      "7152/7152 [==============================] - 5s 674us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 150/600\n",
      "7152/7152 [==============================] - 5s 711us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 151/600\n",
      "7152/7152 [==============================] - 5s 682us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 152/600\n",
      "7152/7152 [==============================] - 5s 637us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 153/600\n",
      "7152/7152 [==============================] - 5s 653us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 154/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 155/600\n",
      "7152/7152 [==============================] - 5s 654us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 156/600\n",
      "7152/7152 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 157/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 158/600\n",
      "7152/7152 [==============================] - 5s 658us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 159/600\n",
      "7152/7152 [==============================] - 5s 672us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 160/600\n",
      "7152/7152 [==============================] - 5s 663us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 161/600\n",
      "7152/7152 [==============================] - 5s 656us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 162/600\n",
      "7152/7152 [==============================] - 5s 655us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 163/600\n",
      "7152/7152 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 164/600\n",
      "7152/7152 [==============================] - 5s 665us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 165/600\n",
      "7152/7152 [==============================] - 5s 693us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 166/600\n",
      "7152/7152 [==============================] - 5s 664us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 167/600\n",
      "7152/7152 [==============================] - 5s 649us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 168/600\n",
      "7152/7152 [==============================] - 6s 844us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 169/600\n",
      "7152/7152 [==============================] - 7s 940us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 170/600\n",
      "7152/7152 [==============================] - 6s 770us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 171/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 172/600\n",
      "7152/7152 [==============================] - 6s 813us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 173/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7152/7152 [==============================] - 6s 788us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 174/600\n",
      "7152/7152 [==============================] - 5s 722us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 175/600\n",
      "7152/7152 [==============================] - 5s 744us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 176/600\n",
      "7152/7152 [==============================] - 6s 779us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 177/600\n",
      "7152/7152 [==============================] - 5s 710us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 178/600\n",
      "7152/7152 [==============================] - 5s 758us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 179/600\n",
      "7152/7152 [==============================] - 6s 782us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 180/600\n",
      "7152/7152 [==============================] - 5s 714us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 181/600\n",
      "7152/7152 [==============================] - 5s 718us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 182/600\n",
      "7152/7152 [==============================] - 6s 777us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 183/600\n",
      "7152/7152 [==============================] - 6s 776us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 184/600\n",
      "7152/7152 [==============================] - 6s 873us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 185/600\n",
      "7152/7152 [==============================] - 5s 738us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 186/600\n",
      "7152/7152 [==============================] - 5s 743us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 187/600\n",
      "7152/7152 [==============================] - 5s 727us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 188/600\n",
      "7152/7152 [==============================] - 5s 724us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 189/600\n",
      "7152/7152 [==============================] - 6s 831us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 190/600\n",
      "7152/7152 [==============================] - 5s 745us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 191/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 192/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 193/600\n",
      "7152/7152 [==============================] - 6s 883us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 194/600\n",
      "7152/7152 [==============================] - 6s 781us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 195/600\n",
      "7152/7152 [==============================] - 6s 775us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 196/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 197/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 198/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 199/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 200/600\n",
      "7152/7152 [==============================] - 6s 803us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 201/600\n",
      "7152/7152 [==============================] - 6s 856us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 202/600\n",
      "7152/7152 [==============================] - 7s 982us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 203/600\n",
      "7152/7152 [==============================] - 6s 855us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 204/600\n",
      "7152/7152 [==============================] - 6s 794us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 205/600\n",
      "7152/7152 [==============================] - 5s 740us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 206/600\n",
      "7152/7152 [==============================] - 5s 678us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 207/600\n",
      "7152/7152 [==============================] - 5s 675us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 208/600\n",
      "7152/7152 [==============================] - 6s 822us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 209/600\n",
      "7152/7152 [==============================] - 6s 866us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 210/600\n",
      "7152/7152 [==============================] - 6s 825us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 211/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 212/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 213/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 214/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 215/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 216/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 217/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 218/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 219/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 220/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 221/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 222/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 223/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 224/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 225/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 226/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 227/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 228/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 229/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 230/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 231/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 232/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 233/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 234/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 235/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 236/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 237/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 238/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 239/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 240/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 241/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 242/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 243/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 244/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 245/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 246/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 247/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 248/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 249/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 250/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 251/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 252/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 253/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 254/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 255/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 256/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 257/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 258/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 259/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 260/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 261/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 262/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 263/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 264/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 265/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 266/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 267/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 268/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 269/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 270/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 271/600\n",
      "7152/7152 [==============================] - 18s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 272/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 273/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 274/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 275/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 276/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 277/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 278/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 279/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 280/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 281/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 282/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 283/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 284/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 285/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 286/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 287/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 289/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 290/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 291/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 292/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 293/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 294/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 295/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 296/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 297/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 298/600\n",
      "7152/7152 [==============================] - 7s 995us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 299/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 300/600\n",
      "7152/7152 [==============================] - 7s 993us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 301/600\n",
      "7152/7152 [==============================] - 7s 978us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 302/600\n",
      "7152/7152 [==============================] - 7s 912us/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 303/600\n",
      "7152/7152 [==============================] - 7s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 304/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 305/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 306/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 307/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 308/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 309/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 310/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 311/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 312/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 313/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 314/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 315/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 316/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 317/600\n",
      "7152/7152 [==============================] - 20s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 318/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 319/600\n",
      "7152/7152 [==============================] - 17s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 320/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 321/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 322/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 323/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 324/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 325/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 326/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 327/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 328/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 329/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 330/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 331/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 332/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 333/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 334/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 335/600\n",
      "7152/7152 [==============================] - 21s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 336/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 337/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 338/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 339/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 340/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 341/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 342/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 343/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 344/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 345/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 346/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 347/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 348/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 349/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 350/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 351/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 352/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 353/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 354/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 355/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 356/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 357/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 358/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 359/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 360/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 361/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 362/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 363/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 364/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 365/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 366/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 367/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 368/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 369/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 370/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 371/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 372/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 373/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 374/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 375/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 376/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 377/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 378/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 379/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 380/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 381/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 382/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 383/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 384/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 385/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 386/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 387/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 388/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 389/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 390/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 391/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 392/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 393/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 394/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 395/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 396/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 397/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 398/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 399/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 400/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 401/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 402/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 403/600\n",
      "7152/7152 [==============================] - 11s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 404/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 405/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 406/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 407/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 408/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 409/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 410/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 411/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 412/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 413/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 414/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 415/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 416/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 417/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 418/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 419/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 420/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 421/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 422/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 423/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 424/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 425/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 426/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 427/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 428/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 429/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250-07 - acc: 1.000\n",
      "Epoch 430/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 431/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 432/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 433/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 434/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 435/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 436/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 437/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 438/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 439/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 440/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 441/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 442/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 443/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 444/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 445/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 446/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 447/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 448/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 449/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 450/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 451/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 452/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 453/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 454/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 455/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 456/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 457/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 458/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 459/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 460/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 461/600\n",
      "7152/7152 [==============================] - 8s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 462/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 463/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 464/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 465/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 466/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 467/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 468/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 469/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 470/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 471/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 472/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 473/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 474/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 475/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 476/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 477/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 478/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 479/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 480/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 481/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 482/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 483/600\n",
      "7152/7152 [==============================] - 17s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 484/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 485/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 486/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 487/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 488/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 489/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 490/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 491/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 492/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 493/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 494/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 495/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 496/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 497/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 498/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 499/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 500/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 501/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 502/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 503/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 504/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 505/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 506/600\n",
      "7152/7152 [==============================] - 20s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 507/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 508/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 509/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 510/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 511/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 512/600\n",
      "7152/7152 [==============================] - 20s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 513/600\n",
      "7152/7152 [==============================] - 20s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 514/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 515/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 516/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 517/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 518/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 519/600\n",
      "7152/7152 [==============================] - 17s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 520/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 521/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 522/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 523/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 524/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 525/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 526/600\n",
      "7152/7152 [==============================] - 18s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 527/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 528/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 529/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 530/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 531/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 532/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 533/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 534/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 535/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 536/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 537/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 538/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 539/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 540/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 541/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 542/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 543/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 544/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 545/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 546/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 547/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 548/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 549/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 550/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 551/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 552/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 553/600\n",
      "7152/7152 [==============================] - 19s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 554/600\n",
      "7152/7152 [==============================] - 18s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 555/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 556/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 557/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 558/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 559/600\n",
      "7152/7152 [==============================] - 17s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 560/600\n",
      "7152/7152 [==============================] - 18s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 561/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 562/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 563/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 564/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 565/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 566/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 567/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 568/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 569/600\n",
      "7152/7152 [==============================] - 12s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 570/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 571/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 572/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 573/600\n",
      "7152/7152 [==============================] - 15s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 574/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 575/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 576/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 577/600\n",
      "7152/7152 [==============================] - 13s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 578/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 579/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 580/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 581/600\n",
      "7152/7152 [==============================] - 9s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 582/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 583/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 584/600\n",
      "7152/7152 [==============================] - 21s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 585/600\n",
      "7152/7152 [==============================] - 20s 3ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 586/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 587/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 588/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 589/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 590/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 591/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 592/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 593/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 594/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 595/600\n",
      "7152/7152 [==============================] - 10s 1ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 596/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 597/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 598/600\n",
      "7152/7152 [==============================] - 16s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 599/600\n",
      "7152/7152 [==============================] - 14s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "Epoch 600/600\n",
      "7152/7152 [==============================] - 11s 2ms/step - loss: 1.1921e-07 - acc: 1.0000 - val_loss: 2.8200 - val_acc: 0.8250\n",
      "1307/1307 [==============================] - 0s 131us/step\n",
      "==========================================================================\n",
      "FOLD 6\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461, 1.0]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461, 1.0]\n",
      "RF Confusion Matrix: \n",
      "[[802   0   0]\n",
      " [  0 307   0]\n",
      " [  0   0 198]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436, 0.35577658760520275]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436, 0.35577658760520275]\n",
      "SVM Confusion Matrix: \n",
      "[[ 98 257 447]\n",
      " [  0 307   0]\n",
      " [  0 138  60]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663, 0.5470543228768171]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663, 0.5470543228768171]\n",
      "DT Confusion Matrix: \n",
      "[[447 355   0]\n",
      " [  0  70 237]\n",
      " [  0   0 198]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355, 1.0, 0.9920697858842189, 0.7547021943573667, 0.7077276205049732]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355], [1.1920928955078125e-07, 1.0], [0.0848679807713635, 0.9920697858842189], [3.9528459112472296, 0.7547021943573667], [4.517363900707802, 0.7077276205220748]]\n",
      "sNN Confusion Matrix: \n",
      "[[420 382   0]\n",
      " [  0 307   0]\n",
      " [  0   0 198]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 7\n",
      "Train on 7164 samples, validate on 1792 samples\n",
      "Epoch 1/600\n",
      "  32/7164 [..............................] - ETA: 5s - loss: 1.1921e-07 - acc: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0157 - acc: 0.9989 - val_loss: 4.1307 - val_acc: 0.7009\n",
      "Epoch 2/600\n",
      "7164/7164 [==============================] - 11s 2ms/step - loss: 0.0143 - acc: 0.9989 - val_loss: 6.7278 - val_acc: 0.5162\n",
      "Epoch 3/600\n",
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0140 - acc: 0.9989 - val_loss: 5.0313 - val_acc: 0.6060\n",
      "Epoch 4/600\n",
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0162 - acc: 0.9987 - val_loss: 4.1296 - val_acc: 0.6105\n",
      "Epoch 5/600\n",
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0183 - acc: 0.9987 - val_loss: 3.2407 - val_acc: 0.7327\n",
      "Epoch 6/600\n",
      "7164/7164 [==============================] - 6s 777us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 7/600\n",
      "7164/7164 [==============================] - 5s 730us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 8/600\n",
      "7164/7164 [==============================] - 6s 797us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 9/600\n",
      "7164/7164 [==============================] - 7s 944us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 10/600\n",
      "7164/7164 [==============================] - 6s 809us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 11/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 12/600\n",
      "7164/7164 [==============================] - 5s 693us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 13/600\n",
      "7164/7164 [==============================] - 5s 737us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 14/600\n",
      "7164/7164 [==============================] - 7s 912us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 15/600\n",
      "7164/7164 [==============================] - 6s 879us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 16/600\n",
      "7164/7164 [==============================] - 6s 823us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 17/600\n",
      "7164/7164 [==============================] - 6s 905us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 18/600\n",
      "7164/7164 [==============================] - 5s 763us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 19/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 20/600\n",
      "7164/7164 [==============================] - 6s 825us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 21/600\n",
      "7164/7164 [==============================] - 6s 879us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 22/600\n",
      "7164/7164 [==============================] - 6s 808us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 23/600\n",
      "7164/7164 [==============================] - 6s 771us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 24/600\n",
      "7164/7164 [==============================] - 5s 758us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 25/600\n",
      "7164/7164 [==============================] - 5s 726us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 26/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 27/600\n",
      "7164/7164 [==============================] - 5s 713us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 28/600\n",
      "7164/7164 [==============================] - 5s 743us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 29/600\n",
      "7164/7164 [==============================] - 5s 727us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 30/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 31/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 32/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 33/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 34/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 35/600\n",
      "7164/7164 [==============================] - 5s 662us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 36/600\n",
      "7164/7164 [==============================] - 5s 668us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 37/600\n",
      "7164/7164 [==============================] - 5s 655us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 38/600\n",
      "7164/7164 [==============================] - 5s 657us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 39/600\n",
      "7164/7164 [==============================] - 5s 650us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 40/600\n",
      "7164/7164 [==============================] - 5s 638us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 41/600\n",
      "7164/7164 [==============================] - 5s 659us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 42/600\n",
      "7164/7164 [==============================] - 5s 655us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 43/600\n",
      "7164/7164 [==============================] - 5s 654us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 44/600\n",
      "7164/7164 [==============================] - 5s 658us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 45/600\n",
      "7164/7164 [==============================] - 5s 652us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 46/600\n",
      "7164/7164 [==============================] - 5s 651us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 47/600\n",
      "7164/7164 [==============================] - 5s 656us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 48/600\n",
      "7164/7164 [==============================] - 5s 658us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 49/600\n",
      "7164/7164 [==============================] - 5s 641us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 50/600\n",
      "7164/7164 [==============================] - 5s 652us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 51/600\n",
      "7164/7164 [==============================] - 5s 653us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 52/600\n",
      "7164/7164 [==============================] - 5s 655us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 53/600\n",
      "7164/7164 [==============================] - 5s 655us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 54/600\n",
      "7164/7164 [==============================] - 5s 654us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 55/600\n",
      "7164/7164 [==============================] - 5s 657us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 56/600\n",
      "7164/7164 [==============================] - 5s 653us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 57/600\n",
      "7164/7164 [==============================] - 5s 656us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 58/600\n",
      "7164/7164 [==============================] - 5s 656us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 59/600\n",
      "7164/7164 [==============================] - 5s 638us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 60/600\n",
      "7164/7164 [==============================] - 5s 653us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 61/600\n",
      "7164/7164 [==============================] - 5s 652us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 62/600\n",
      "7164/7164 [==============================] - 5s 653us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 63/600\n",
      "7164/7164 [==============================] - 5s 658us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 64/600\n",
      "7164/7164 [==============================] - 5s 651us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 65/600\n",
      "7164/7164 [==============================] - 5s 653us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 66/600\n",
      "7164/7164 [==============================] - 5s 656us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 67/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 68/600\n",
      "7164/7164 [==============================] - 6s 799us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 69/600\n",
      "7164/7164 [==============================] - 6s 768us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 70/600\n",
      "7164/7164 [==============================] - 6s 777us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 71/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 72/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 73/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 74/600\n",
      "7164/7164 [==============================] - 6s 795us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 75/600\n",
      "7164/7164 [==============================] - 6s 770us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 76/600\n",
      "7164/7164 [==============================] - 5s 735us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 77/600\n",
      "7164/7164 [==============================] - 5s 712us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 78/600\n",
      "7164/7164 [==============================] - 5s 765us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 79/600\n",
      "7164/7164 [==============================] - 6s 771us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 80/600\n",
      "7164/7164 [==============================] - 5s 697us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 81/600\n",
      "7164/7164 [==============================] - 5s 730us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 82/600\n",
      "7164/7164 [==============================] - 6s 828us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 83/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 84/600\n",
      "7164/7164 [==============================] - 5s 704us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 85/600\n",
      "7164/7164 [==============================] - 5s 724us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 86/600\n",
      "7164/7164 [==============================] - 5s 739us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 87/600\n",
      "7164/7164 [==============================] - 5s 717us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 88/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 89/600\n",
      "7164/7164 [==============================] - 5s 761us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 90/600\n",
      "7164/7164 [==============================] - 5s 685us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 91/600\n",
      "7164/7164 [==============================] - 5s 757us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 92/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 93/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 94/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 95/600\n",
      "7164/7164 [==============================] - 5s 710us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 96/600\n",
      "7164/7164 [==============================] - 5s 733us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 97/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 98/600\n",
      "7164/7164 [==============================] - 5s 749us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 99/600\n",
      "7164/7164 [==============================] - 5s 681us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 100/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 101/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 102/600\n",
      "7164/7164 [==============================] - 5s 749us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 103/600\n",
      "7164/7164 [==============================] - 7s 926us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 104/600\n",
      "7164/7164 [==============================] - 5s 689us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 105/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 106/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 107/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 108/600\n",
      "7164/7164 [==============================] - 6s 892us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 109/600\n",
      "7164/7164 [==============================] - 5s 658us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 110/600\n",
      "7164/7164 [==============================] - 5s 660us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 111/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 112/600\n",
      "7164/7164 [==============================] - 5s 676us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 113/600\n",
      "7164/7164 [==============================] - 5s 681us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 114/600\n",
      "7164/7164 [==============================] - 5s 675us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 115/600\n",
      "7164/7164 [==============================] - 5s 647us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 116/600\n",
      "7164/7164 [==============================] - 5s 641us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 117/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 118/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 119/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 662us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 120/600\n",
      "7164/7164 [==============================] - 5s 661us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 121/600\n",
      "7164/7164 [==============================] - 5s 656us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 122/600\n",
      "7164/7164 [==============================] - 5s 655us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 123/600\n",
      "7164/7164 [==============================] - 6s 841us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 124/600\n",
      "7164/7164 [==============================] - 6s 832us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 125/600\n",
      "7164/7164 [==============================] - 5s 733us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 126/600\n",
      "7164/7164 [==============================] - 6s 826us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 127/600\n",
      "7164/7164 [==============================] - 6s 806us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 128/600\n",
      "7164/7164 [==============================] - 6s 771us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 129/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 130/600\n",
      "7164/7164 [==============================] - 5s 758us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 131/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 132/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 133/600\n",
      "7164/7164 [==============================] - 6s 805us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 134/600\n",
      "7164/7164 [==============================] - 7s 923us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 135/600\n",
      "7164/7164 [==============================] - 5s 744us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 136/600\n",
      "7164/7164 [==============================] - 6s 770us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 137/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 138/600\n",
      "7164/7164 [==============================] - 6s 803us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 139/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 140/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 141/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 142/600\n",
      "7164/7164 [==============================] - 5s 724us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 143/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 144/600\n",
      "7164/7164 [==============================] - 6s 839us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 145/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 146/600\n",
      "7164/7164 [==============================] - 6s 880us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 147/600\n",
      "7164/7164 [==============================] - 6s 838us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 148/600\n",
      "7164/7164 [==============================] - 6s 841us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 149/600\n",
      "7164/7164 [==============================] - 6s 856us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 150/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 151/600\n",
      "7164/7164 [==============================] - 7s 914us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 152/600\n",
      "7164/7164 [==============================] - 5s 748us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 153/600\n",
      "7164/7164 [==============================] - 6s 830us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 154/600\n",
      "7164/7164 [==============================] - 7s 962us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 155/600\n",
      "7164/7164 [==============================] - 6s 782us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 156/600\n",
      "7164/7164 [==============================] - 6s 855us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 157/600\n",
      "7164/7164 [==============================] - 5s 766us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 158/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 159/600\n",
      "7164/7164 [==============================] - 6s 884us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 160/600\n",
      "7164/7164 [==============================] - 6s 806us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 161/600\n",
      "7164/7164 [==============================] - 6s 836us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 162/600\n",
      "7164/7164 [==============================] - 6s 784us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 163/600\n",
      "7164/7164 [==============================] - 5s 754us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 164/600\n",
      "7164/7164 [==============================] - 6s 812us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 165/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 166/600\n",
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 167/600\n",
      "7164/7164 [==============================] - 6s 877us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 168/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 169/600\n",
      "7164/7164 [==============================] - 7s 999us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 170/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 171/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 172/600\n",
      "7164/7164 [==============================] - 6s 807us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 173/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 174/600\n",
      "7164/7164 [==============================] - 5s 722us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 175/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 176/600\n",
      "7164/7164 [==============================] - 6s 793us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 177/600\n",
      "7164/7164 [==============================] - 5s 746us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 178/600\n",
      "7164/7164 [==============================] - 6s 900us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 179/600\n",
      "7164/7164 [==============================] - 6s 871us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 180/600\n",
      "7164/7164 [==============================] - 5s 748us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 181/600\n",
      "7164/7164 [==============================] - 5s 741us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 182/600\n",
      "7164/7164 [==============================] - 6s 861us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 183/600\n",
      "7164/7164 [==============================] - 6s 810us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 184/600\n",
      "7164/7164 [==============================] - 6s 792us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 185/600\n",
      "7164/7164 [==============================] - 6s 862us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 186/600\n",
      "7164/7164 [==============================] - 6s 827us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 187/600\n",
      "7164/7164 [==============================] - 7s 983us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 188/600\n",
      "7164/7164 [==============================] - 6s 899us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 189/600\n",
      "7164/7164 [==============================] - 6s 815us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 190/600\n",
      "7164/7164 [==============================] - 6s 837us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 191/600\n",
      "7164/7164 [==============================] - 6s 796us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 192/600\n",
      "7164/7164 [==============================] - 6s 855us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 193/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 194/600\n",
      "7164/7164 [==============================] - 6s 800us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 195/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 196/600\n",
      "7164/7164 [==============================] - 6s 821us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 197/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 198/600\n",
      "7164/7164 [==============================] - 6s 825us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 199/600\n",
      "7164/7164 [==============================] - 5s 745us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 200/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 201/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 202/600\n",
      "7164/7164 [==============================] - 5s 710us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 203/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 204/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 205/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 206/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 207/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 208/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 209/600\n",
      "7164/7164 [==============================] - 5s 690us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 210/600\n",
      "7164/7164 [==============================] - 5s 689us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 211/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 212/600\n",
      "7164/7164 [==============================] - 5s 682us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 213/600\n",
      "7164/7164 [==============================] - 5s 659us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 214/600\n",
      "7164/7164 [==============================] - 5s 673us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 215/600\n",
      "7164/7164 [==============================] - 5s 666us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 216/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 217/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 218/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 219/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 220/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 221/600\n",
      "7164/7164 [==============================] - 5s 657us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 222/600\n",
      "7164/7164 [==============================] - 5s 715us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 223/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 224/600\n",
      "7164/7164 [==============================] - 5s 656us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 225/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 226/600\n",
      "7164/7164 [==============================] - 5s 667us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 227/600\n",
      "7164/7164 [==============================] - 5s 669us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 228/600\n",
      "7164/7164 [==============================] - 5s 657us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 229/600\n",
      "7164/7164 [==============================] - 5s 677us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 230/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 231/600\n",
      "7164/7164 [==============================] - 6s 795us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 232/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 233/600\n",
      "7164/7164 [==============================] - 5s 756us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 234/600\n",
      "7164/7164 [==============================] - 7s 987us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 235/600\n",
      "7164/7164 [==============================] - 6s 819us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 236/600\n",
      "7164/7164 [==============================] - 5s 765us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 237/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 238/600\n",
      "7164/7164 [==============================] - 6s 814us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 239/600\n",
      "7164/7164 [==============================] - 5s 742us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 240/600\n",
      "7164/7164 [==============================] - 6s 778us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 241/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 242/600\n",
      "7164/7164 [==============================] - 5s 761us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 243/600\n",
      "7164/7164 [==============================] - 6s 813us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 244/600\n",
      "7164/7164 [==============================] - 6s 780us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 245/600\n",
      "7164/7164 [==============================] - 6s 849us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 246/600\n",
      "7164/7164 [==============================] - 6s 876us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 247/600\n",
      "7164/7164 [==============================] - 6s 863us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 248/600\n",
      "7164/7164 [==============================] - 6s 790us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 249/600\n",
      "7164/7164 [==============================] - 6s 820us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 250/600\n",
      "7164/7164 [==============================] - 6s 857us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 251/600\n",
      "7164/7164 [==============================] - 6s 823us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 252/600\n",
      "7164/7164 [==============================] - 6s 865us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 253/600\n",
      "7164/7164 [==============================] - 6s 810us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 254/600\n",
      "7164/7164 [==============================] - 7s 916us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 255/600\n",
      "7164/7164 [==============================] - 6s 813us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 256/600\n",
      "7164/7164 [==============================] - 6s 832us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 257/600\n",
      "7164/7164 [==============================] - 6s 844us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 258/600\n",
      "7164/7164 [==============================] - 6s 855us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 259/600\n",
      "7164/7164 [==============================] - 6s 870us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 260/600\n",
      "7164/7164 [==============================] - 6s 850us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 261/600\n",
      "7164/7164 [==============================] - 6s 864us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 262/600\n",
      "7164/7164 [==============================] - 6s 904us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 263/600\n",
      "7164/7164 [==============================] - 6s 859us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 264/600\n",
      "7164/7164 [==============================] - 6s 815us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 265/600\n",
      "7164/7164 [==============================] - 7s 929us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 266/600\n",
      "7164/7164 [==============================] - 7s 960us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 267/600\n",
      "7164/7164 [==============================] - 6s 798us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 268/600\n",
      "7164/7164 [==============================] - 6s 869us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 269/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 270/600\n",
      "7164/7164 [==============================] - 5s 756us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 271/600\n",
      "7164/7164 [==============================] - 6s 856us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 272/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 273/600\n",
      "7164/7164 [==============================] - 6s 815us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 274/600\n",
      "7164/7164 [==============================] - 6s 906us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 275/600\n",
      "7164/7164 [==============================] - 6s 892us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 276/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 277/600\n",
      "7164/7164 [==============================] - 5s 691us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 278/600\n",
      "7164/7164 [==============================] - 5s 674us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 279/600\n",
      "7164/7164 [==============================] - 5s 719us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 280/600\n",
      "7164/7164 [==============================] - 5s 694us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 281/600\n",
      "7164/7164 [==============================] - 6s 805us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 282/600\n",
      "7164/7164 [==============================] - 7s 914us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 283/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 284/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 285/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 286/600\n",
      "7164/7164 [==============================] - 6s 844us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 287/600\n",
      "7164/7164 [==============================] - 7s 937us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 288/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 289/600\n",
      "7164/7164 [==============================] - 6s 897us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 290/600\n",
      "7164/7164 [==============================] - 6s 906us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 291/600\n",
      "7164/7164 [==============================] - 7s 981us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 292/600\n",
      "7164/7164 [==============================] - 6s 829us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 293/600\n",
      "7164/7164 [==============================] - 6s 821us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 294/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 295/600\n",
      "7164/7164 [==============================] - 6s 803us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 296/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 297/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 298/600\n",
      "7164/7164 [==============================] - 7s 990us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 299/600\n",
      "7164/7164 [==============================] - 6s 840us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 300/600\n",
      "7164/7164 [==============================] - 5s 738us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 301/600\n",
      "7164/7164 [==============================] - 5s 720us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 302/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 303/600\n",
      "7164/7164 [==============================] - 5s 717us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 304/600\n",
      "7164/7164 [==============================] - 5s 741us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 305/600\n",
      "7164/7164 [==============================] - 5s 713us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 306/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 307/600\n",
      "7164/7164 [==============================] - 5s 721us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 308/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 309/600\n",
      "7164/7164 [==============================] - 6s 771us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 310/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 311/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 312/600\n",
      "7164/7164 [==============================] - 7s 971us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 313/600\n",
      "7164/7164 [==============================] - 6s 899us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 314/600\n",
      "7164/7164 [==============================] - 7s 911us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 315/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 316/600\n",
      "7164/7164 [==============================] - 5s 717us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 317/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 318/600\n",
      "7164/7164 [==============================] - 5s 680us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 319/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 320/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 321/600\n",
      "7164/7164 [==============================] - 5s 699us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 322/600\n",
      "7164/7164 [==============================] - 7s 926us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 323/600\n",
      "7164/7164 [==============================] - 6s 879us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 324/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 325/600\n",
      "7164/7164 [==============================] - 7s 952us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 326/600\n",
      "7164/7164 [==============================] - 5s 759us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 327/600\n",
      "7164/7164 [==============================] - 5s 738us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 328/600\n",
      "7164/7164 [==============================] - 5s 745us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 329/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 330/600\n",
      "7164/7164 [==============================] - 5s 671us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 331/600\n",
      "7164/7164 [==============================] - 5s 756us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 332/600\n",
      "7164/7164 [==============================] - 6s 815us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 333/600\n",
      "7164/7164 [==============================] - 5s 720us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 334/600\n",
      "7164/7164 [==============================] - 5s 753us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 335/600\n",
      "7164/7164 [==============================] - 5s 742us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 336/600\n",
      "7164/7164 [==============================] - 5s 737us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 337/600\n",
      "7164/7164 [==============================] - 5s 712us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 338/600\n",
      "7164/7164 [==============================] - 6s 787us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 339/600\n",
      "7164/7164 [==============================] - 5s 722us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 340/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 341/600\n",
      "7164/7164 [==============================] - 5s 727us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 342/600\n",
      "7164/7164 [==============================] - 6s 794us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 343/600\n",
      "7164/7164 [==============================] - 6s 794us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 344/600\n",
      "7164/7164 [==============================] - 5s 693us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 345/600\n",
      "7164/7164 [==============================] - 6s 781us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 346/600\n",
      "7164/7164 [==============================] - 5s 687us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 347/600\n",
      "7164/7164 [==============================] - 5s 693us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 348/600\n",
      "7164/7164 [==============================] - 5s 678us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 349/600\n",
      "7164/7164 [==============================] - 5s 692us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 350/600\n",
      "7164/7164 [==============================] - 5s 670us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 351/600\n",
      "7164/7164 [==============================] - 5s 693us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 352/600\n",
      "7164/7164 [==============================] - 5s 746us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 353/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 354/600\n",
      "7164/7164 [==============================] - 6s 812us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 355/600\n",
      "7164/7164 [==============================] - 6s 790us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 356/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 357/600\n",
      "7164/7164 [==============================] - 6s 826us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 358/600\n",
      "7164/7164 [==============================] - 6s 849us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 359/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 360/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 361/600\n",
      "7164/7164 [==============================] - 6s 862us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 362/600\n",
      "7164/7164 [==============================] - 6s 870us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 363/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 364/600\n",
      "7164/7164 [==============================] - 7s 990us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 365/600\n",
      "7164/7164 [==============================] - 6s 802us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 366/600\n",
      "7164/7164 [==============================] - 7s 940us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 367/600\n",
      "7164/7164 [==============================] - 10s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 368/600\n",
      "7164/7164 [==============================] - 16s 2ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 369/600\n",
      "7164/7164 [==============================] - 11s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 370/600\n",
      "7164/7164 [==============================] - 7s 912us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 371/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 372/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 373/600\n",
      "7164/7164 [==============================] - 9s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 374/600\n",
      "7164/7164 [==============================] - 7s 929us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 375/600\n",
      "7164/7164 [==============================] - 6s 850us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 376/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 377/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 378/600\n",
      "7164/7164 [==============================] - 7s 911us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 379/600\n",
      "7164/7164 [==============================] - 6s 816us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 380/600\n",
      "7164/7164 [==============================] - 6s 902us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 381/600\n",
      "7164/7164 [==============================] - 6s 802us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 382/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 383/600\n",
      "7164/7164 [==============================] - 6s 867us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 384/600\n",
      "7164/7164 [==============================] - 6s 883us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 385/600\n",
      "7164/7164 [==============================] - 6s 780us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 386/600\n",
      "7164/7164 [==============================] - 6s 841us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 387/600\n",
      "7164/7164 [==============================] - 5s 765us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 388/600\n",
      "7164/7164 [==============================] - 6s 808us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 389/600\n",
      "7164/7164 [==============================] - 6s 833us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 390/600\n",
      "7164/7164 [==============================] - 5s 766us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 391/600\n",
      "7164/7164 [==============================] - 6s 851us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 392/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 393/600\n",
      "7164/7164 [==============================] - 6s 880us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 394/600\n",
      "7164/7164 [==============================] - 6s 879us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 395/600\n",
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 396/600\n",
      "7164/7164 [==============================] - 6s 788us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 397/600\n",
      "7164/7164 [==============================] - 6s 770us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 398/600\n",
      "7164/7164 [==============================] - 6s 782us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 399/600\n",
      "7164/7164 [==============================] - 5s 763us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 400/600\n",
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 401/600\n",
      "7164/7164 [==============================] - 5s 759us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 402/600\n",
      "7164/7164 [==============================] - 6s 795us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 403/600\n",
      "7164/7164 [==============================] - 6s 818us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 404/600\n",
      "7164/7164 [==============================] - 6s 775us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 405/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 406/600\n",
      "7164/7164 [==============================] - 5s 726us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 407/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 408/600\n",
      "7164/7164 [==============================] - 5s 729us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 409/600\n",
      "7164/7164 [==============================] - 5s 715us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 410/600\n",
      "7164/7164 [==============================] - 6s 883us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 411/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 412/600\n",
      "7164/7164 [==============================] - 5s 757us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 413/600\n",
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 414/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 766us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 415/600\n",
      "7164/7164 [==============================] - 5s 737us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 416/600\n",
      "7164/7164 [==============================] - 6s 768us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 417/600\n",
      "7164/7164 [==============================] - 6s 781us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 418/600\n",
      "7164/7164 [==============================] - 6s 793us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 419/600\n",
      "7164/7164 [==============================] - 6s 899us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 420/600\n",
      "7164/7164 [==============================] - 7s 977us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 421/600\n",
      "7164/7164 [==============================] - 6s 807us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 422/600\n",
      "7164/7164 [==============================] - 6s 873us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 423/600\n",
      "7164/7164 [==============================] - 6s 883us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 424/600\n",
      "7164/7164 [==============================] - 6s 865us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 425/600\n",
      "7164/7164 [==============================] - 6s 884us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 426/600\n",
      "7164/7164 [==============================] - 7s 971us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 427/600\n",
      "7164/7164 [==============================] - 6s 902us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 428/600\n",
      "7164/7164 [==============================] - 6s 879us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 429/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 430/600\n",
      "7164/7164 [==============================] - 5s 702us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 431/600\n",
      "7164/7164 [==============================] - 5s 716us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 432/600\n",
      "7164/7164 [==============================] - 5s 692us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 433/600\n",
      "7164/7164 [==============================] - 6s 786us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 434/600\n",
      "7164/7164 [==============================] - 7s 932us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 435/600\n",
      "7164/7164 [==============================] - 6s 901us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 436/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 437/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 438/600\n",
      "7164/7164 [==============================] - 7s 953us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 439/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 440/600\n",
      "7164/7164 [==============================] - 5s 738us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 441/600\n",
      "7164/7164 [==============================] - 6s 771us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 442/600\n",
      "7164/7164 [==============================] - 5s 743us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 443/600\n",
      "7164/7164 [==============================] - 5s 763us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 444/600\n",
      "7164/7164 [==============================] - 6s 773us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 445/600\n",
      "7164/7164 [==============================] - 7s 918us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 446/600\n",
      "7164/7164 [==============================] - 7s 916us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 447/600\n",
      "7164/7164 [==============================] - 7s 908us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 448/600\n",
      "7164/7164 [==============================] - 6s 794us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 449/600\n",
      "7164/7164 [==============================] - 5s 688us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 450/600\n",
      "7164/7164 [==============================] - 6s 815us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 451/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 452/600\n",
      "7164/7164 [==============================] - 5s 727us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 453/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 454/600\n",
      "7164/7164 [==============================] - 6s 800us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 455/600\n",
      "7164/7164 [==============================] - 6s 806us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 456/600\n",
      "7164/7164 [==============================] - 6s 832us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 457/600\n",
      "7164/7164 [==============================] - 5s 679us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 458/600\n",
      "7164/7164 [==============================] - 5s 724us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 459/600\n",
      "7164/7164 [==============================] - 5s 696us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 460/600\n",
      "7164/7164 [==============================] - 7s 909us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 461/600\n",
      "7164/7164 [==============================] - 6s 812us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 462/600\n",
      "7164/7164 [==============================] - 5s 719us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 463/600\n",
      "7164/7164 [==============================] - 5s 708us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 464/600\n",
      "7164/7164 [==============================] - 5s 712us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 465/600\n",
      "7164/7164 [==============================] - 5s 701us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 466/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 467/600\n",
      "7164/7164 [==============================] - 5s 698us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 468/600\n",
      "7164/7164 [==============================] - 5s 715us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 469/600\n",
      "7164/7164 [==============================] - 5s 705us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 470/600\n",
      "7164/7164 [==============================] - 5s 738us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 471/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 472/600\n",
      "7164/7164 [==============================] - 5s 732us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 473/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 715us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 474/600\n",
      "7164/7164 [==============================] - 5s 719us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 475/600\n",
      "7164/7164 [==============================] - 5s 741us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 476/600\n",
      "7164/7164 [==============================] - 5s 716us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 477/600\n",
      "7164/7164 [==============================] - 5s 726us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 478/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 479/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 480/600\n",
      "7164/7164 [==============================] - 5s 709us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 481/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 482/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 483/600\n",
      "7164/7164 [==============================] - 5s 756us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 484/600\n",
      "7164/7164 [==============================] - 5s 764us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 485/600\n",
      "7164/7164 [==============================] - 5s 762us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 486/600\n",
      "7164/7164 [==============================] - 5s 762us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 487/600\n",
      "7164/7164 [==============================] - 5s 744us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 488/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 489/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 490/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 491/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 492/600\n",
      "7164/7164 [==============================] - 6s 777us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 493/600\n",
      "7164/7164 [==============================] - 5s 766us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 494/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 495/600\n",
      "7164/7164 [==============================] - 6s 773us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 496/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 497/600\n",
      "7164/7164 [==============================] - 6s 785us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 498/600\n",
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 499/600\n",
      "7164/7164 [==============================] - 5s 756us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 500/600\n",
      "7164/7164 [==============================] - 5s 757us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 501/600\n",
      "7164/7164 [==============================] - 5s 751us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 502/600\n",
      "7164/7164 [==============================] - 6s 785us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 503/600\n",
      "7164/7164 [==============================] - 5s 755us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 504/600\n",
      "7164/7164 [==============================] - 6s 784us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 505/600\n",
      "7164/7164 [==============================] - 6s 774us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 506/600\n",
      "7164/7164 [==============================] - 5s 749us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 507/600\n",
      "7164/7164 [==============================] - 5s 748us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 508/600\n",
      "7164/7164 [==============================] - 5s 746us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 509/600\n",
      "7164/7164 [==============================] - 5s 739us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 510/600\n",
      "7164/7164 [==============================] - 5s 754us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 511/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 512/600\n",
      "7164/7164 [==============================] - 5s 756us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 513/600\n",
      "7164/7164 [==============================] - 5s 767us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 514/600\n",
      "7164/7164 [==============================] - 5s 743us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 515/600\n",
      "7164/7164 [==============================] - 5s 730us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 516/600\n",
      "7164/7164 [==============================] - 5s 724us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 517/600\n",
      "7164/7164 [==============================] - 5s 753us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 518/600\n",
      "7164/7164 [==============================] - 5s 725us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 519/600\n",
      "7164/7164 [==============================] - 5s 736us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 520/600\n",
      "7164/7164 [==============================] - 5s 730us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 521/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 522/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 523/600\n",
      "7164/7164 [==============================] - 5s 726us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 524/600\n",
      "7164/7164 [==============================] - 5s 732us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 525/600\n",
      "7164/7164 [==============================] - 5s 707us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 526/600\n",
      "7164/7164 [==============================] - 5s 750us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 527/600\n",
      "7164/7164 [==============================] - 5s 718us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 528/600\n",
      "7164/7164 [==============================] - 5s 732us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 529/600\n",
      "7164/7164 [==============================] - 5s 760us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 530/600\n",
      "7164/7164 [==============================] - 5s 728us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 531/600\n",
      "7164/7164 [==============================] - 5s 730us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 532/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 6s 805us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 533/600\n",
      "7164/7164 [==============================] - 6s 776us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 534/600\n",
      "7164/7164 [==============================] - 6s 821us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 535/600\n",
      "7164/7164 [==============================] - 6s 811us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 536/600\n",
      "7164/7164 [==============================] - 6s 815us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 537/600\n",
      "7164/7164 [==============================] - 6s 823us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 538/600\n",
      "7164/7164 [==============================] - 6s 798us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 539/600\n",
      "7164/7164 [==============================] - 6s 778us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 540/600\n",
      "7164/7164 [==============================] - 6s 822us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 541/600\n",
      "7164/7164 [==============================] - 6s 891us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 542/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 543/600\n",
      "7164/7164 [==============================] - 6s 820us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 544/600\n",
      "7164/7164 [==============================] - 5s 740us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 545/600\n",
      "7164/7164 [==============================] - 6s 805us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 546/600\n",
      "7164/7164 [==============================] - 7s 961us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 547/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 548/600\n",
      "7164/7164 [==============================] - 6s 853us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 549/600\n",
      "7164/7164 [==============================] - 6s 901us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 550/600\n",
      "7164/7164 [==============================] - 8s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 551/600\n",
      "7164/7164 [==============================] - 6s 846us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 552/600\n",
      "7164/7164 [==============================] - 6s 880us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 553/600\n",
      "7164/7164 [==============================] - 5s 727us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 554/600\n",
      "7164/7164 [==============================] - 6s 768us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 555/600\n",
      "7164/7164 [==============================] - 5s 757us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 556/600\n",
      "7164/7164 [==============================] - 6s 835us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 557/600\n",
      "7164/7164 [==============================] - 6s 830us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 558/600\n",
      "7164/7164 [==============================] - 6s 781us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 559/600\n",
      "7164/7164 [==============================] - 6s 804us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 560/600\n",
      "7164/7164 [==============================] - 6s 852us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 561/600\n",
      "7164/7164 [==============================] - 6s 791us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 562/600\n",
      "7164/7164 [==============================] - 6s 834us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 563/600\n",
      "7164/7164 [==============================] - 6s 842us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 564/600\n",
      "7164/7164 [==============================] - 6s 811us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 565/600\n",
      "7164/7164 [==============================] - 6s 825us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 566/600\n",
      "7164/7164 [==============================] - 6s 788us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 567/600\n",
      "7164/7164 [==============================] - 6s 806us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 568/600\n",
      "7164/7164 [==============================] - 5s 753us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 569/600\n",
      "7164/7164 [==============================] - 5s 747us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 570/600\n",
      "7164/7164 [==============================] - 5s 711us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 571/600\n",
      "7164/7164 [==============================] - 5s 683us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 572/600\n",
      "7164/7164 [==============================] - 5s 685us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 573/600\n",
      "7164/7164 [==============================] - 6s 841us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 574/600\n",
      "7164/7164 [==============================] - 7s 947us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 575/600\n",
      "7164/7164 [==============================] - 6s 817us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 576/600\n",
      "7164/7164 [==============================] - 5s 761us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 577/600\n",
      "7164/7164 [==============================] - 6s 895us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 578/600\n",
      "7164/7164 [==============================] - 6s 789us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 579/600\n",
      "7164/7164 [==============================] - 6s 812us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 580/600\n",
      "7164/7164 [==============================] - 5s 723us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 581/600\n",
      "7164/7164 [==============================] - 5s 734us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 582/600\n",
      "7164/7164 [==============================] - 5s 742us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 583/600\n",
      "7164/7164 [==============================] - 6s 809us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 584/600\n",
      "7164/7164 [==============================] - 6s 787us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 585/600\n",
      "7164/7164 [==============================] - 5s 743us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 586/600\n",
      "7164/7164 [==============================] - 5s 739us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 587/600\n",
      "7164/7164 [==============================] - 6s 800us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 588/600\n",
      "7164/7164 [==============================] - 5s 752us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 589/600\n",
      "7164/7164 [==============================] - 6s 822us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 590/600\n",
      "7164/7164 [==============================] - 5s 723us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 591/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7164/7164 [==============================] - 5s 704us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 592/600\n",
      "7164/7164 [==============================] - 5s 732us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 593/600\n",
      "7164/7164 [==============================] - 7s 992us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 594/600\n",
      "7164/7164 [==============================] - 7s 1ms/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 595/600\n",
      "7164/7164 [==============================] - 6s 881us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 596/600\n",
      "7164/7164 [==============================] - 6s 772us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 597/600\n",
      "7164/7164 [==============================] - 6s 778us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 598/600\n",
      "7164/7164 [==============================] - 6s 874us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 599/600\n",
      "7164/7164 [==============================] - 6s 870us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "Epoch 600/600\n",
      "7164/7164 [==============================] - 6s 790us/step - loss: 0.0045 - acc: 0.9997 - val_loss: 4.2526 - val_acc: 0.7048\n",
      "1292/1292 [==============================] - 0s 102us/step\n",
      "==========================================================================\n",
      "FOLD 7\n",
      "==========================================================================\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461, 1.0, 0.32739938080495357]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461, 1.0, 0.32739938080495357]\n",
      "RF Confusion Matrix: \n",
      "[[138  39   0]\n",
      " [404  64   0]\n",
      " [  0 426 221]]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436, 0.35577658760520275, 0.6377708978328174]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436, 0.35577658760520275, 0.6377708978328174]\n",
      "SVM Confusion Matrix: \n",
      "[[177   0   0]\n",
      " [404   0  64]\n",
      " [  0   0 647]]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663, 0.5470543228768171, 0.32739938080495357]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663, 0.5470543228768171, 0.32739938080495357]\n",
      "DT Confusion Matrix: \n",
      "[[138  39   0]\n",
      " [404  64   0]\n",
      " [  0 426 221]]\n",
      "sNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355, 1.0, 0.9920697858842189, 0.7547021943573667, 0.7077276205049732, 1.0]\n",
      "sNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355], [1.1920928955078125e-07, 1.0], [0.0848679807713635, 0.9920697858842189], [3.9528459112472296, 0.7547021943573667], [4.517363900707802, 0.7077276205220748], [1.1939382405472983e-07, 1.0]]\n",
      "sNN Confusion Matrix: \n",
      "[[177   0   0]\n",
      " [  0 468   0]\n",
      " [  0   0 647]]\n",
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure 8\n",
      "==========================================================================\n",
      "Accuracy summaries\n",
      "RF Accuracy A: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461, 1.0, 0.32739938080495357]\n",
      "RF Accuracy B: [0.6702371843917369, 0.6449044585987261, 0.26412092283214, 0.5054179566563467, 0.24107850911974624, 0.3236677115987461, 1.0, 0.32739938080495357]\n",
      "SVM Accuracy A: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436, 0.35577658760520275, 0.6377708978328174]\n",
      "SVM Accuracy B: [0.7712318286151492, 0.6449044585987261, 0.624502784407319, 0.30030959752321984, 0.23632038065027755, 0.24608150470219436, 0.35577658760520275, 0.6377708978328174]\n",
      "DT Accuracy A: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663, 0.5470543228768171, 0.32739938080495357]\n",
      "DT Accuracy B: [0.8163733741392502, 0.856687898089172, 0.624502784407319, 0.5054179566563467, 0.23632038065027755, 0.44905956112852663, 0.5470543228768171, 0.32739938080495357]\n",
      "SNN Accuracy A: [0.4269319051262433, 0.7953821656050956, 0.9896579156722355, 1.0, 0.9920697858842189, 0.7547021943573667, 0.7077276205049732, 1.0]\n",
      "SNN Accuracy B: [[5.707709207002133, 0.42693190500083184], [3.228106459994225, 0.7953821656050956], [0.12490938013775284, 0.9896579156722355], [1.1920928955078125e-07, 1.0], [0.0848679807713635, 0.9920697858842189], [3.9528459112472296, 0.7547021943573667], [4.517363900707802, 0.7077276205220748], [1.1939382405472983e-07, 1.0]]\n",
      "Acurracy A is manually calculated\n",
      "==========================================================================\n",
      "==========================================================================\n",
      "FINAL RESULTS\n",
      "==========================================================================\n",
      "RF Mean 0.49710326550029943, SD 0.24584129889122658\n",
      "SVM Mean 0.47711225499186327, SD 0.19985406333651878\n",
      "DT Mean 0.5453519573440828, SD 0.2032612908389891\n",
      "SNN Mean 0.8333089483937666, SD 0.19162210542592117\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAALICAYAAABiqwZ2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deZSkV33n6e+vFrQLkFSokWQhGQMGBJagoMEYA2M2iQGzeDAWdBufscXSjOGYxcIHPGBjg2cYzLAIkEcaY2hDyyzGNLhHYKMGzFqAMItkLW2ESpJFISjtG1V3/ojI0q0kUlkJWRGhfJ/nnDgVGUu+N/M9lfmpWzduVGstAADAyLpZDwAAAOaJQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGuIOoqr+sqtft4WO/U1WP/Wk/D8AQCWQAAOgIZAAA6AhkgFU0Xtrw8qr656q6vqrOqKrDq+rvq+raqvpkVd21e/xTqupbVbW9qs6pqvt2951QVV8dP++/JNl30bH+56o6d/zcz1XVA3/CMf9OVV1UVT+oqr+rqiPGt1dV/XlVfa+qrh5/TceN7zupqr49HttlVfWyn+gbBjCHBDLA6ntGkscluXeSJyf5+yR/kOSwjH7u/m6SVNW9k7wvyUuSbEry8SQfrao7VdWdkvxtkvckOSTJ34w/b8bPfVCSM5M8L8mhSd6V5O+qap+VDLSq/qckr0/yzCR3T3JJkveP7358kl8efx13SfLrSa4a33dGkue11g5KclySf1zJcQHmmUAGWH1vba1d2Vq7LMlnknyxtfa11trNST6c5ITx4349ycdaa59ord2a5I1J9kvyi0kelmRjkje31m5trX0gyZe7Y/xOkne11r7YWtvRWnt3kpvHz1uJZyc5s7X21fH4Xpnk4VV1TJJbkxyU5OeTVGvtvNbaFePn3ZrkflV1cGvth621r67wuABzSyADrL4ru+s3Tvj4wPH1IzKasU2StNZ2Jrk0yZHj+y5rrbXuuZd01++R5KXj5RXbq2p7kp8ZP28lFo/huoxmiY9srf1jkrcleXuSK6vq9Ko6ePzQZyQ5KcklVfXfq+rhKzwuwNwSyACzc3lGoZtktOY3o8i9LMkVSY4c37bg6O76pUn+pLV2l+6yf2vtfT/lGA7IaMnGZUnSWntLa+3BSe6f0VKLl49v/3Jr7VeT3C2jpSBnrfC4AHNLIAPMzllJnlRVv1JVG5O8NKNlEp9L8vkkP0ryu1W1oaqenuSh3XP/Isnzq+rfj19Md0BVPamqDlrhGP46yW9V1fHj9ct/mtGSkO9U1UPGn39jkuuT3JRkx3iN9LOr6s7jpSHXJNnxU3wfAOaKQAaYkdbavyR5TpK3Jvl+Ri/oe3Jr7ZbW2i1Jnp7kuUl+mNF65Q91z92S0Trkt43vv2j82JWO4R+SvDrJBzOatb5nkmeN7z44oxD/YUbLMK7KaJ10kvyHJN+pqmuSPH/8dQCsCbX78jYAABg2M8gAANARyAAA0BHIAADQEcgAANDZMOsBLOewww5rxxxzzKyHAQDAGvOVr3zl+621TYtvn/tAPuaYY7Jly5ZZDwMAgDWmqi6ZdLslFgAA0JlaIFfVdYsuO6rqrdM6PgAA7ImpLbForR24cL2qDkhyZZK/mdbxAQBgT8xqDfKvJfleks/8JE++9dZbs3Xr1tx0002rO6o5s+++++aoo47Kxo0bZz0UAIDBmFUg/2aSv2pLvM91VZ2S5JQkOfroo3/s/q1bt+aggw7KMccck6raqwOdldZarrrqqmzdujXHHnvsrIcDADAYU3+RXlUdneRRSd691GNaa6e31ja31jZv2vRjO2/kpptuyqGHHrpm4zhJqiqHHnromp8lBwCYN7PYxeI/Jvlsa+1ff5pPspbjeMEQvkYAgHkzq0BecvYYAABmaaqBXFW/mOTI3MF3r9i+fXtOO+20FT/vpJNOyvbt2/fCiAAAWC3TnkH+zSQfaq1dO+XjrqqlAnnHjh23+7yPf/zjuctd7rK3hgUAwCqY6i4WrbXnTfN4e8upp56aiy++OMcff3w2btyYAw88MHe/+91z7rnn5tvf/nae+tSn5tJLL81NN92UF7/4xTnllFOS3Pa22dddd11OPPHE/NIv/VI+97nP5cgjj8xHPvKR7LfffjP+ygAAmNU2b6vmtR/9Vr59+TWr+jnvd8TB+d+ffP8l73/DG96Qb37zmzn33HNzzjnn5ElPelK++c1v7tqO7cwzz8whhxySG2+8MQ95yEPyjGc8I4ceeuhun+PCCy/M+973vvzFX/xFnvnMZ+aDH/xgnvOc56zq1wEAwMrd4QN5Hjz0oQ/dba/it7zlLfnwhz+cJLn00ktz4YUX/lggH3vssTn++OOTJA9+8IPzne98Z2rjBQBgaXf4QL69md5pOeCAA3ZdP+ecc/LJT34yn//857P//vvn0Y9+9MS9jPfZZ59d19evX58bb7xxKmMFAOD2zWKbtzu8gw46KNdeO/l1hldffXXuete7Zv/998/555+fL3zhC1MeHQAAP407/AzyLBx66KF5xCMekeOOOy777bdfDj/88F33PfGJT8w73/nOPPCBD8x97nOfPOxhD5vhSAEAWKlqrc16DLdr8+bNbcuWLbvddt555+W+973vjEY0XUP6WgEApqmqvtJa27z4dkssAACgI5ABAKAjkAEAoCOQAQCgI5ABAKAjkAEAoCOQfwLbt2/Paaed9hM9981vfnNuuOGGVR4RAACrRSD/BAQyAMDa5Z30fgKnnnpqLr744hx//PF53OMel7vd7W4566yzcvPNN+dpT3taXvva1+b666/PM5/5zGzdujU7duzIq1/96lx55ZW5/PLL85jHPCaHHXZYPvWpT836SwEAYJE7fiD//anJv31jdT/nv3tAcuIblrz7DW94Q775zW/m3HPPzdlnn50PfOAD+dKXvpTWWp7ylKfk05/+dLZt25YjjjgiH/vYx5IkV199de585zvnTW96Uz71qU/lsMMOW90xAwCwKiyxWMo1VyRXXbzsw84+++ycffbZOeGEE/KgBz0o559/fi688MI84AEPyCc/+cn8/u//fj7zmc/kzne+8xQGDQDAT+uOP4N8OzO9P5Uf/Gvyo5uXfVhrLa985SvzvOc978fu+8pXvpKPf/zjeeUrX5nHP/7x+cM//MO9MVIAAFaRGeTb1SbeetBBB+Xaa69NkjzhCU/ImWeemeuuuy5Jctlll+V73/teLr/88uy///55znOek5e97GX56le/+mPPBQBg/tzxZ5Bn4NBDD80jHvGIHHfccTnxxBNz8skn5+EPf3iS5MADD8x73/veXHTRRXn5y1+edevWZePGjXnHO96RJDnllFNy4okn5u53v7sX6QEAzKFqbfIs6bzYvHlz27Jly263nXfeebnvfe+7dw/8g/+R3Hpjcvj99+5xljGVrxUAYICq6iuttc2Lb7fEAgAAOgL59sz57DoAAKvvDhvIe31pyBy08bwvfwEAWIvukIG877775qqrrlrTAdlay1VXXZV999131kMBABiUO+QuFkcddVS2bt2abdu27b2DXL8t+dEtyQ/X771jLGPffffNUUcdNbPjAwAM0R0ykDdu3Jhjjz127x7kr1+TbN2SvGL5d9MDAGDtuEMusZiK1pK2c9ajAABgygTyklrm4pV6AABMlUBeSttpBhkAYIAE8lJasw8yAMAACeQlWYMMADBEAnkpZpABAAZJIC/JDDIAwBAJ5KXY5g0AYJAE8pIEMgDAEAnkpTT7IAMADJFAXoolFgAAgySQlySQAQCGSCAvZWGLN1u9AQAMikBekkAGABgigbyUXTPIllkAAAyJQF6SQAYAGCKBvJSFMBbIAACDIpCXsmvtsTXIAABDIpCXZIkFAMAQCeSleJEeAMAgCeQlCWQAgCESyEvxRiEAAIMkkJdkBhkAYIgE8lJ2bfNmBhkAYEgE8lJ27fJmBhkAYEgE8pLsgwwAMEQCeSm2eQMAGCSBvCSBDAAwRAJ5KWaQAQAGSSAvyT7IAABDJJCXYgYZAGCQBPJSdu2DLJABAIZEIC/JDDIAwBAJ5KVYewwAMEgCeUlmkAEAhkggL8WL9AAABkkgL0kgAwAMkUBeSrMPMgDAEAnkpVhiAQAwSAJ5SQIZAGCIBPJSzCADAAySQF5SW/QnAABDIJCXYgYZAGCQBPKS7GIBADBEAnkpZpABAAZJIC9lIYzNIAMADIpAXpIZZACAIRLIS7HEAgBgkATykgQyAMAQCeSlNPsgAwAM0dQDuaqeVVXnVdX1VXVxVT1y2mPYM2aQAQCGaMM0D1ZVj0vyZ0l+PcmXktx9msdfEWuQAQAGaaqBnOS1Sf6otfaF8ceXTfn4e04gAwAM0tSWWFTV+iSbk2yqqouqamtVva2q9pvw2FOqaktVbdm2bdu0hrhI2+0PAACGYZprkA9PsjHJryV5ZJLjk5yQ5FWLH9haO721trm1tnnTpk1THOJugxj/aQYZAGBIphnIN47/fGtr7YrW2veTvCnJSVMcwwoIZACAIZpaILfWfphka+4oixbMIAMADNK0t3n7f5P8b1V1t6q6a5KXJPmvUx7DHrIPMgDAEE17F4s/TnJYkguS3JTkrCR/MuUx7BkzyAAAgzTVQG6t3ZrkhePLfFsIY4EMADAo3mp6SWaQAQCGSCAvZdcSC2uQAQCGRCAvyQwyAMAQCeSlmEEGABgkgbwkM8gAAEMkkJfS7IMMADBEAnlJZpABAIZIIE/SrzsWyAAAgyKQJxHIAACDJZAn6gPZGmQAgCERyJOYQQYAGCyBPJFABgAYKoE8iRlkAIDBEsiTiGIAgMESyBOZQQYAGCqBPIklFgAAgyWQJxLIAABDJZAnafZBBgAYKoE8kRlkAIChEsiTWIMMADBYAnmSPooFMgDAoAjkidoS1wEAWOsE8iSWWAAADJZAXo5dLAAABkUgT2IGGQBgsATyRAIZAGCoBPIk3igEAGCwBPIktnkDABgsgTyRJRYAAEMlkCfxIj0AgMESyBN5oxAAgKESyJOYQQYAGCyBPJFdLAAAhkogT2IGGQBgsATyRGaQAQCGSiBPYh9kAIDBEsiTWGIBADBYAnkigQwAMFQCeZJmH2QAgKESyMsxgwwAMCgCeRJrkAEABksgTySQAQCGSiBPsts2b7MbBgAA0yeQJ7HEAgBgsATyRAIZAGCoBPIkZpABAAZLIE9kH2QAgKESyJOYQQYAGCyBPJFABgAYKoE8yW7bvAlkAIAhEciT7LbEwhpkAIAhEcgTWWIBADBUAnkSM8gAAIMlkCcygwwAMFQCeZK25AcAAKxxAnkiM8gAAEMlkCexzRsAwGAJ5Em8kx4AwGAJ5InsYgEAMFQCeRIzyAAAgyWQJxLIAABDJZAn8UYhAACDJZAnaktcBwBgrRPIkywsq6h1llgAAAyMQJ5kYVmFQAYAGByBPNE4kNdtEMgAAAMjkCfZNYO83ov0AAAGRiBPtDCDvN4MMgDAwAjkSXbNIJdABgAYGIE8Ub/EQiADAAyJQJ6kdUss7IMMADAoAnkSL9IDABgsgTyRF+kBAAyVQJ6kWYMMADBUAnmihRnkdZZYAAAMjECexAwyAMBgCeSJrEEGABiqqQZyVZ1TVTdV1XXjy79M8/h7bCGKzSADAAzOLGaQX9RaO3B8uc8Mjr+8Xfsgb0jajtmOBQCAqbLEYqJuicVOM8gAAEMyi0B+fVV9v6r+qaoePekBVXVKVW2pqi3btm2b8vBy2wzy+o3Jzh9N//gAAMzMtAP595P8bJIjk5ye5KNVdc/FD2qtnd5a29xa27xp06YpDzG5bQZ5oyUWAAADM9VAbq19sbV2bWvt5tbau5P8U5KTpjmGPdL6JRYCGQBgSGa9BrklqRmPYYJ+iYVABgAYkqkFclXdpaqeUFX7VtWGqnp2kl9O8v9Nawx7zC4WAACDtWGKx9qY5HVJfj7JjiTnJ3lqa23+9kLuA9kMMgDAoEwtkFtr25I8ZFrH++n0gWwXCwCAIZn1GuT5ZIkFAMBgCeSJuhfptZ23BTMAAGueQJ6kdfsgJ9YhAwAMiECeqNsHObHMAgBgQATyJG3n6M9149cwmkEGABgMgTxJ/yK9xAwyAMCACOSJuhfpJbZ6AwAYEIE8SVu0BnnnztmNBQCAqRLIEy3axcISCwCAwRDIkzRLLAAAhkogT7R4iYUZZACAoRDIkyze5s0SCwCAwRDIk3gnPQCAwRLIt8cbhQAADI5AnmTxNm+WWAAADIZAnsguFgAAQyWQJ7EGGQBgsATyRAuBbBcLAIChEciT7HqjkIUX6XmraQCAoRDIkyzeB9kaZACAwRDIE1liAQAwVAJ5Ei/SAwAYLIE80aJ9kC2xAAAYDIE8SVu8xMKL9AAAhkIgT7T4jUIssQAAGAqBPMmPrUG2xAIAYCgE8iS7tnkbr0G2iwUAwGAI5IkWrUG2xAIAYDAE8iTjPrYGGQBgeATyRN4oBABgqATyJIu3eTODDAAwGAJ5osXbvNnFAgBgKATyJAszyGUXCwCAoRHIk+za5s0SCwCAoRHIE7Uk1e2D7K2mAQCGQiBP0lpSldT422MNMgDAYAjkiRZmkC2xAAAYGoE8ycIMsreaBgAYHIE8URstr1jYxcISCwCAwRDIk7RFL9Lb6UV6AABDIZAnaTt3f5GeJRYAAIMhkCcazyBXjZZZWGIBADAYAnmShRfpJaNlFnaxAAAYDIG8pIVA3mCJBQDAgAjkSfoZ5DKDDAAwJAJ5onbbC/TWrRPIAAADIpAnWdjmLbHEAgBgYATyJG3nrj62xAIAYFgE8kT9DLJt3gAAhkQgT7LbNm8bRjPKAAAMgkCeqJtBLi/SAwAYEoE8yY+9UYglFgAAQyGQJ+q3ebOLBQDAkAjkSfpt3uxiAQAwKAJ5krZz0RILgQwAMBQCeaJFL9KzxAIAYDAE8iSLt3kzgwwAMBgbZj2A+TThjUKuuTy58BMzHRUAwJr0wGcmG/eb9Sh2EciTtNw2g1zrR0ssPv3GZMsZMx0WAMCadJ+TBPL862eQNyQ7dya33pgcdETy25+c6cgAANac/Q+Z9Qh2I5Anaf0+yOuSHbeOZpHXb0zufORsxwYAwF7lRXqTtJ27JpBH+yD/aPRCvXXrZzosAAD2PoE80eIlFjtGM8glkAEA1jqBPMlu27yNX6RnBhkAYBAE8kQT3mq67TSDDAAwAAJ5ksUzyDsXZpB9uwAA1jrFN9GiNwpp1iADAAyFQJ6k3+bNLhYAAIMikCdpO7slFnaxAAAYEm8UMsnhxyX7HDS6vm79KJjNIAMADIJAnuRRL7/teq0bLbGwiwUAwCBYYrGchSUWO3fctuwCAIA1SyAvp9/FwhILAIA1TyAvp9/FwhILAIA1TyAvZ936ZOdOM8gAAAMhkJezsMRipxfpAQAMgUBeTq2/bR9kM8gAAGveTAK5qu5VVTdV1XtncfwVWdevQfbvCQCAtW5Wxff2JF+e0bFXZt0Gu1gAAAzI1AO5qp6VZHuSf5j2sX8iC+uOd/7IGmQAgAGYaiBX1cFJ/ijJS5d53ClVtaWqtmzbtm06g1vKuvG3aMetZpABAAZg2jPIf5zkjNbapbf3oNba6a21za21zZs2bZrS0Jawbvxu3DtuMYMMADAAG6Z1oKo6Psljk5wwrWOuioUo/tEtt80mAwCwZk0tkJM8OskxSb5bVUlyYJL1VXW/1tqDpjiOlVlYVmEGGQBgEKYZyKcneX/38csyCuYXTHEMK9cvsbAGGQBgzZtaILfWbkhyw8LHVXVdkptaazN+Fd4yFvY+bjvMIAMADMA0Z5B301p7zayOvSL9rLEZZACANc+rzpazrvs3hHfSAwBY8xTfcvplFQIZAGDNU3zLscQCAGBQBPJydptBFsgAAGudQF6OGWQAgEERyMtZZwYZAGBIBPJyygwyAMCQCOTl2OYNAGBQFN9yrEEGABgUgbycftbYGmQAgDVPIC+nX2JhBhkAYM0TyMuxiwUAwKAI5OXYxQIAYFAE8nLsYgEAMCiKbznrum+RGWQAgDVPIC+nrEEGABgSgbwcu1gAAAyKQF7ObrtY+HYBAKx1im85llgAAAyKQF7Obm817dsFALDWKb7leKMQAIBBEcjL8UYhAACDIpCXYwYZAGBQBPJybPMGADAoAnk5drEAABgUgbyc3d5q2rcLAGCtU3zL6ZdYmEEGAFjzBPJy7GIBADAoAnk5drEAABgUgbwcu1gAAAyKQF5Odd8iM8gAAGueQF5O1W2RbBcLAIA1T/HtiYVlFmaQAQDWPIG8JxbCuHy7AADWOsW3JxZenOdFegAAa55A3hO7ZpAFMgDAWieQ94QZZACAwRDIe2KdNcgAAEOh+PZEmUEGABiKFQVyVW2qqk3dxw+oqtdV1W+s/tDmiG3eAAAGY6UzyGcleXKSVNVhST6d5GlJ3llVL13lsc2PhTcIMYMMALDmrTSQH5jkC+Prv5bkotba/ZP8xyTPW82BzRW7WAAADMZKA3m/JNeNrz82yd+Nr381yc+s1qDmzsISCzPIAABr3koD+cIkT6+qn0ny+CRnj28/PMn21RzYXLGLBQDAYKy0+F6b5M+SfCfJF1prXxzf/oQkX1vFcc0Xu1gAAAzGhpU8uLX2oao6OskRSb7e3fXJJB9czYHNlXXWIAMADMWKAjlJWmtXJrly4eOq+rkkX2+t3bSaA5sr3kkPAGAwVroP8p9W1W+Or1dVfSLJBUmuqKp/vzcGOBfsYgEAMBgrXYP87CT/Mr5+YpLjkzwsyV8lecMqjmu+2MUCAGAwVrrE4vAkW8fXT0pyVmvtS1X1gyRbVnVk82Td+iSVVM16JAAA7GUrnUG+Ksk9xtcfn+Qfx9c3JFm79VjrbPEGADAQK51B/mCSv66qC5IckuS/jW8/PslFqzmwubJug+UVAAADsdJA/r0klyQ5OskrWmvXj2+/e5J3rObA5sq69V6gBwAwECvdB/lHSf6vCbf/+aqNaB7VejPIAAADseJ9kKvq8CT/Kcn9krQk307y9tba91Z5bPPDDDIAwGCsdB/kR2S01vjkJDcmuSmjrd8uqqqHr/7w5sS69ck6L9IDABiClc4gvzHJ+5I8v7W2M0mqal2Sd2a09OIXV3d4c6LMIAMADMVKA/n4JM9diOMkaa3trKo3Jfnaqo5snqyzBhkAYChWum7g6iTHTrj92CTbf/rhzKl1G8wgAwAMxEpnkN+f5IyqekWSz2X0Ir1fyuhtpt+3ymObH/sdkux311mPAgCAKVhpIL8io3fMOzO3vXveLRntgXzq6g5tjjzmD5JH/O6sRwEAwBSsdB/kW5K8uKpemeSeGQXyRa21G/bG4ObGvgePLgAArHnLBnJV/d0ePCZJ0lp7yiqMCQAAZmZPZpCv2uujAACAObFsILfWfmsaAwEAgHng7eEAAKAjkAEAoCOQAQCgI5ABAKAjkAEAoCOQAQCgI5ABAKAjkAEAoCOQAQCgI5ABAKAjkAEAoCOQAQCgM9VArqr3VtUVVXVNVV1QVb89zeMDAMBypj2D/Pokx7TWDk7ylCSvq6oHT3kMAACwpKkGcmvtW621mxc+HF/uOc0xAADA7Zn6GuSqOq2qbkhyfpIrknx82mMAAIClTD2QW2svTHJQkkcm+VCSmxc/pqpOqaotVbVl27Zt0x4iAAADNpNdLFprO1prn01yVJIXTLj/9Nba5tba5k2bNk1/gAAADNast3nbEGuQAQCYI1ML5Kq6W1U9q6oOrKr1VfWEJL+R5B+nNQYAAFjOhikeq2W0nOKdGYX5JUle0lr7yBTHAAAAt2tqgdxa25bkUdM6HgAA/CRmvQYZAADmikAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAICOQAYAgI5ABgCAjkAGAIDO1AK5qvapqjOq6pKquraqvlZVJ07r+AAAsCemOYO8IcmlSR6V5M5JXp3krKo6ZopjAACA27VhWgdqrV2f5DXdTf+1qv41yYOTfGda4wAAgNszszXIVXV4knsn+dasxgAAAIvNJJCramOS/5zk3a218yfcf0pVbamqLdu2bZv+AAEAGKypB3JVrUvyniS3JHnRpMe01k5vrW1urW3etGnTVMcHAMCwTW0NcpJUVSU5I8nhSU5qrd06zeMDAMByphrISd6R5L5JHttau3HKxwYAgGVNcx/keyR5XpLjk/xbVV03vjx7WmMAAIDlTHObt0uS1LSOBwAAPwlvNQ0AAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgamlK6cAAA02SURBVAwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAACdqQZyVb2oqrZU1c1V9ZfTPDYAAOyJDVM+3uVJXpfkCUn2m/KxAQBgWVMN5Nbah5KkqjYnOWqaxwYAgD0xl2uQq+qU8VKMLdu2bZv1cAAAGJC5DOTW2umttc2ttc2bNm2a9XAAABiQuQxkAACYFYEMAACdqb5Ir6o2jI+5Psn6qto3yY9aaz+a5jgAAGAp055BflWSG5OcmuQ54+uvmvIYAABgSdPe5u01SV4zzWMCAMBKWIMMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAB2BDAAAHYEMAAAdgQwAAJ2pBnJVHVJVH66q66vqkqo6eZrH31MXb7su37r86ly+/cZc9L3r8tGvX56vXPKDWQ8LAIAp2DDl4709yS1JDk9yfJKPVdXXW2vfmvI4btebzr4gH/vGFbvdtq6S1z/9ATnm0ANmNCoAgLXphKPvmjttmJ+FDdVam86Bqg5I8sMkx7XWLhjf9p4kl7XWTl3qeZs3b25btmyZyhgXfPvya/LdH1yf7Tfcmg3r1+Vedzswf/Dhb+Rbl18z1XEAAAzBllc9NocduM/Uj1tVX2mtbV58+zRnkO+dZMdCHI99PcmjFj+wqk5JckqSHH300dMZXed+Rxyc+x1x8G63/c3zH55zL92eTOffEwAAg3HwvhtnPYTdTDOQD0xy9aLbrk5y0OIHttZOT3J6MppB3vtDW97+d9qQX7znYbMeBgAAe9k0F3tcl+TgRbcdnOTaKY4BAABu1zQD+YIkG6rqXt1tv5Bkrl6gBwDAsE0tkFtr1yf5UJI/qqoDquoRSX41yXumNQYAAFjOtPfTeGGS/ZJ8L8n7krxg3rZ4AwBg2Ka6D3Jr7QdJnjrNYwIAwErMz47MAAAwBwQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMgAAdKq1Nusx3K6q2pbkkhkc+rAk35/BcVmaczJ/nJP545zMH+dk/jgn82dW5+QerbVNi2+c+0Celara0lrbPOtxcBvnZP44J/PHOZk/zsn8cU7mz7ydE0ssAACgI5ABAKAjkJd2+qwHwI9xTuaPczJ/nJP545zMH+dk/szVObEGGQAAOmaQAQCgI5ABAKAjkAEAoCOQF6mqQ6rqw1V1fVVdUlUnz3pMa11VvaiqtlTVzVX1l4vu+5WqOr+qbqiqT1XVPbr79qmqM6vqmqr6t6r6vakPfo0af2/PGP8duLaqvlZVJ3b3Oy8zUFXvraorxt/bC6rqt7v7nJMZqqp7VdVNVfXe7raTx3+Hrq+qv62qQ7r7/K7ZS6rqnPG5uG58+ZfuPudkRqrqWVV13vj7e3FVPXJ8+1z+7BLIP+7tSW5JcniSZyd5R1Xdf7ZDWvMuT/K6JGf2N1bVYUk+lOTVSQ5JsiXJf+ke8pok90pyjySPSfKKqnriFMY7BBuSXJrkUUnunNE5OKuqjnFeZur1SY5prR2c5ClJXldVD3ZO5sLbk3x54YPx7413JfkPGf0+uSHJaYse73fN3vOi1tqB48t9EudklqrqcUn+LMlvJTkoyS8n+R/z/LPLLhadqjogyQ+THNdau2B823uSXNZaO3WmgxuAqnpdkqNaa88df3xKkue21n5x/PEBGb0N5QmttfOr6rIkv9VaO3t8/x8nuVdr7Vkz+QLWuKr65ySvTXJonJeZq6r7JDknyYuT3CXOycxU1bOSPD3Jt5P8XGvtOVX1pxn9Y+bk8WPumeS8jP7+7IzfNXtNVZ2T5L2ttf9n0e3OyYxU1eeSnNFaO2PR7XP7e94M8u7unWTHwl+Osa8n8S/I2bh/Rt//JElr7fokFye5f1XdNckR/f1xrvaaqjo8o78f34rzMlNVdVpV3ZDk/CRXJPl4nJOZqaqDk/xRkpcuumvxObk4o9nJe8fvmml4fVV9v6r+qaoePb7NOZmBqlqfZHOSTVV1UVVtraq3VdV+meOfXQJ5dwcmuXrRbVdn9N8BTN/tnY8Du48X38cqqqqNSf5zkne31s6P8zJTrbUXZvT9fGRG/zV5c5yTWfrjjGbGLl10+3LnxO+avef3k/xskiMzevOJj45ni52T2Tg8ycYkv5bRz63jk5yQ5FWZ459dAnl31yU5eNFtBye5dgZj4fbPx3Xdx4vvY5VU1bok78loluVF45udlxlrre1orX02yVFJXhDnZCaq6vgkj03y5xPuXu6c+F2zl7TWvthau7a1dnNr7d1J/inJSXFOZuXG8Z9vba1d0Vr7fpI3Zc/OSTKjn10CeXcXJNlQVffqbvuFjP5bmen7Vkbf/yS71ibdM8m3Wms/zOi/l3+he7xztYqqqpKckdG//p/RWrt1fJfzMj82ZPy9j3MyC49OckyS71bVvyV5WZJnVNVX8+Pn5GeT7JPR7xm/a6arJak4JzMx/hm0NaPzsNj8/uxqrbl0lyTvT/K+JAckeURG0/n3n/W41vIlo1/y+2b0Cv33jK9vSLJp/P1/xvi2P0vyhe55b0jy35PcNcnPZ/QX6Ymz/nrWyiXJO5N8IcmBi253XmZzPu6W5FkZ/bfj+iRPSHJ9kl91TmZ2TvZP8u+6yxuTfGB8Pu6f5JqM/kv5gCTvTfL+7rl+1+ydc3KX8d+Nhd8jzx7/PbmPczLT8/JHGe3ycrfxz6HPZLQ8aW5/ds38mzZvl4y2Gfnb8V+o7yY5edZjWuuXjLZxaYsurxnf99iMXox0Y0av2D+me94+GW0Nd02SK5P83qy/lrVyyWhLnZbkpoz+m2vh8mznZWbnZNP4F8X28ff2G0l+p7vfOZn9OXpNRrsnLHx88vj3yPVJPpLkkO4+v2v2zjnYNA6xa8d/V76Q5HHOyczPy8aMttTbnuTfkrwlyb7j++byZ5dt3gAAoGMNMgAAdAQyAAB0BDIAAHQEMgAAdAQyAAB0BDIAAHQEMsCAVdUxVdWqavOsxwIwLwQyAAB0BDIAAHQEMsAM1cgrquriqrqxqr5RVc8Z37ew/OHkqvpsVd1UVedX1eMXfY5frqovju+/sqr+vKrutOgYL62qC6vq5qraWlWvXzSUe1TVJ6rqhqr6dlU9bgpfPsBcEsgAs/W6JP9rkv+U5H5JXp/kXVX1pO4x/0eStyQ5Psknknykqo5MkvGff5/ka0lOGH+u3xh/ngV/muTV49vun+R/SXLponH8yfgYv5Dky0neX1UHrtpXCXAHUq21WY8BYJCq6oAk30/y+NbaZ7rb35zk3klemORfk7yqtfYn4/vWJTk/yVmttVdV1Z8k+fUk926t7Rw/5rlJ3pXkrhlNhHw/yUtaa++cMIZjxsd4fmvtXePbjkyyNckjW2ufXf2vHGC+bZj1AAAG7H5J9k3y36qqn63YmOQ73cefX7jSWttZVV8cPzdJ7pvk8wtxPPbZJHdK8nPjz79Pkn9YZiz/3F2/fPzn3fbsywBYWwQywOwsLHN7cpLvLrrv1iS1B5+jkiz1X4FtDz/HwvFGT2qtVVU/PoBB8cMPYHa+neTmJPdorV206HJJ97iHLVypUbk+NMl53ed4+HjpxYJfSnJLkou7Y/zKXvw6ANYUM8gAM9Jau7aq3pjkjePw/XSSAzMK4p1Jzh4/9AVVdUGSb2S0LvkeSd4xvu+0JC9JclpV/d9JfjbJG5K8rbV2Q5KMb399Vd08PsahSR7cWlv4HAB0BDLAbL06yZVJXpZR9F6T5NyMdq5YcGqS30vyoCSXJHlaa21rkrTWLquqE5P8n+PnbU/y10n+oHv+K5P8cHyso8bH+6u99yUB3LHZxQJgTnU7TDyktbZltqMBGA5rkAEAoCOQAQCgY4kFAAB0zCADAEBHIAMAQEcgAwBARyADAEBHIAMAQOf/BypTvmiZI2IFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Multiclass Classification\")\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, auc, roc_curve\n",
    "from scipy import interp\n",
    "\n",
    "logo = GroupKFold(n_splits=8)\n",
    "np.random.seed(SEED)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(SEED)\n",
    "\n",
    "\n",
    "cRF_tprs = []\n",
    "cRF_aucs = []\n",
    "cSVM_tprs = []\n",
    "cSVM_aucs = []\n",
    "cDT_tprs = []\n",
    "cDT_aucs = []\n",
    "cSNN_tprs = []\n",
    "cSNN_aucs = []\n",
    "\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "cRF_resultsA = []\n",
    "cSVM_resultsA = []\n",
    "cDT_resultsA = []\n",
    "cSNN_resultsA = []\n",
    "\n",
    "cRF_resultsB = []\n",
    "cSVM_resultsB = []\n",
    "cDT_resultsB = []\n",
    "cSNN_resultsB = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "# Define the neural network here\n",
    "snn1 = Sequential()\n",
    "snn1.add(Dense(350, input_shape=(53,), activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn1.add(Dense(350, activation='relu'))   \n",
    "#snn.add(Dropout(0.2)) \n",
    "snn1.add(Dense(350, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn1.add(Dense(350, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn1.add(Dense(350, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn1.add(Dense(350, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn1.add(Dense(350, activation='relu'))\n",
    "\n",
    "snn1.add(Dense(3, kernel_initializer='random_normal', activation='softmax'))\n",
    "snn1.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#logo.get_n_splits(groups=groups) # 'groups' is always required\n",
    "for train_index, test_index in logo.split(X, y, groups):\n",
    "    X_train_folds, X_test_folds = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_folds, y_test_folds = y[train_index], y[test_index]\n",
    "    scaler = preprocessing.StandardScaler().fit(X_train_folds)\n",
    "    X_train_transformed = scaler.transform(X_train_folds)\n",
    "    #\n",
    "    # Compile model\n",
    "    cRF = RandomForestClassifier(n_estimators=13, max_depth=None, min_samples_split=2, random_state=30).fit(X_train_transformed, y_train_folds)\n",
    "    cSVM = svm.LinearSVC(C=1).fit(X_train_transformed, y_train_folds)\n",
    "    cDT = DecisionTreeClassifier(random_state=8).fit(X_train_transformed, y_train_folds)\n",
    "    cSNN = snn1.fit(X_train_transformed, to_categorical(y_train_folds), epochs=600, validation_split=0.2, batch_size=32,verbose=True) \n",
    "    \n",
    "    # Transform X_Test  \n",
    "    X_test_transformed = scaler.transform(X_test_folds)\n",
    "    \n",
    "    \n",
    "    # Evaluate classifiers\n",
    "    cRF_y_pred = cRF.predict(X_test_transformed)\n",
    "    cSVM_y_pred = cSVM.predict(X_test_transformed)\n",
    "    cDT_y_pred = cDT.predict(X_test_transformed)\n",
    "    cSNN_y_pred = cSNN.model.predict_classes(X_test_transformed)\n",
    "\n",
    "    # Corrects\n",
    "    cRF_n_correct = sum(cRF_y_pred == y_test_folds)\n",
    "    cSVM_n_correct = sum(cSVM_y_pred == y_test_folds)\n",
    "    cDT_n_correct = sum(cDT_y_pred == y_test_folds)\n",
    "    cSNN_n_correct = sum(cSNN_y_pred == y_test_folds.values)\n",
    "    \n",
    "    #\n",
    "    cRF_accuracy1 = cRF_n_correct / len(cRF_y_pred)\n",
    "    cSVM_accuracy1 = cSVM_n_correct / len(cSVM_y_pred)\n",
    "    cDT_accuracy1 = cDT_n_correct / len(cDT_y_pred)\n",
    "    cSNN_accuracy1 = cSNN_n_correct / len(cSNN_y_pred)\n",
    "    \n",
    "    #\n",
    "    cRF_accuracy2 = cRF.score(X_test_transformed, y_test_folds)   \n",
    "    cSVM_accuracy2 = cSVM.score(X_test_transformed, y_test_folds)   \n",
    "    cDT_accuracy2 = cDT.score(X_test_transformed, y_test_folds)\n",
    "    cSNN_accuracy2 = snn1.evaluate(X_test_transformed, to_categorical(y_test_folds))    \n",
    "    \n",
    "    #\n",
    "    cRF_resultsA.append(cRF_accuracy1)\n",
    "    cSVM_resultsA.append(cSVM_accuracy1)\n",
    "    cDT_resultsA.append(cDT_accuracy1)\n",
    "    cSNN_resultsA.append(cSNN_accuracy1)\n",
    "    \n",
    "    #\n",
    "    cRF_resultsB.append(cRF_accuracy2)\n",
    "    cSVM_resultsB.append(cSVM_accuracy2)\n",
    "    cDT_resultsB.append(cDT_accuracy2)\n",
    "    cSNN_resultsB.append(cSNN_accuracy2)\n",
    "\n",
    "    \n",
    "    \n",
    "    print(\"==========================================================================\")\n",
    "    print(\"FOLD {}\".format(i))\n",
    "    print(\"==========================================================================\")\n",
    "#     # Compute ROC curve and area the curve RF\n",
    "#     cRF_fpr, cRF_tpr, cRF_thresholds = roc_curve(y_test_folds, cRF_y_pred)\n",
    "#     cRF_tprs.append(interp(mean_fpr, cRF_fpr, cRF_tpr))\n",
    "#     cRF_tprs[-1][0] = 0.0\n",
    "#     cRF_roc_auc = auc(cRF_fpr, cRF_tpr)\n",
    "#     cRF_aucs.append(cRF_roc_auc)\n",
    "    \n",
    "#     # Compute ROC curve and area the curve SVM\n",
    "#     cSVM_fpr, cSVM_tpr, cSVM_thresholds = roc_curve(y_test_folds, cSVM_y_pred)\n",
    "#     cSVM_tprs.append(interp(mean_fpr, cSVM_fpr, cSVM_tpr))\n",
    "#     cSVM_tprs[-1][0] = 0.0\n",
    "#     cSVM_roc_auc = auc(cSVM_fpr, cSVM_tpr)\n",
    "#     cSVM_aucs.append(cSVM_roc_auc)\n",
    "    \n",
    "#     # Compute ROC curve and area the curve\n",
    "#     cDT_fpr, cDT_tpr, cDT_thresholds = roc_curve(y_test_folds, cDT_y_pred)\n",
    "#     cDT_tprs.append(interp(mean_fpr, cDT_fpr, cDT_tpr))\n",
    "#     cDT_tprs[-1][0] = 0.0\n",
    "#     cDT_roc_auc = auc(cDT_fpr, cDT_tpr)\n",
    "#     cDT_aucs.append(cDT_roc_auc)\n",
    "    \n",
    "#         # Compute ROC curve and area the curve RF\n",
    "#     cSNN_fpr, cSNN_tpr, cSNN_thresholds = roc_curve(y_test_folds, cSNN_y_pred)\n",
    "#     cSNN_tprs.append(interp(mean_fpr, cSNN_fpr, cSNN_tpr))\n",
    "#     cSNN_tprs[-1][0] = 0.0\n",
    "#     cSNN_roc_auc = auc(cSNN_fpr, cSNN_tpr)\n",
    "#     cSNN_aucs.append(cSNN_roc_auc)\n",
    "    \n",
    "    \n",
    "#     plt.plot(cSNN_fpr, cSNN_tpr, lw=3, alpha=0.3, label='cRF_ROC fold %d (AUC = %0.2f)' % (i, cSNN_roc_auc))  \n",
    "#     plt.plot(cRF_fpr, cRF_tpr, lw=3, alpha=0.3, label='cRF_ROC fold %d (AUC = %0.2f)' % (i, cRF_roc_auc))\n",
    "#     plt.plot(cSVM_fpr, cSVM_tpr, lw=3, alpha=0.3, label='cSVM_ROC fold %d (AUC = %0.2f)' % (i, cSVM_roc_auc))\n",
    "#     plt.plot(cDT_fpr, cDT_tpr, lw=3, alpha=0.3, label='cDT_ROC fold %d (AUC = %0.2f)' % (i, cDT_roc_auc))\n",
    "\n",
    "    i += 1\n",
    "    print(\"RF Accuracy A: {}\".format(cRF_resultsA))\n",
    "    print(\"RF Accuracy B: {}\".format(cRF_resultsB))\n",
    "    print(\"RF Confusion Matrix: \\n{}\".format(confusion_matrix(y_test_folds, cRF_y_pred)))\n",
    "    \n",
    "    \n",
    "    print(\"SVM Accuracy A: {}\".format(cSVM_resultsA))\n",
    "    print(\"SVM Accuracy B: {}\".format(cSVM_resultsB))\n",
    "    print(\"SVM Confusion Matrix: \\n{}\".format(confusion_matrix(y_test_folds, cSVM_y_pred)))\n",
    "        \n",
    "    print(\"DT Accuracy A: {}\".format(cDT_resultsA))\n",
    "    print(\"DT Accuracy B: {}\".format(cDT_resultsB))\n",
    "    print(\"DT Confusion Matrix: \\n{}\".format(confusion_matrix(y_test_folds, cDT_y_pred)))\n",
    "\n",
    "    \n",
    "    print(\"sNN Accuracy A: {}\".format(cSNN_resultsA))\n",
    "    print(\"sNN Accuracy B: {}\".format(cSNN_resultsB))\n",
    "    print(\"sNN Confusion Matrix: \\n{}\".format(confusion_matrix(y_test_folds, cSNN_y_pred)))\n",
    "\n",
    "\n",
    "    \n",
    "    print(cSNN.history.keys())\n",
    "    import matplotlib.pyplot as plt\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(cSNN.history['acc'])\n",
    "    plt.plot(cSNN.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(cSNN.history['loss'])\n",
    "    plt.plot(cSNN.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    save_fig(i)\n",
    "    \n",
    "print(\"==========================================================================\")    \n",
    "print(\"Accuracy summaries\")\n",
    "print(\"RF Accuracy A: {}\".format(cRF_resultsA))\n",
    "print(\"RF Accuracy B: {}\".format(cRF_resultsB))\n",
    "print(\"SVM Accuracy A: {}\".format(cSVM_resultsA))\n",
    "print(\"SVM Accuracy B: {}\".format(cSVM_resultsB))\n",
    "print(\"DT Accuracy A: {}\".format(cDT_resultsA))\n",
    "print(\"DT Accuracy B: {}\".format(cDT_resultsB))\n",
    "print(\"SNN Accuracy A: {}\".format(cSNN_resultsA))\n",
    "print(\"SNN Accuracy B: {}\".format(cSNN_resultsB))\n",
    "print(\"Acurracy A is manually calculated\")\n",
    "print(\"==========================================================================\")      \n",
    "print(\"==========================================================================\")\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"==========================================================================\")    \n",
    "print(\"RF Mean {}, SD {}\".format(np.mean(cRF_resultsA), np.std(cRF_resultsA)))\n",
    "print(\"SVM Mean {}, SD {}\".format(np.mean(cSVM_resultsA), np.std(cSVM_resultsA)))\n",
    "print(\"DT Mean {}, SD {}\".format(np.mean(cDT_resultsA), np.std(cDT_resultsA)))\n",
    "print(\"SNN Mean {}, SD {}\".format(np.mean(cSNN_resultsA), np.std(cSNN_resultsA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cSNN_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Mean 0.49710326550029943, SD 0.24584129889122658\n",
      "SVM Mean 0.47711225499186327, SD 0.19985406333651878\n",
      "DT Mean 0.5453519573440828, SD 0.2032612908389891\n",
      "SNN Mean 0.8333089483937666, SD 0.19162210542592117\n"
     ]
    }
   ],
   "source": [
    "print(\"RF Mean {}, SD {}\".format(np.mean(cRF_resultsA), np.std(cRF_resultsA)))\n",
    "print(\"SVM Mean {}, SD {}\".format(np.mean(cSVM_resultsA), np.std(cSVM_resultsA)))\n",
    "print(\"DT Mean {}, SD {}\".format(np.mean(cDT_resultsA), np.std(cDT_resultsA)))\n",
    "print(\"SNN Mean {}, SD {}\".format(np.mean(cSNN_resultsA), np.std(cSNN_resultsA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (snn1.metrics_names[1], cSNN_accuracy2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20% - 80% Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "cRF_final = []\n",
    "cSVM_final = []\n",
    "cDT_final = []\n",
    "\n",
    "for i in range(1,300):\n",
    "    scaler = preprocessing.StandardScaler().fit(X_tr)\n",
    "    X_tr_transformed = scaler.transform(X_tr)\n",
    "    cRF = RandomForestClassifier(n_estimators=i, max_depth=None, min_samples_split=2, random_state=30).fit(X_tr_transformed, y_tr)\n",
    "    cSVM = svm.LinearSVC(C=i).fit(X_tr_transformed, y_tr)\n",
    "    cDT = DecisionTreeClassifier(random_state=i).fit(X_tr_transformed, y_tr)\n",
    "    X_tst_transformed = scaler.transform(X_tst)\n",
    "\n",
    "    # Automatic \n",
    "    cRF_accuracy = cRF.score(X_tst_transformed, y_tst)\n",
    "    cSVM_accuracy = cSVM.score(X_tst_transformed, y_tst)\n",
    "    cDT_accuracy = cRF.score(X_tst_transformed, y_tst)\n",
    "\n",
    "    # Evaluate classifiers\n",
    "    cRF_y_pred = cRF.predict(X_tst_transformed)\n",
    "    cSVM_y_pred = cSVM.predict(X_tst_transformed)\n",
    "    cDT_y_pred = cDT.predict(X_tst_transformed)\n",
    "\n",
    "    # Corrects\n",
    "    cRF_n_correct = sum(cRF_y_pred == y_tst)\n",
    "    cSVM_n_correct = sum(cSVM_y_pred == y_tst)\n",
    "    cDT_n_correct = sum(cDT_y_pred == y_tst)\n",
    "\n",
    "    cRF_accuracy = cRF.score(X_tst_transformed, y_tst)\n",
    "    cSVM_accuracy = cSVM.score(X_tst_transformed, y_tst)\n",
    "    cDT_accuracy = cDT.score(X_tst_transformed, y_tst)\n",
    "    \n",
    "\n",
    "    cRF_final.append(cRF_accuracy)\n",
    "    cSVM_final.append(cSVM_accuracy)\n",
    "    cDT_final.append(cDT_accuracy)\n",
    "    print(i)\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.584924965893588\n",
      "1 0.5474079126875853\n",
      "2 0.6490450204638472\n",
      "3 0.43963165075034105\n",
      "4 0.43963165075034105\n",
      "5 0.4021145975443383\n",
      "6 0.5037517053206003\n",
      "7 0.5037517053206003\n",
      "8 0.5879945429740792\n",
      "9 0.4427012278308322\n",
      "10 0.4427012278308322\n",
      "11 0.4427012278308322\n",
      "12 0.5879945429740792\n",
      "13 0.6681446111869032\n",
      "14 0.5879945429740792\n",
      "15 0.6681446111869032\n",
      "16 0.6681446111869032\n",
      "17 0.6681446111869032\n",
      "18 0.6681446111869032\n",
      "19 0.6681446111869032\n",
      "20 0.6681446111869032\n",
      "21 0.6681446111869032\n",
      "22 0.6681446111869032\n",
      "23 0.6681446111869032\n",
      "24 0.5879945429740792\n",
      "25 0.5879945429740792\n",
      "26 0.5879945429740792\n",
      "27 0.5879945429740792\n",
      "28 0.5879945429740792\n",
      "29 0.5879945429740792\n",
      "30 0.5879945429740792\n",
      "31 0.5879945429740792\n",
      "32 0.5879945429740792\n",
      "33 0.5879945429740792\n",
      "34 0.5879945429740792\n",
      "35 0.5879945429740792\n",
      "36 0.5879945429740792\n",
      "37 0.5879945429740792\n",
      "38 0.5879945429740792\n",
      "39 0.5879945429740792\n",
      "40 0.5879945429740792\n",
      "41 0.5879945429740792\n",
      "42 0.5879945429740792\n",
      "43 0.5879945429740792\n",
      "44 0.5879945429740792\n",
      "45 0.5879945429740792\n",
      "46 0.5879945429740792\n",
      "47 0.5879945429740792\n",
      "48 0.5879945429740792\n",
      "49 0.5879945429740792\n",
      "50 0.5879945429740792\n",
      "51 0.5879945429740792\n",
      "52 0.5879945429740792\n",
      "53 0.5879945429740792\n",
      "54 0.5879945429740792\n",
      "55 0.5879945429740792\n",
      "56 0.5879945429740792\n",
      "57 0.5879945429740792\n",
      "58 0.5879945429740792\n",
      "59 0.5879945429740792\n",
      "60 0.5879945429740792\n",
      "61 0.5879945429740792\n",
      "62 0.5879945429740792\n",
      "63 0.5879945429740792\n",
      "64 0.5879945429740792\n",
      "65 0.5879945429740792\n",
      "66 0.5879945429740792\n",
      "67 0.5879945429740792\n",
      "68 0.5879945429740792\n",
      "69 0.5879945429740792\n",
      "70 0.5879945429740792\n",
      "71 0.5879945429740792\n",
      "72 0.5879945429740792\n",
      "73 0.5879945429740792\n",
      "74 0.5879945429740792\n",
      "75 0.5879945429740792\n",
      "76 0.5879945429740792\n",
      "77 0.5879945429740792\n",
      "78 0.5879945429740792\n",
      "79 0.5879945429740792\n",
      "80 0.5879945429740792\n",
      "81 0.5879945429740792\n",
      "82 0.5879945429740792\n",
      "83 0.5879945429740792\n",
      "84 0.5879945429740792\n",
      "85 0.5879945429740792\n",
      "86 0.5879945429740792\n",
      "87 0.5879945429740792\n",
      "88 0.5879945429740792\n",
      "89 0.5879945429740792\n",
      "90 0.5879945429740792\n",
      "91 0.5879945429740792\n",
      "92 0.5879945429740792\n",
      "93 0.5879945429740792\n",
      "94 0.5879945429740792\n",
      "95 0.5879945429740792\n",
      "96 0.5879945429740792\n",
      "97 0.5879945429740792\n",
      "98 0.5879945429740792\n",
      "99 0.5879945429740792\n",
      "100 0.5879945429740792\n",
      "101 0.5879945429740792\n",
      "102 0.5879945429740792\n",
      "103 0.5879945429740792\n",
      "104 0.5879945429740792\n",
      "105 0.5879945429740792\n",
      "106 0.5879945429740792\n",
      "107 0.5879945429740792\n",
      "108 0.5879945429740792\n",
      "109 0.5879945429740792\n",
      "110 0.5879945429740792\n",
      "111 0.5879945429740792\n",
      "112 0.5879945429740792\n",
      "113 0.5879945429740792\n",
      "114 0.5879945429740792\n",
      "115 0.5879945429740792\n",
      "116 0.5879945429740792\n",
      "117 0.5879945429740792\n",
      "118 0.5879945429740792\n",
      "119 0.5879945429740792\n",
      "120 0.5879945429740792\n",
      "121 0.5879945429740792\n",
      "122 0.5879945429740792\n",
      "123 0.5879945429740792\n",
      "124 0.5879945429740792\n",
      "125 0.5879945429740792\n",
      "126 0.5879945429740792\n",
      "127 0.5879945429740792\n",
      "128 0.5879945429740792\n",
      "129 0.5879945429740792\n",
      "130 0.5879945429740792\n",
      "131 0.5879945429740792\n",
      "132 0.5879945429740792\n",
      "133 0.5879945429740792\n",
      "134 0.5879945429740792\n",
      "135 0.5879945429740792\n",
      "136 0.5879945429740792\n",
      "137 0.5879945429740792\n",
      "138 0.5879945429740792\n",
      "139 0.5879945429740792\n",
      "140 0.5879945429740792\n",
      "141 0.5879945429740792\n",
      "142 0.5879945429740792\n",
      "143 0.5879945429740792\n",
      "144 0.5879945429740792\n",
      "145 0.5879945429740792\n",
      "146 0.5879945429740792\n",
      "147 0.5879945429740792\n",
      "148 0.5879945429740792\n",
      "149 0.5879945429740792\n",
      "150 0.5879945429740792\n",
      "151 0.5879945429740792\n",
      "152 0.5879945429740792\n",
      "153 0.5879945429740792\n",
      "154 0.5879945429740792\n",
      "155 0.5879945429740792\n",
      "156 0.5879945429740792\n",
      "157 0.5879945429740792\n",
      "158 0.5879945429740792\n",
      "159 0.5879945429740792\n",
      "160 0.5879945429740792\n",
      "161 0.5879945429740792\n",
      "162 0.5879945429740792\n",
      "163 0.5879945429740792\n",
      "164 0.5879945429740792\n",
      "165 0.5879945429740792\n",
      "166 0.5879945429740792\n",
      "167 0.5879945429740792\n",
      "168 0.5879945429740792\n",
      "169 0.5879945429740792\n",
      "170 0.5879945429740792\n",
      "171 0.5879945429740792\n",
      "172 0.5879945429740792\n",
      "173 0.5879945429740792\n",
      "174 0.5879945429740792\n",
      "175 0.5879945429740792\n",
      "176 0.5879945429740792\n",
      "177 0.5879945429740792\n",
      "178 0.5879945429740792\n",
      "179 0.5879945429740792\n",
      "180 0.5879945429740792\n",
      "181 0.5879945429740792\n",
      "182 0.5879945429740792\n",
      "183 0.5879945429740792\n",
      "184 0.5879945429740792\n",
      "185 0.5879945429740792\n",
      "186 0.5879945429740792\n",
      "187 0.5879945429740792\n",
      "188 0.5879945429740792\n",
      "189 0.5879945429740792\n",
      "190 0.5879945429740792\n",
      "191 0.5879945429740792\n",
      "192 0.5879945429740792\n",
      "193 0.5879945429740792\n",
      "194 0.5879945429740792\n",
      "195 0.5879945429740792\n",
      "196 0.5879945429740792\n",
      "197 0.5879945429740792\n",
      "198 0.5879945429740792\n",
      "199 0.5879945429740792\n",
      "200 0.5879945429740792\n",
      "201 0.5879945429740792\n",
      "202 0.5879945429740792\n",
      "203 0.5879945429740792\n",
      "204 0.5879945429740792\n",
      "205 0.5879945429740792\n",
      "206 0.5879945429740792\n",
      "207 0.5879945429740792\n",
      "208 0.5879945429740792\n",
      "209 0.5879945429740792\n",
      "210 0.5879945429740792\n",
      "211 0.5879945429740792\n",
      "212 0.5879945429740792\n",
      "213 0.5879945429740792\n",
      "214 0.5879945429740792\n",
      "215 0.5879945429740792\n",
      "216 0.5879945429740792\n",
      "217 0.5879945429740792\n",
      "218 0.5879945429740792\n",
      "219 0.5879945429740792\n",
      "220 0.5879945429740792\n",
      "221 0.5879945429740792\n",
      "222 0.5879945429740792\n",
      "223 0.5879945429740792\n",
      "224 0.5879945429740792\n",
      "225 0.5879945429740792\n",
      "226 0.5879945429740792\n",
      "227 0.5879945429740792\n",
      "228 0.5879945429740792\n",
      "229 0.5879945429740792\n",
      "230 0.5879945429740792\n",
      "231 0.5879945429740792\n",
      "232 0.5879945429740792\n",
      "233 0.5879945429740792\n",
      "234 0.5879945429740792\n",
      "235 0.5879945429740792\n",
      "236 0.5879945429740792\n",
      "237 0.5879945429740792\n",
      "238 0.5879945429740792\n",
      "239 0.5879945429740792\n",
      "240 0.5879945429740792\n",
      "241 0.5879945429740792\n",
      "242 0.5879945429740792\n",
      "243 0.5879945429740792\n",
      "244 0.5879945429740792\n",
      "245 0.5879945429740792\n",
      "246 0.5879945429740792\n",
      "247 0.5879945429740792\n",
      "248 0.5879945429740792\n",
      "249 0.5879945429740792\n",
      "250 0.5879945429740792\n",
      "251 0.5879945429740792\n",
      "252 0.5879945429740792\n",
      "253 0.5879945429740792\n",
      "254 0.5879945429740792\n",
      "255 0.5879945429740792\n",
      "256 0.5879945429740792\n",
      "257 0.5879945429740792\n",
      "258 0.5879945429740792\n",
      "259 0.5879945429740792\n",
      "260 0.5879945429740792\n",
      "261 0.5879945429740792\n",
      "262 0.5879945429740792\n",
      "263 0.5879945429740792\n",
      "264 0.5879945429740792\n",
      "265 0.5879945429740792\n",
      "266 0.5879945429740792\n",
      "267 0.5879945429740792\n",
      "268 0.5879945429740792\n",
      "269 0.5879945429740792\n",
      "270 0.5879945429740792\n",
      "271 0.5879945429740792\n",
      "272 0.5879945429740792\n",
      "273 0.5879945429740792\n",
      "274 0.5879945429740792\n",
      "275 0.5879945429740792\n",
      "276 0.5879945429740792\n",
      "277 0.5879945429740792\n",
      "278 0.5879945429740792\n",
      "279 0.5879945429740792\n",
      "280 0.5879945429740792\n",
      "281 0.5879945429740792\n",
      "282 0.5879945429740792\n",
      "283 0.5879945429740792\n",
      "284 0.5879945429740792\n",
      "285 0.5879945429740792\n",
      "286 0.5879945429740792\n",
      "287 0.5879945429740792\n",
      "288 0.5879945429740792\n",
      "289 0.5879945429740792\n",
      "290 0.5879945429740792\n",
      "291 0.5879945429740792\n",
      "292 0.5879945429740792\n",
      "293 0.5879945429740792\n",
      "294 0.5879945429740792\n",
      "295 0.5879945429740792\n",
      "296 0.5879945429740792\n",
      "297 0.5879945429740792\n",
      "298 0.5879945429740792\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-aa18c6f3b642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcRF_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(0,300):\n",
    "    print (i,cRF_final[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.919849931787176\n",
      "2 0.919849931787176\n",
      "3 0.919849931787176\n",
      "4 0.919849931787176\n",
      "5 0.919849931787176\n",
      "6 0.919849931787176\n",
      "7 0.919849931787176\n",
      "8 0.919849931787176\n",
      "9 0.919849931787176\n",
      "10 0.919849931787176\n",
      "11 0.919849931787176\n",
      "12 0.919849931787176\n",
      "13 0.919849931787176\n",
      "14 0.919849931787176\n",
      "15 0.919849931787176\n",
      "16 0.919849931787176\n",
      "17 0.919849931787176\n",
      "18 0.919849931787176\n",
      "19 0.919849931787176\n",
      "20 0.919849931787176\n",
      "21 0.919849931787176\n",
      "22 0.919849931787176\n",
      "23 0.919849931787176\n",
      "24 0.919849931787176\n",
      "25 0.919849931787176\n",
      "26 0.919849931787176\n",
      "27 0.919849931787176\n",
      "28 0.919849931787176\n",
      "29 0.919849931787176\n",
      "30 0.919849931787176\n",
      "31 0.919849931787176\n",
      "32 0.919849931787176\n",
      "33 0.919849931787176\n",
      "34 0.919849931787176\n",
      "35 0.919849931787176\n",
      "36 0.919849931787176\n",
      "37 0.919849931787176\n",
      "38 0.919849931787176\n",
      "39 0.919849931787176\n",
      "40 0.919849931787176\n",
      "41 0.919849931787176\n",
      "42 0.919849931787176\n",
      "43 0.919849931787176\n",
      "44 0.919849931787176\n",
      "45 0.919849931787176\n",
      "46 0.919849931787176\n",
      "47 0.919849931787176\n",
      "48 0.919849931787176\n",
      "49 0.919849931787176\n",
      "50 0.919849931787176\n",
      "51 0.919849931787176\n",
      "52 0.919849931787176\n",
      "53 0.919849931787176\n",
      "54 0.919849931787176\n",
      "55 0.919849931787176\n",
      "56 0.919849931787176\n",
      "57 0.919849931787176\n",
      "58 0.919849931787176\n",
      "59 0.919849931787176\n",
      "60 0.919849931787176\n",
      "61 0.919849931787176\n",
      "62 0.919849931787176\n",
      "63 0.919849931787176\n",
      "64 0.919849931787176\n",
      "65 0.919849931787176\n",
      "66 0.919849931787176\n",
      "67 0.919849931787176\n",
      "68 0.919849931787176\n",
      "69 0.919849931787176\n",
      "70 0.919849931787176\n",
      "71 0.919849931787176\n",
      "72 0.919849931787176\n",
      "73 0.919849931787176\n",
      "74 0.919849931787176\n",
      "75 0.919849931787176\n",
      "76 0.919849931787176\n",
      "77 0.919849931787176\n",
      "78 0.919849931787176\n",
      "79 0.919849931787176\n",
      "80 0.919849931787176\n",
      "81 0.919849931787176\n",
      "82 0.919849931787176\n",
      "83 0.919849931787176\n",
      "84 0.919849931787176\n",
      "85 0.919849931787176\n",
      "86 0.919849931787176\n",
      "87 0.919849931787176\n",
      "88 0.919849931787176\n",
      "89 0.919849931787176\n",
      "90 0.919849931787176\n",
      "91 0.919849931787176\n",
      "92 0.919849931787176\n",
      "93 0.919849931787176\n",
      "94 0.919849931787176\n",
      "95 0.919849931787176\n",
      "96 0.919849931787176\n",
      "97 0.919849931787176\n",
      "98 0.919849931787176\n",
      "99 0.919849931787176\n",
      "100 0.919849931787176\n",
      "101 0.919849931787176\n",
      "102 0.919849931787176\n",
      "103 0.919849931787176\n",
      "104 0.919849931787176\n",
      "105 0.919849931787176\n",
      "106 0.919849931787176\n",
      "107 0.919849931787176\n",
      "108 0.919849931787176\n",
      "109 0.919849931787176\n",
      "110 0.919849931787176\n",
      "111 0.919849931787176\n",
      "112 0.919849931787176\n",
      "113 0.919849931787176\n",
      "114 0.919849931787176\n",
      "115 0.919849931787176\n",
      "116 0.919849931787176\n",
      "117 0.919849931787176\n",
      "118 0.919849931787176\n",
      "119 0.919849931787176\n",
      "120 0.919849931787176\n",
      "121 0.919849931787176\n",
      "122 0.919849931787176\n",
      "123 0.919849931787176\n",
      "124 0.919849931787176\n",
      "125 0.919849931787176\n",
      "126 0.919849931787176\n",
      "127 0.919849931787176\n",
      "128 0.919849931787176\n",
      "129 0.919849931787176\n",
      "130 0.919849931787176\n",
      "131 0.919849931787176\n",
      "132 0.919849931787176\n",
      "133 0.919849931787176\n",
      "134 0.919849931787176\n",
      "135 0.919849931787176\n",
      "136 0.919849931787176\n",
      "137 0.919849931787176\n",
      "138 0.919849931787176\n",
      "139 0.919849931787176\n",
      "140 0.919849931787176\n",
      "141 0.919849931787176\n",
      "142 0.919849931787176\n",
      "143 0.919849931787176\n",
      "144 0.919849931787176\n",
      "145 0.919849931787176\n",
      "146 0.919849931787176\n",
      "147 0.919849931787176\n",
      "148 0.919849931787176\n",
      "149 0.919849931787176\n",
      "150 0.919849931787176\n",
      "151 0.919849931787176\n",
      "152 0.919849931787176\n",
      "153 0.919849931787176\n",
      "154 0.919849931787176\n",
      "155 0.919849931787176\n",
      "156 0.919849931787176\n",
      "157 0.919849931787176\n",
      "158 0.919849931787176\n",
      "159 0.919849931787176\n",
      "160 0.919849931787176\n",
      "161 0.919849931787176\n",
      "162 0.919849931787176\n",
      "163 0.919849931787176\n",
      "164 0.919849931787176\n",
      "165 0.919849931787176\n",
      "166 0.919849931787176\n",
      "167 0.919849931787176\n",
      "168 0.919849931787176\n",
      "169 0.919849931787176\n",
      "170 0.919849931787176\n",
      "171 0.919849931787176\n",
      "172 0.919849931787176\n",
      "173 0.919849931787176\n",
      "174 0.919849931787176\n",
      "175 0.919849931787176\n",
      "176 0.919849931787176\n",
      "177 0.919849931787176\n",
      "178 0.919849931787176\n",
      "179 0.919849931787176\n",
      "180 0.919849931787176\n",
      "181 0.919849931787176\n",
      "182 0.919849931787176\n",
      "183 0.919849931787176\n",
      "184 0.919849931787176\n",
      "185 0.919849931787176\n",
      "186 0.919849931787176\n",
      "187 0.919849931787176\n",
      "188 0.919849931787176\n",
      "189 0.919849931787176\n",
      "190 0.919849931787176\n",
      "191 0.919849931787176\n",
      "192 0.919849931787176\n",
      "193 0.919849931787176\n",
      "194 0.919849931787176\n",
      "195 0.919849931787176\n",
      "196 0.919849931787176\n",
      "197 0.919849931787176\n",
      "198 0.919849931787176\n",
      "199 0.919849931787176\n",
      "200 0.919849931787176\n",
      "201 0.919849931787176\n",
      "202 0.919849931787176\n",
      "203 0.919849931787176\n",
      "204 0.919849931787176\n",
      "205 0.919849931787176\n",
      "206 0.919849931787176\n",
      "207 0.919849931787176\n",
      "208 0.919849931787176\n",
      "209 0.919849931787176\n",
      "210 0.919849931787176\n",
      "211 0.919849931787176\n",
      "212 0.919849931787176\n",
      "213 0.919849931787176\n",
      "214 0.919849931787176\n",
      "215 0.919849931787176\n",
      "216 0.919849931787176\n",
      "217 0.919849931787176\n",
      "218 0.919849931787176\n",
      "219 0.919849931787176\n",
      "220 0.919849931787176\n",
      "221 0.919849931787176\n",
      "222 0.919849931787176\n",
      "223 0.919849931787176\n",
      "224 0.919849931787176\n",
      "225 0.919849931787176\n",
      "226 0.919849931787176\n",
      "227 0.919849931787176\n",
      "228 0.919849931787176\n",
      "229 0.919849931787176\n",
      "230 0.919849931787176\n",
      "231 0.919849931787176\n",
      "232 0.919849931787176\n",
      "233 0.919849931787176\n",
      "234 0.919849931787176\n",
      "235 0.919849931787176\n",
      "236 0.919849931787176\n",
      "237 0.919849931787176\n",
      "238 0.919849931787176\n",
      "239 0.919849931787176\n",
      "240 0.919849931787176\n",
      "241 0.919849931787176\n",
      "242 0.919849931787176\n",
      "243 0.919849931787176\n",
      "244 0.919849931787176\n",
      "245 0.919849931787176\n",
      "246 0.919849931787176\n",
      "247 0.919849931787176\n",
      "248 0.919849931787176\n",
      "249 0.919849931787176\n",
      "250 0.919849931787176\n",
      "251 0.919849931787176\n",
      "252 0.919849931787176\n",
      "253 0.919849931787176\n",
      "254 0.919849931787176\n",
      "255 0.919849931787176\n",
      "256 0.919849931787176\n",
      "257 0.919849931787176\n",
      "258 0.919849931787176\n",
      "259 0.919849931787176\n",
      "260 0.919849931787176\n",
      "261 0.919849931787176\n",
      "262 0.919849931787176\n",
      "263 0.919849931787176\n",
      "264 0.919849931787176\n",
      "265 0.919849931787176\n",
      "266 0.919849931787176\n",
      "267 0.919849931787176\n",
      "268 0.919849931787176\n",
      "269 0.919849931787176\n",
      "270 0.919849931787176\n",
      "271 0.919849931787176\n",
      "272 0.919849931787176\n",
      "273 0.919849931787176\n",
      "274 0.919849931787176\n",
      "275 0.919849931787176\n",
      "276 0.919849931787176\n",
      "277 0.919849931787176\n",
      "278 0.919849931787176\n",
      "279 0.919849931787176\n",
      "280 0.919849931787176\n",
      "281 0.919849931787176\n",
      "282 0.919849931787176\n",
      "283 0.919849931787176\n",
      "284 0.919849931787176\n",
      "285 0.919849931787176\n",
      "286 0.919849931787176\n",
      "287 0.919849931787176\n",
      "288 0.919849931787176\n",
      "289 0.919849931787176\n",
      "290 0.919849931787176\n",
      "291 0.919849931787176\n",
      "292 0.919849931787176\n",
      "293 0.919849931787176\n",
      "294 0.919849931787176\n",
      "295 0.919849931787176\n",
      "296 0.919849931787176\n",
      "297 0.919849931787176\n",
      "298 0.919849931787176\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-1fcf6353a6de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcSVM_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(1,300):\n",
    "    print (i,cSVM_final[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.6354024556616644\n",
      "2 0.497612551159618\n",
      "3 0.8823328785811733\n",
      "4 0.8823328785811733\n",
      "5 0.7820600272851296\n",
      "6 0.8823328785811733\n",
      "7 0.919849931787176\n",
      "8 0.497612551159618\n",
      "9 0.919849931787176\n",
      "10 0.6354024556616644\n",
      "11 0.6354024556616644\n",
      "12 0.580150068212824\n",
      "13 0.8823328785811733\n",
      "14 0.6354024556616644\n",
      "15 0.8823328785811733\n",
      "16 0.8823328785811733\n",
      "17 0.919849931787176\n",
      "18 0.8823328785811733\n",
      "19 0.7445429740791268\n",
      "20 0.497612551159618\n",
      "21 0.8823328785811733\n",
      "22 0.7445429740791268\n",
      "23 0.919849931787176\n",
      "24 0.8823328785811733\n",
      "25 0.7179399727148704\n",
      "26 0.7445429740791268\n",
      "27 0.8823328785811733\n",
      "28 0.8823328785811733\n",
      "29 0.6354024556616644\n",
      "30 0.7820600272851296\n",
      "31 0.8823328785811733\n",
      "32 0.6354024556616644\n",
      "33 0.7179399727148704\n",
      "34 0.6354024556616644\n",
      "35 0.7820600272851296\n",
      "36 0.8823328785811733\n",
      "37 0.919849931787176\n",
      "38 0.580150068212824\n",
      "39 0.6354024556616644\n",
      "40 0.919849931787176\n",
      "41 0.580150068212824\n",
      "42 0.497612551159618\n",
      "43 0.580150068212824\n",
      "44 0.7445429740791268\n",
      "45 0.8823328785811733\n",
      "46 0.7445429740791268\n",
      "47 0.8823328785811733\n",
      "48 0.7445429740791268\n",
      "49 0.8823328785811733\n",
      "50 0.919849931787176\n",
      "51 0.6354024556616644\n",
      "52 0.8823328785811733\n",
      "53 0.6354024556616644\n",
      "54 0.8823328785811733\n",
      "55 0.8823328785811733\n",
      "56 0.919849931787176\n",
      "57 0.919849931787176\n",
      "58 0.8823328785811733\n",
      "59 0.7179399727148704\n",
      "60 0.497612551159618\n",
      "61 0.497612551159618\n",
      "62 0.580150068212824\n",
      "63 0.6354024556616644\n",
      "64 0.8823328785811733\n",
      "65 0.8823328785811733\n",
      "66 0.7445429740791268\n",
      "67 0.8823328785811733\n",
      "68 0.919849931787176\n",
      "69 0.7179399727148704\n",
      "70 0.7179399727148704\n",
      "71 0.8823328785811733\n",
      "72 0.497612551159618\n",
      "73 0.8823328785811733\n",
      "74 0.7445429740791268\n",
      "75 0.6354024556616644\n",
      "76 0.919849931787176\n",
      "77 0.7179399727148704\n",
      "78 0.8823328785811733\n",
      "79 0.8823328785811733\n",
      "80 0.497612551159618\n",
      "81 0.7179399727148704\n",
      "82 0.7820600272851296\n",
      "83 0.8823328785811733\n",
      "84 0.497612551159618\n",
      "85 0.919849931787176\n",
      "86 0.8823328785811733\n",
      "87 0.7179399727148704\n",
      "88 0.6354024556616644\n",
      "89 0.7179399727148704\n",
      "90 0.6354024556616644\n",
      "91 0.7445429740791268\n",
      "92 0.6354024556616644\n",
      "93 0.8823328785811733\n",
      "94 0.7179399727148704\n",
      "95 0.6354024556616644\n",
      "96 0.7179399727148704\n",
      "97 0.7445429740791268\n",
      "98 0.8823328785811733\n",
      "99 0.8823328785811733\n",
      "100 0.8823328785811733\n",
      "101 0.8823328785811733\n",
      "102 0.580150068212824\n",
      "103 0.8823328785811733\n",
      "104 0.7179399727148704\n",
      "105 0.7445429740791268\n",
      "106 0.7445429740791268\n",
      "107 0.8823328785811733\n",
      "108 0.6354024556616644\n",
      "109 0.497612551159618\n",
      "110 0.7179399727148704\n",
      "111 0.6354024556616644\n",
      "112 0.7445429740791268\n",
      "113 0.7445429740791268\n",
      "114 0.919849931787176\n",
      "115 0.7445429740791268\n",
      "116 0.8823328785811733\n",
      "117 0.497612551159618\n",
      "118 0.8823328785811733\n",
      "119 0.7445429740791268\n",
      "120 0.919849931787176\n",
      "121 0.8823328785811733\n",
      "122 0.919849931787176\n",
      "123 0.7179399727148704\n",
      "124 0.919849931787176\n",
      "125 0.7445429740791268\n",
      "126 0.7179399727148704\n",
      "127 0.8823328785811733\n",
      "128 0.6354024556616644\n",
      "129 0.7445429740791268\n",
      "130 0.8823328785811733\n",
      "131 0.7445429740791268\n",
      "132 0.6354024556616644\n",
      "133 0.7179399727148704\n",
      "134 0.7445429740791268\n",
      "135 0.6354024556616644\n",
      "136 0.580150068212824\n",
      "137 0.6354024556616644\n",
      "138 0.919849931787176\n",
      "139 0.7820600272851296\n",
      "140 0.6354024556616644\n",
      "141 0.7179399727148704\n",
      "142 0.6354024556616644\n",
      "143 0.6354024556616644\n",
      "144 0.8823328785811733\n",
      "145 0.8823328785811733\n",
      "146 0.8823328785811733\n",
      "147 0.580150068212824\n",
      "148 0.7179399727148704\n",
      "149 0.8823328785811733\n",
      "150 0.7445429740791268\n",
      "151 0.6354024556616644\n",
      "152 0.6354024556616644\n",
      "153 0.8823328785811733\n",
      "154 0.8823328785811733\n",
      "155 0.8823328785811733\n",
      "156 0.7445429740791268\n",
      "157 0.8823328785811733\n",
      "158 0.7179399727148704\n",
      "159 0.7179399727148704\n",
      "160 0.7820600272851296\n",
      "161 0.7445429740791268\n",
      "162 0.6354024556616644\n",
      "163 0.8823328785811733\n",
      "164 0.7445429740791268\n",
      "165 0.8823328785811733\n",
      "166 0.6354024556616644\n",
      "167 0.6354024556616644\n",
      "168 0.8823328785811733\n",
      "169 0.7179399727148704\n",
      "170 0.7445429740791268\n",
      "171 0.919849931787176\n",
      "172 0.8823328785811733\n",
      "173 0.6354024556616644\n",
      "174 0.6354024556616644\n",
      "175 0.7445429740791268\n",
      "176 0.7820600272851296\n",
      "177 0.497612551159618\n",
      "178 0.497612551159618\n",
      "179 0.7820600272851296\n",
      "180 0.6354024556616644\n",
      "181 0.6354024556616644\n",
      "182 0.7179399727148704\n",
      "183 0.580150068212824\n",
      "184 0.919849931787176\n",
      "185 0.8823328785811733\n",
      "186 0.8823328785811733\n",
      "187 0.497612551159618\n",
      "188 0.6354024556616644\n",
      "189 0.580150068212824\n",
      "190 0.8823328785811733\n",
      "191 0.6354024556616644\n",
      "192 0.919849931787176\n",
      "193 0.919849931787176\n",
      "194 0.6354024556616644\n",
      "195 0.8823328785811733\n",
      "196 0.7179399727148704\n",
      "197 0.6354024556616644\n",
      "198 0.8823328785811733\n",
      "199 0.6354024556616644\n",
      "200 0.8823328785811733\n",
      "201 0.6354024556616644\n",
      "202 0.8823328785811733\n",
      "203 0.8823328785811733\n",
      "204 0.8823328785811733\n",
      "205 0.7179399727148704\n",
      "206 0.7179399727148704\n",
      "207 0.497612551159618\n",
      "208 0.8823328785811733\n",
      "209 0.497612551159618\n",
      "210 0.497612551159618\n",
      "211 0.7820600272851296\n",
      "212 0.7445429740791268\n",
      "213 0.7445429740791268\n",
      "214 0.8823328785811733\n",
      "215 0.6354024556616644\n",
      "216 0.7179399727148704\n",
      "217 0.580150068212824\n",
      "218 0.8823328785811733\n",
      "219 0.7179399727148704\n",
      "220 0.7820600272851296\n",
      "221 0.8823328785811733\n",
      "222 0.8823328785811733\n",
      "223 0.8823328785811733\n",
      "224 0.7445429740791268\n",
      "225 0.6354024556616644\n",
      "226 0.7179399727148704\n",
      "227 0.580150068212824\n",
      "228 0.919849931787176\n",
      "229 0.919849931787176\n",
      "230 0.8823328785811733\n",
      "231 0.7445429740791268\n",
      "232 0.7820600272851296\n",
      "233 0.6354024556616644\n",
      "234 0.8823328785811733\n",
      "235 0.8823328785811733\n",
      "236 0.919849931787176\n",
      "237 0.6354024556616644\n",
      "238 0.6354024556616644\n",
      "239 0.8823328785811733\n",
      "240 0.6354024556616644\n",
      "241 0.8823328785811733\n",
      "242 0.6354024556616644\n",
      "243 0.8823328785811733\n",
      "244 0.6354024556616644\n",
      "245 0.497612551159618\n",
      "246 0.6354024556616644\n",
      "247 0.8823328785811733\n",
      "248 0.7445429740791268\n",
      "249 0.8823328785811733\n",
      "250 0.497612551159618\n",
      "251 0.7445429740791268\n",
      "252 0.6354024556616644\n",
      "253 0.580150068212824\n",
      "254 0.7820600272851296\n",
      "255 0.580150068212824\n",
      "256 0.6354024556616644\n",
      "257 0.6354024556616644\n",
      "258 0.7179399727148704\n",
      "259 0.497612551159618\n",
      "260 0.8823328785811733\n",
      "261 0.7445429740791268\n",
      "262 0.7179399727148704\n",
      "263 0.8823328785811733\n",
      "264 0.7445429740791268\n",
      "265 0.919849931787176\n",
      "266 0.8823328785811733\n",
      "267 0.497612551159618\n",
      "268 0.919849931787176\n",
      "269 0.6354024556616644\n",
      "270 0.6354024556616644\n",
      "271 0.8823328785811733\n",
      "272 0.7445429740791268\n",
      "273 0.8823328785811733\n",
      "274 0.7820600272851296\n",
      "275 0.7445429740791268\n",
      "276 0.6354024556616644\n",
      "277 0.7445429740791268\n",
      "278 0.7445429740791268\n",
      "279 0.6354024556616644\n",
      "280 0.7820600272851296\n",
      "281 0.7179399727148704\n",
      "282 0.7445429740791268\n",
      "283 0.6354024556616644\n",
      "284 0.7179399727148704\n",
      "285 0.6354024556616644\n",
      "286 0.8823328785811733\n",
      "287 0.7445429740791268\n",
      "288 0.8823328785811733\n",
      "289 0.6354024556616644\n",
      "290 0.8823328785811733\n",
      "291 0.8823328785811733\n",
      "292 0.8823328785811733\n",
      "293 0.8823328785811733\n",
      "294 0.8823328785811733\n",
      "295 0.919849931787176\n",
      "296 0.7445429740791268\n",
      "297 0.8823328785811733\n",
      "298 0.7179399727148704\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-019522975b62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcDT_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(1,300):\n",
    "    print (i,cDT_final[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haristhemistocleous/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "cRF_final = []\n",
    "cSVM_final = []\n",
    "cDT_final = []\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_tr)\n",
    "X_tr_transformed = scaler.transform(X_tr)\n",
    "cRF = RandomForestClassifier(n_estimators=13, max_depth=None, min_samples_split=2, random_state=30).fit(X_tr_transformed, y_tr)\n",
    "cSVM = svm.LinearSVC(C=1).fit(X_tr_transformed, y_tr)\n",
    "cDT = DecisionTreeClassifier(random_state=8).fit(X_tr_transformed, y_tr)\n",
    "X_tst_transformed = scaler.transform(X_tst)\n",
    "\n",
    "# Automatic \n",
    "cRF_accuracy = cRF.score(X_tst_transformed, y_tst)\n",
    "cSVM_accuracy = cSVM.score(X_tst_transformed, y_tst)\n",
    "cDT_accuracy = cRF.score(X_tst_transformed, y_tst)\n",
    "\n",
    "# Evaluate classifiers\n",
    "cRF_y_pred = cRF.predict(X_tst_transformed)\n",
    "cSVM_y_pred = cSVM.predict(X_tst_transformed)\n",
    "cDT_y_pred = cDT.predict(X_tst_transformed)\n",
    "\n",
    "# Corrects\n",
    "cRF_n_correct = sum(cRF_y_pred == y_tst)\n",
    "cSVM_n_correct = sum(cSVM_y_pred == y_tst)\n",
    "cDT_n_correct = sum(cDT_y_pred == y_tst)\n",
    "\n",
    "cRF_final = cRF.score(X_tst_transformed, y_tst)\n",
    "cSVM_final = cSVM.score(X_tst_transformed, y_tst)\n",
    "cDT_final = cDT.score(X_tst_transformed, y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RF Accuracy {}\".format(cRF_final))\n",
    "print(\"SVM Accuracy {}\".format(cSVM_final))\n",
    "print(\"DT Accuracy {}\".format(cDT_final))\n",
    "#print(\"SNN Mean {}, SD {}\".format(np.mean(cSNN1_resultsA), np.std(cSNN1_resultsA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = to_categorical(y_tr)\n",
    "y_cat_test  = to_categorical(y_tst)\n",
    "cSNN_finalA = []\n",
    "cSNN_finalB = []\n",
    "# Define the neural network here\n",
    "snn = Sequential()\n",
    "snn.add(Dense(180, input_shape=(62,), activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(180, activation='relu'))   \n",
    "#snn.add(Dropout(0.2)) \n",
    "snn.add(Dense(180, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(180, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(180, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(180, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(3, kernel_initializer='random_normal', activation='softmax'))\n",
    "snn.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_tr)\n",
    "X_tr_transformed = scaler.transform(X_tr)\n",
    "\n",
    "cSNN = snn.fit(X_tr_transformed, \n",
    "               y_cat_train, \n",
    "               epochs=30, \n",
    "               batch_size=5,\n",
    "               verbose=True) \n",
    "\n",
    "# Transform X_Test  \n",
    "X_tst_transformed = scaler.transform(X_tst)\n",
    "cSNN_accuracy2 = snn.evaluate(X_tst_transformed, y_cat_test)\n",
    "print(\"\\n%s: %.2f%%\" % (snn.metrics_names[1], cSNN_accuracy2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(SEED)\n",
    "# Define the neural network here\n",
    "snn = Sequential()\n",
    "snn.add(Dense(150, input_shape=(62,), activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(150, activation='relu'))   \n",
    "#snn.add(Dropout(0.2)) \n",
    "snn.add(Dense(150, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(3, kernel_initializer='random_normal', activation='softmax'))\n",
    "snn.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_tr)\n",
    "X_tr_transformed = scaler.transform(X_tr)\n",
    "\n",
    "cSNN = snn.fit(X_tr_transformed, \n",
    "               y_cat_train, \n",
    "               epochs=50, \n",
    "               batch_size=18,\n",
    "               verbose=True) \n",
    "\n",
    "# Transform X_Test  \n",
    "X_tst_transformed = scaler.transform(X_tst)\n",
    "cSNN_accuracy2 = snn.evaluate(X_tst_transformed, y_cat_test)\n",
    "print(\"\\n%s: %.2f%%\" % (snn.metrics_names[1], cSNN_accuracy2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat_train = to_categorical(y_tr)\n",
    "y_cat_test  = to_categorical(y_tst)\n",
    "set_random_seed(SEED)\n",
    "# Define the neural network here\n",
    "snn = Sequential()\n",
    "snn.add(Dense(150, input_shape=(62,), activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(150, activation='relu'))   \n",
    "#snn.add(Dropout(0.2)) \n",
    "snn.add(Dense(150, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "snn.add(Dense(150, activation='relu'))\n",
    "#snn.add(Dropout(0.2))\n",
    "snn.add(Dense(3, kernel_initializer='random_normal', activation='softmax'))\n",
    "snn.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_tr)\n",
    "X_tr_transformed = scaler.transform(X_tr)\n",
    "\n",
    "cSNN = snn.fit(X_tr_transformed, \n",
    "               y_cat_train, \n",
    "               epochs=50, \n",
    "               batch_size=18,\n",
    "               verbose=True) \n",
    "\n",
    "# Transform X_Test  \n",
    "X_tst_transformed = scaler.transform(X_tst)\n",
    "cSNN_accuracy2 = cSNN.evaluate(X_tst_transformed, y_cat_test)\n",
    "cSNN1_y_pred = snn1.predict_classes(X_tst_transformed)\n",
    "print(\"\\n%s: %.2f%%\" % (snn.metrics_names[1], cSNN_accuracy2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cSNN_accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_cat_train, cSNN1_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_json = snn.to_json()\n",
    "with open(\"snn.json\", \"w\") as json_file:\n",
    "    json_file.write(snn_json)\n",
    "# serialize weights to HDF5\n",
    "snn.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json and create model\n",
    "json_file = open('snn.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_tst_transformed, y_cat_test,verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snn_y_pred = loaded_model.predict_classes(X_tst_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_tst,snn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnf_matrix = metrics.confusion_matrix(y_tst,snn_y_pred)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(cnf_matrix/cnf_matrix.sum(axis=1),decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "class_names = ['svPPA', 'lvPPA', 'nfvPPA']\n",
    "# Plot non-normalized confusion matrix\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "#                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to YAML\n",
    "snn_yaml = snn.to_yaml()\n",
    "with open(\"snn.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(snn_yaml)\n",
    "# serialize weights to HDF5\n",
    "snn.save_weights(\"weights_snn.h5\")\n",
    "print(\"Saved model to disk\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# later...\n",
    " \n",
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_binary_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loss: -96.24%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n%s: %.2f%%\" % (snn.metrics_names[1], cSNN_accuracy2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.         ...,  0.66608315  0.66608315\n",
      "  0.66608315]\n"
     ]
    }
   ],
   "source": [
    "print(npcSNN_accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39.1652342123\n",
      "1.08494026444\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cSNN_accuracy1)*100)\n",
    "print(np.mean(cSNN_accuracy2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.671772428884\n"
     ]
    }
   ],
   "source": [
    "print(cRF_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.648140043764\n"
     ]
    }
   ],
   "source": [
    "print(cSVM_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.355361050328\n"
     ]
    }
   ],
   "source": [
    "print(cDT_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.75      1.00      0.86       723\n",
      "        1.0       0.90      0.43      0.58      1208\n",
      "        2.0       0.39      0.84      0.54       354\n",
      "\n",
      "avg / total       0.78      0.67      0.66      2285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_tst, cRF_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      1.00      0.68       723\n",
      "        1.0       1.00      0.33      0.50      1208\n",
      "        2.0       0.76      1.00      0.87       354\n",
      "\n",
      "avg / total       0.81      0.65      0.61      2285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_tst,cSVM_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       723\n",
      "        1.0       0.90      0.43      0.58      1208\n",
      "        2.0       0.20      0.84      0.32       354\n",
      "\n",
      "avg / total       0.51      0.36      0.36      2285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_tst,cDT_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[723,   0,   0],\n",
       "       [639,   0, 569],\n",
       "       [  0,  56, 298]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_tst, cRF_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[723,   0,   0],\n",
       "       [694, 404, 110],\n",
       "       [  0,   0, 354]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_tst, cSVM_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0, 723],\n",
       "       [235, 514, 459],\n",
       "       [  0,  56, 298]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_tst, cDT_y_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
